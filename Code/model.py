# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CEmfH1llHOL39GTCKCGnBTUdbmsn2XOf
"""

from numpy.lib.arraysetops import ediff1d
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch_geometric.nn import GCNConv,GATConv
import networkx as nx
import numpy as np
import scipy.sparse as sp
import os
from tqdm import tqdm
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler

class DGNN(nn.Module):
    def __init__(self, nfeat, nhid, nout, n_nodes, window, dropout, nhist=7):
        super(DGNN, self).__init__()

        self.window = window
        self.n_nodes = n_nodes
        self.n_state = 1519
        self.nhid = nhid
        self.nfeat = nfeat
        self.nhist = nhist
        self.conv1 = GATConv(nfeat, nhid, concat=False)
        self.conv2 = GATConv(nhid, nhid, concat=False)
        self.bn1 = nn.BatchNorm1d(nhid)
        self.bn2 = nn.BatchNorm1d(nhid)
        self.rnn1 = nn.GRU(2*nhid, nhid, 1)
        self.rnn2 = nn.GRU(nhid, nhid, 1)
        self.fc1 = nn.Linear(2*nhid+window*nhist, nhid)
        self.fc2 = nn.Linear(nhid, nout)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
        self.w1 = Parameter(torch.Tensor(nhid, 1))
        self.w2 = Parameter(torch.Tensor(nhid, 1))
        self.m = nn.Softmax(dim=0)


    def forward(self, adj, x):
        lst = list()
        weight = adj.coalesce().values()
        adj = adj.coalesce().indices()
        skip = x.view(-1,self.window,self.n_nodes,self.nfeat)  
        skip = skip[:,:,:self.n_state,:]
        skip = torch.transpose(skip, 1, 2).reshape(-1,self.window,self.nfeat)
        skip = skip[:,:,:self.nhist]

        x, (edge,att) = self.conv1(x, adj, return_attention_weights=True)
        x = self.dropout(self.bn1(self.relu(x)))
        x2 = x.reshape(-1,self.n_nodes,self.nhid)
        x2 = x2[:,:self.n_state,:]
        x2 = x2.reshape(-1,self.nhid)
        lst.append(x2)
        x = self.dropout(self.bn2(self.relu(self.conv2(x, adj))))
        x2 = x.reshape(-1,self.n_nodes,self.nhid)
        x2 = x2[:,:self.n_state,:]
        x2 = x2.reshape(-1,self.nhid)
        lst.append(x2)

        x = torch.cat(lst, dim=1)
        x = x.view(-1, self.window, self.n_state, x.size(1))
        x = torch.transpose(x, 0, 1)
        x = x.contiguous().view(self.window, -1, x.size(3))
        out1, hn1 = self.rnn1(x)   
        out2, hn2 = self.rnn2(out1)
        x = torch.cat([hn1[0,:,:],hn2[0,:,:]], dim=1)

        skip = skip.reshape(skip.size(0),-1)
        x = torch.cat([x,skip], dim=1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x)).squeeze()
        x = x.view(-1)
        return x, (edge,att)