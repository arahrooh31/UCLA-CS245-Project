# -*- coding: utf-8 -*-
"""data_prep

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tC9q4hX9ZLJ7WMxSuaOCBjujg1u2WgyB
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import pandas as pd
import numpy as np
import json

#!/usr/bin/env python
# coding: utf-8


############################ Make static features ################################


# All States
# Takes a state's populations data (population, population by age-gender group) and removes irrelevant columns
# while also removing non-2020 years
def find_pop_age_gender_counties(state):
    state_2020 = pd.DataFrame(columns=state.columns)
    for row in state.iterrows():
        if row[1]['YEAR'] == 13:
            state_2020 = state_2020.append(row[1])
    state_2020 = state_2020.drop(columns=['SUMLEV','STATE','COUNTY','YEAR'])
    state_2020 = state_2020.drop(columns=[state_2020.columns[i] for i in range(5,35)])
    state_2020.index = range(len(state_2020))
    return state_2020

# Stores population information (population size, population by age-gender groups)
Alabama = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Alabama.csv')
Alaska = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Alaska.csv')
Arizona = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Arizona.csv')
Arkansas = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Arkansas.csv')
California = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_California.csv')
Colorado = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Colorado.csv')
Connecticut = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Connecticut.csv')
Delaware = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Delaware.csv')
Florida = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Florida.csv')
Georgia = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Georgia.csv')
Hawaii = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Hawaii.csv')
Idaho = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Idaho.csv')
Illinois = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Illinois.csv')
Indiana = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Indiana.csv')
Iowa = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Iowa.csv')
Kansas = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Kansas.csv')
Kentucky = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Kentucky.csv')
Louisiana = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Louisiana.csv')
Maine = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Maine.csv')
Maryland = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Maryland.csv')
Massachusetts = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Massachusetts.csv')
Michigan = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Michigan.csv')
Minnesota = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Minnesota.csv')
Mississippi = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Mississippi.csv')
Missouri = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Missouri.csv')
Montana = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Montana.csv')
Nebraska = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Nebraska.csv')
Nevada = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Nevada.csv')
New_Hampshire = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_New Hampshire.csv')
New_Jersey = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_New Jersey.csv')
New_Mexico = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_New Mexico.csv')
New_York = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_New York.csv')
North_Carolina = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_North Carolina.csv')
North_Dakota = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_North Dakota.csv')
Ohio = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Ohio.csv')
Oklahoma = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Oklahoma.csv')
Oregon = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Oregon.csv')
Pennsylvania = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Pennsylvania.csv')
Rhode_Island = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Rhode Island.csv')
South_Carolina = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_South Carolina.csv')
South_Dakota = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_South Dakota.csv')
Tennessee = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Tennessee.csv')
Texas = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Texas.csv')
Utah = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Utah.csv')
Vermont = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Vermont.csv')
Virginia = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Virginia.csv')
Washington = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Washington.csv')
Washington_DC = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Washington DC.csv')
West_Virginia = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_West Virginia.csv')
Wisconsin = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Wisconsin.csv')
Wyoming = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2020_Population_Wyoming.csv')

#Stores those state dataframes in a list
states = [Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New_Hampshire, New_Jersey, New_Mexico, New_York, North_Carolina, North_Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode_Island, South_Carolina, South_Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, Washington_DC, West_Virginia, Wisconsin, Wyoming]

# Initializes a list to store the state dataframes with data just for 2020
states_2020 = []

# Stores the 2020 data for the states
for state in states:
    locals()[state['STNAME'][0]+'_2020'] = find_pop_age_gender_counties(state)
    states_2020.append(locals()[state['STNAME'][0]+'_2020'])

# Fixing Alaska, numbers were str instead of int or float
for i in range(0,len(states_2020[1])):
    for j in range(2,len(states_2020[1].columns)):
        states_2020[1].iloc[i,j] = float(states_2020[1].iloc[i,j])
        
        
# Combines all the states' counties into one dataframe
all_states = pd.DataFrame(columns=California_2020.columns)
for state in states_2020:
    #print(state['STNAME'][0]+'_2020', end='\r')
    for row in state.iterrows():
        all_states = all_states.append(row[1])
all_states.rename(columns = {'STNAME':'State','CTYNAME':'County'}, inplace = True)
for row in all_states.iterrows():
    if row[1]['County'] == 'LaSalle Parish' and row[1]['State'] == 'Louisiana':
        #print('ok')
        row[1]['County'] = 'La Salle Parish'
    if row[1]['County'] == 'DoÃ±a Ana County' and row[1]['State'] == 'New Mexico':
        #print('ok')
        row[1]['County'] = 'Dona Ana County'

        
more_data = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/state_demo/2021_County_Data.csv')

# Make state:county dictionary
state_county = {}
for state in ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'Washington DC','West Virginia', 'Wisconsin', 'Wyoming']:
    state_county[state] = []
for row in all_states.iterrows():
    state_county[row[1]['State']].append(row[1]['County'])
with open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_county_dict.json', 'w') as j:
    json.dump(state_county, j)
    
# Harmonize the county names between all_states and more_data dataframes
for index, row in more_data.iterrows():
    #print(index)
    md_county = row[2]
    md_state = row[1]
    if type(md_county) != float:
        i = 0
        for sc_county in state_county[md_state]:
            if sc_county.lower().startswith(md_county.lower()+' ') or sc_county.lower() == md_county.lower():
                i += 1
                county_name = sc_county
        if i == 1:
            more_data.loc[index, 'County'] = county_name
        else:
            print('Issue with', md_state, '|', md_county)
        #print(md_county, md_state, '-->', county_name)
        
# Relevant features/columns from more_data
md_features = ['FIPS','Life Expectancy','Age-Adjusted Death Rate','Child Mortality Rate','% Frequent Physical Distress','% Frequent Mental Distress','% Adults with Diabetes','HIV Prevalence Rate','% Food Insecure','% Limited Access to Healthy Foods','Drug Overdose Mortality Rate','Motor Vehicle Mortality Rate','% Insufficient Sleep','High School Graduation Rate','Average Grade Performance','Median Household Income','% Enrolled in Free or Reduced Lunch','Homicide Rate','Suicide Rate (Age-Adjusted)','Firearm Fatalities Rate','% Homeowners','% Broadband Access','% Black','% American Indian & Alaska Native','% Asian','% Native Hawaiian/Other Pacific Islander','% Hispanic','% Non-Hispanic White','% Not Proficient in English','% Rural']
cf_col = md_features
for col in all_states.columns:
    cf_col.append(col)
    
# Splits more_data into df for states and df for counties
more_data_states = pd.DataFrame(columns = more_data.columns)
more_data_counties = more_data.copy()
delete_ind = []
for ind, row in more_data.iterrows():
    if type(row[2]) == float:
        delete_ind.append(ind)
        
for i in range(0,len(delete_ind)):
        more_data_states = more_data_states.append(more_data_counties.iloc[delete_ind[i]])
for i in range(len(delete_ind)-1, -1, -1):
    more_data_counties = more_data_counties.drop(more_data_counties.index[delete_ind[i]])
more_data_states.reset_index(drop=True, inplace=True)
more_data_counties.reset_index(drop=True, inplace=True)

# Remove uninteresting columns from more_data_states and more_data_counties
for col in more_data_counties.columns:
    if col not in cf_col:
        del more_data_counties[col]
        del more_data_states[col]
del more_data_states['County']
county_features = pd.merge(more_data_counties, all_states)
county_features.to_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_static_features.csv')

#del state_pops
state_pops = pd.DataFrame(columns=all_states.columns)
del state_pops['County']

for STATE in states_2020:
    p0 = pd.Series({'State':STATE.iloc[0,0]})
    #Creating first part of row
    p1 = STATE.iloc[:,2:59].sum()

    #Creating second part of row
    pop = np.matrix(STATE.iloc[:,2:3])
    #print(pop, type(pop))
    pop_total = pop.sum()
    medians = np.matrix(STATE.iloc[:,59:])
    med = 0
    for i in range(0,len(STATE)):
        med += pop[i]/pop_total*medians[i]
    p2 = pd.Series({'MEDIAN_AGE_TOT':med.tolist()[0][0], 'MEDIAN_AGE_MALE':med.tolist()[0][1],'MEDIAN_AGE_FEM':med.tolist()[0][2]})
    p = p0.append(p1).append(p2)
    state_pops = state_pops.append(p, ignore_index=True)
#state_pops

state_features = pd.merge(more_data_states, state_pops)

state_features.to_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv')
state_features = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv')
county_features = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_static_features.csv')

state_and_county_feats = county_features.append(state_features)

state_and_county_feats = state_and_county_feats.reset_index(drop=True)

state_and_county_feats.to_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_and_county_static_features.csv')



############################################# State borderings ###########################33
df_states = pd.read_csv(r'/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/US Bordering States.csv')
df_states.iloc[23][1] = ['Alabama', 'Arkansas', 'Louisiana', 'Tennessee']
# Create state nodes
states_dict = {}
for i in range(0, len(df_states)):
    state = df_states.iloc[i][0]
    if i == 23:
        df_states.iloc[23][1] = ['Alabama', 'Arkansas', 'Louisiana', 'Tennessee']
    else:
        bordering = df_states.iloc[i][1].split(', ')
    states_dict[state] = bordering
states_dict['Washington DC'] = ['Maryland', 'Virginia']


# Identify unique bordering states relationships
SB1 = []
SB2 = []
for state in states_dict.keys():
    for bordering_state in states_dict[state]:
        if('water border' not in state and 'water border' not in bordering_state and 'None' not in bordering_state):
            SB1.append({state, bordering_state})

for i in range(0,len(SB1)):
    if (SB1[i] not in SB2):
        SB2.append(SB1[i])         

state_state_for_json = []
for pair in SB2:
    i = 0
    for state in pair:
        i+=1
        if i == 1:
            p0 = state
        else:
            p1 = state
    state_state_for_json.append({p0:p1})

import json
with open ('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state-state_bordering.json', "w") as f:
    json.dump(state_state_for_json, f)


################################################## Make county code borders #########################
c_a = json.load(open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/county_adjacency.json'))

id_county = {} 
for county in c_a['Counties']:
    county_n = county['Name']
    if not county_n.endswith(' PR') and not county_n.endswith(', AS') and not county_n.endswith(', MP') and not county_n.endswith(', VI'):
        ID = int(county['Id'])
        id_county[ID] = county_n
    
id_code = {}
for county in c_a['Counties']:
    county_n = county['Name']
    if not county_n.endswith(' PR') and not county_n.endswith(', AS') and not county_n.endswith(', MP') and not county_n.endswith(', VI') and not county_n.endswith(', GU'):
        code = county['Zip']
        if code.startswith('0'):
            code = code[1:]
        code = int(code)
        ID = int(county['Id'])
        id_code[ID] = code
    
code_codes = {}
for county in c_a['Counties']:
    county_n = county['Name']
    if not county_n.endswith(' PR') and not county_n.endswith(', AS') and not county_n.endswith(', MP') and not county_n.endswith(', VI') and not county_n.endswith(', GU'):
        code = int(county['Zip'])
        codes = []
        ids = county['Neighbors']
        for ID in ids:
            codes.append(id_code[ID])
        code_codes[code] = codes

county_counties = {}
for county in c_a['Counties']:
    county_n = county['Name']#.split(',')[0]
    if not county_n.endswith(' PR') and not county_n.endswith(', AS') and not county_n.endswith(', MP') and not county_n.endswith(', VI') and not county_n.endswith(', GU'):
        counties_n = []
        ids = county['Neighbors']
        for ID in ids:
            counties_n.append(id_county[ID])
        county_counties[county_n] = counties_n

county_counties['Washington DC'] = county_counties['District of Columbia, DC']
del county_counties['District of Columbia, DC']

with open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countycodes_border.json','w') as j:
    json.dump(code_codes, j)
    
with open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countynames_border.json','w') as k:
    json.dump(county_counties, k)

fips_to_id = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/input/FIPS_ID_mappping.csv', usecols = ['FIPS','ID'])


############################### CREATING GRAPHS ##############################################################
# Run !python3 make_county_code_borders.py
# Run !python3 make_static_features.py
# Modeled after https://github.com/joey1993/pandemic-forecast/blob/main/pandemic-forecast/code/covid_graph_prep.py, https://github.com/joey1993/pandemic-forecast/blob/main/pandemic-forecast/code/utils.py
import pandas as pd
import json
import csv
import networkx as nx

def pre_graphs():
    # (1) Edges: County-County 'borders' (county1, county2, weight=1)
    # Make bordering county prep file
    c_borders_file = json.load(open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countycodes_border.json', 'r'))
    c_borders = {int(k):v for k,v in c_borders_file.items()}
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for c1, c2 in c_borders.items():
        for C2 in c2:
          graph_writer.writerow([c1, C2, 1])
    borders_csv.close()
    c_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv', header=None)

    # (2) Edges: State-State 'borders' (state1, state2, weight=1) ####
    # Make bordering state prep file
    state_fips = {}
    state_fipsDF = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS', 'State'])
    for row in state_fipsDF.iterrows(): state_fips[str(row[1][1])] = int(row[1][0])
    # Make dictionary of state:fips
    s_s_name = json.load(open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state-state_bordering.json', 'r')) # Undirected relationships
    # Make directed relationships with fips codes (translate using state_fips' state:id)
    s_s_fips = {} 
    for pair in s_s_name:
        state1 = list(pair.keys())[0]
        state2 = list(pair.values())[0]
        fips1 = state_fips[state1]
        fips2 = state_fips[state2]
        if fips1 not in s_s_fips.keys(): s_s_fips[fips1] = []
        if fips2 not in s_s_fips.keys(): s_s_fips[fips2] = []
        s_s_fips[fips1].append(fips2)
        s_s_fips[fips2].append(fips1)
    s_borders = s_s_fips
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for s1, s2 in s_borders.items():
        for S2 in s2:
            graph_writer.writerow([s1, S2, 1])
    borders_csv.close()
    s_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv', header=None)


    # (3) Edges: County-State 'belongs to' (state, county, weight=1) ####
    # Creating dictionary of stateFIPS:[countyFIPS,...]
    sName_cFIPS = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_and_county_static_features.csv', usecols = ['FIPS', 'State'])
    sc_belongs = {}
    for row in sName_cFIPS.iterrows():
        stateName = row[1][1]
        stateFIPS = state_fips[stateName]
        countyFIPS = row[1][0]
        if stateFIPS not in sc_belongs.keys(): sc_belongs[stateFIPS] = []
        sc_belongs[stateFIPS].append(countyFIPS)
    # Creating a dataframe with rows of (stateFIPS, countyFIPS, 1)
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for s, c in sc_belongs.items():
        for C in c:
            graph_writer.writerow([s, C, 1])
    borders_csv.close()
    scb_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv', header=None)

    #### Combined Edge Tables
    # 2 Edge Types: County-county 'borders', state-state 'borders' (loc1, loc2, weight=1)
    #sc_graph_prep1 = s_graph_prep
    #sc_graph_prep1 = sc_graph_prep1.append(c_graph_prep)
    # 3 Edge Types: County-county 'borders', state-state 'borders', state-county 'belongs' rows (loc1, loc2, weight=1)
    #sc_graph_prep2 = sc_graph_prep1.append(scb_graph_prep)
    
    return c_borders, s_borders, sc_belongs

# Use for experiment with 1519 counties (those with complete data)
def pre_graphs_completedata():
    # (1) Edges: County-County 'borders' (county1, county2, weight=1)
    # Make bordering county prep file
    c_borders_file = json.load(open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countycodes_border.json', 'r'))
    c_borders = {int(k):v for k,v in c_borders_file.items()}
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for c1, c2 in c_borders.items():
        for C2 in c2:
            if C2 in f2iK and c1 in f2iK:
                graph_writer.writerow([f2i[c1], f2i[C2], 1])
    borders_csv.close()
    c_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv', header=None)

    # (2) Edges: State-State 'borders' (state1, state2, weight=1) ####
    # Make bordering state prep file
    state_fips = {}
    state_fipsDF = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS', 'State'])
    for row in state_fipsDF.iterrows(): state_fips[str(row[1][1])] = int(row[1][0])
    # Make dictionary of state:fips
    s_s_name = json.load(open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state-state_bordering.json', 'r')) # Undirected relationships
    # Make directed relationships with fips codes (translate using state_fips' state:id)
    s_s_fips = {} 
    for pair in s_s_name:
        state1 = list(pair.keys())[0]
        state2 = list(pair.values())[0]
        fips1 = state_fips[state1]
        fips2 = state_fips[state2]
        if fips1 not in s_s_fips.keys(): s_s_fips[fips1] = []
        if fips2 not in s_s_fips.keys(): s_s_fips[fips2] = []
        s_s_fips[fips1].append(fips2)
        s_s_fips[fips2].append(fips1)
    s_borders = s_s_fips
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for s1, s2 in s_borders.items():
        for S2 in s2:
            if S2 in f2iK and s1 in f2iK:
                graph_writer.writerow([f2i[s1], f2i[S2], 1])
    borders_csv.close()
    s_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv', header=None)


    # (3) Edges: County-State 'belongs to' (state, county, weight=1) ####
    # Creating dictionary of stateFIPS:[countyFIPS,...]
    sName_cFIPS = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_and_county_static_features.csv', usecols = ['FIPS', 'State'])
    sc_belongs = {}
    for row in sName_cFIPS.iterrows():
        stateName = row[1][1]
        stateFIPS = state_fips[stateName]
        countyFIPS = row[1][0]
        if stateFIPS not in sc_belongs.keys(): sc_belongs[stateFIPS] = []
        sc_belongs[stateFIPS].append(countyFIPS)
    # Creating a dataframe with rows of (stateFIPS, countyFIPS, 1)
    borders_csv = open('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv','w')
    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='"')
    for s, c in sc_belongs.items():
        for C in c:
            if C in f2iK and s in f2iK:
              graph_writer.writerow([f2i[s], f2i[C], 1])
    borders_csv.close()
    scb_graph_prep = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv', header=None)

    #### Combined Edge Tables
    # 2 Edge Types: County-county 'borders', state-state 'borders' (loc1, loc2, weight=1)
    #sc_graph_prep1 = s_graph_prep
    #sc_graph_prep1 = sc_graph_prep1.append(c_graph_prep)
    # 3 Edge Types: County-county 'borders', state-state 'borders', state-county 'belongs' rows (loc1, loc2, weight=1)
    #sc_graph_prep2 = sc_graph_prep1.append(scb_graph_prep)
    
    return c_borders, s_borders, sc_belongs


####### Graph Construction Functions
#### OPTION 1:  Graph of bordering counties
def create_tmp_graphs1(date_range):
    c_borders, s_borders, sc_belongs = pre_graphs()
    G = nx.Graph() # Undirected graph
    Gs = []
    #nodes = (len(c_borders))
    edges = []
    for k, v in c_borders.items():
        for V in v:
            edges.append((k,V))
    G.add_edges_from(edges)
    print("Total number of nodes: ", int(G.number_of_nodes()))
    print("Total number of edges: ", int(G.number_of_edges()))
    for date in date_range:
        Gs.append(G)
    return Gs

#### OPTION 2: Graph of bordering counties, bordering states
def create_tmp_graphs2(date_range):
    c_borders, s_borders, sc_belongs = pre_graphs()
    G = nx.Graph() # Directed graph
    Gs = []
    #nodes = len(c_borders) + len(s_borders)
    edges = []
    for k, v in c_borders.items():
        for V in v:
            edges.append((k,V))
    for k, v in s_borders.items():
        for V in v:
            edges.append((k,V))
    G.add_edges_from(edges)
    print("Total number of nodes: ", int(G.number_of_nodes()))
    print("Total number of edges: ", int(G.number_of_edges()))
    for date in date_range:
        Gs.append(G)
    return Gs

#### OPTION 3: Graph of bordering counties, bordering states, county-state edges
def create_tmp_graphs3(date_range):
    c_borders, s_borders, sc_belongs = pre_graphs()
    G = nx.DiGraph() # Directed graph
    Gs = []
    #nodes = len(c_borders) + len(s_borders)
    edges = []
    for c1, c2 in c_borders.items(): # county<->county 'borders'
        for C2 in c2:
            edges.append((c1,C2))
            edges.append((C2,c1))
    for s1, s2 in s_borders.items(): # state<->state 'borders'
        for S2 in s2:
            edges.append((s1,S2))
            edges.append((S2,s1))
    for s, c in sc_belongs.items(): # county->state 'belongs
        for C in c:
            edges.append((C,s))

    G.add_edges_from(edges)
    print("Total number of nodes: ", int(G.number_of_nodes()))
    print("Total number of edges: ", int(G.number_of_edges()))
    for date in date_range:
        Gs.append(G)
    return Gs


#### OPTION 3b: Graph of bordering counties, bordering states, county-state edges, complete data only
def create_tmp_graphs3b(date_range, f2iK, f2i):
    c_borders, s_borders, sc_belongs = pre_graphs_completedata()
    G = nx.DiGraph() # Directed graph
    Gs = []
    #nodes = len(c_borders) + len(s_borders)
    edges = []
    for c1, c2 in c_borders.items(): # county<->county 'borders'
        for C2 in c2:
          if C2 in f2iK and c1 in f2iK:
              edges.append((f2i[c1],f2i[C2]))
              edges.append((f2i[C2],f2i[c1]))
    for s1, s2 in s_borders.items(): # state<->state 'borders'
        for S2 in s2:
            if S2 in f2iK and s1 in f2iK:
              edges.append((f2i[s1],f2i[S2]))
              edges.append((f2i[S2],f2i[s1]))
    for s, c in sc_belongs.items(): # county->state 'belongs
        for C in c:
            if s in f2iK and C in f2iK:
              edges.append((f2i[C],f2i[s]))

    G.add_edges_from(edges)
    print("Total number of nodes: ", int(G.number_of_nodes()))
    print("Total number of edges: ", int(G.number_of_edges()))
    for date in date_range:
        Gs.append(G)
    return Gs
    

#### OPTION 4: Graph of bordering counties, mobility
def create_tmp_graphs4(date_range):
    c_borders, s_borders, sc_belongs = pre_graphs()
    Gs = []
    for date in date_range:
        Gs.append(G)
    return Gs

#### OPTION 5: Graph of bordering counties, bordering states, county-state belongs, edges w/mobility
def create_tmp_graphs5(date_range):
    c_borders, s_borders, sc_belongs = pre_graphs()
    Gs = []
    for date in date_range:
        Gs.append(G)
    return Gs


################################################# Features #################################################

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Choose a range of date for which we want to generate the feature matrix 
# The earliest start date allowed is 2021-01-01
start_date = '2021-08-01T00:00:00.000'
end_date = '2021-11-28T00:00:00.000'

# Read in the dynamic and static data files
df_state_static = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/static_state_features.csv')
df_county_static = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/counties_static_features.csv')
df_covid_cases = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/CDC_COVID19_cases_and_vaccine_time_series/covid_cases_by_county.csv')
df_covid_vaccine = pd.read_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/CDC_COVID19_cases_and_vaccine_time_series/vaccine_status_by_county.csv')

# Delete the unnecessary indexing column
df_state_static = df_state_static.drop(df_state_static.columns[0], axis=1)
df_county_static = df_county_static.drop(df_county_static.columns[0], axis=1)
df_covid_cases = df_covid_cases.drop(df_covid_cases.columns[0], axis = 1)

# For county data, we don't need the column that indicates the state
df_county_static = df_county_static.drop('State', 1)

# Combine the state and county static features together into a single matrix
# The first 51 rows are state-wise features, the rest of the 3142 rows are county-wise features
# Both state and county have the same kind of features
df_state_static = df_state_static.rename({'State': 'location'}, axis='columns') 
df_county_static = df_county_static.rename({'County': 'location'}, axis='columns')
df_state_and_county_static = pd.concat([df_state_static, df_county_static])
df_state_and_county_static = df_state_and_county_static.reset_index(drop = True)
df_state_and_county_static['location'] = df_state_and_county_static['location'].str.lower()
# Create a location name -- FIPS mapper in a dictionary
location_fips_mapping = dict(zip(df_state_and_county_static['location'][0:51], df_state_and_county_static['FIPS'][0:51]))


# For COVID cases data, drop any rows that have missing values for cases_per_100k_7_day_count or percent_test_results_reported
df_covid_cases['cases_per_100k_7_day_count'] = df_covid_cases['cases_per_100k_7_day_count'].replace('suppressed', np.nan)

# Count number of missing data for each COVID variable
print('Number of missing for cases per 100k ', df_covid_cases['cases_per_100k_7_day_count'].isnull().sum())
print('Number of missing for percent_test ', df_covid_cases['percent_test_results_reported'].isnull().sum())

df_covid_cases_no_missing = df_covid_cases.dropna(subset = ['cases_per_100k_7_day_count'])
df_covid_cases_no_missing = df_covid_cases_no_missing.reset_index(drop = True)

# Clean the numbers by getting rid of the comma in the numbers
df_covid_cases_no_missing['cases_per_100k_7_day_count'] = df_covid_cases_no_missing.apply(lambda x: x['cases_per_100k_7_day_count'].replace(',', ''),axis=1)
df_covid_cases_no_missing['cases_per_100k_7_day_count'] = pd.to_numeric(df_covid_cases_no_missing['cases_per_100k_7_day_count'])
#df_covid_cases_no_missing['percent_test_results_reported'] = pd.to_numeric(df_covid_cases_no_missing['percent_test_results_reported'])

# For vaccine data, drop any rows that have missing values for series_complete_pop_pct 
df_covid_vaccine_no_missing = df_covid_vaccine.dropna(subset = ['series_complete_yes'])
df_covid_vaccine_no_missing = df_covid_vaccine_no_missing.reset_index(drop = True)

print('Number of rows with missing values for COVID cases info from March 1 2020 to Nov 2021', len(df_covid_cases) - len(df_covid_cases_no_missing))
print('Number of rows with missing values for COVID vaccine info from March 1 2020 to Nov 2021', len(df_covid_vaccine) - len(df_covid_vaccine_no_missing))

# Reference: https://gist.github.com/rogerallen/1583593
us_state_to_abbrev = {
    "Alabama": "AL",
    "Alaska": "AK",
    "Arizona": "AZ",
    "Arkansas": "AR",
    "California": "CA",
    "Colorado": "CO",
    "Connecticut": "CT",
    "Delaware": "DE",
    "Florida": "FL",
    "Georgia": "GA",
    "Hawaii": "HI",
    "Idaho": "ID",
    "Illinois": "IL",
    "Indiana": "IN",
    "Iowa": "IA",
    "Kansas": "KS",
    "Kentucky": "KY",
    "Louisiana": "LA",
    "Maine": "ME",
    "Maryland": "MD",
    "Massachusetts": "MA",
    "Michigan": "MI",
    "Minnesota": "MN",
    "Mississippi": "MS",
    "Missouri": "MO",
    "Montana": "MT",
    "Nebraska": "NE",
    "Nevada": "NV",
    "New Hampshire": "NH",
    "New Jersey": "NJ",
    "New Mexico": "NM",
    "New York": "NY",
    "North Carolina": "NC",
    "North Dakota": "ND",
    "Ohio": "OH",
    "Oklahoma": "OK",
    "Oregon": "OR",
    "Pennsylvania": "PA",
    "Rhode Island": "RI",
    "South Carolina": "SC",
    "South Dakota": "SD",
    "Tennessee": "TN",
    "Texas": "TX",
    "Utah": "UT",
    "Vermont": "VT",
    "Virginia": "VA",
    "Washington": "WA",
    "West Virginia": "WV",
    "Wisconsin": "WI",
    "Wyoming": "WY",
    "Washington DC": "DC",
    "American Samoa": "AS",
    "Guam": "GU",
    "Northern Mariana Islands": "MP",
    "Puerto Rico": "PR",
    "United States Minor Outlying Islands": "UM",
    "U.S. Virgin Islands": "VI",
}

# Swap the key and value because we want to convert abbreviation to full name
us_abbrev_to_state = dict((v,k) for k,v in us_state_to_abbrev.items())

# From the entire COVID dynamic data, take out the data within a desired date range
# Here sorting does not affect results. It was used for debugging purpose.
def select_data_by_dates(df_input):
  df_sorted = df_input.sort_values(['date', 'state_name'])
  df_sorted['date'] = pd.to_datetime(df_sorted['date'])

  # Incorporate additional past 7 days from the starting day because we need to generate the historical 7-day COVID information later on.
  df_sorted = df_sorted[df_sorted['date'] >= pd.to_datetime(start_date) - timedelta(days = 7)]
  df_sorted = df_sorted[df_sorted['date'] <= pd.to_datetime(end_date)]

  df_sorted = df_sorted.reset_index(drop = True)
  df = df_sorted
  return df

# Rename some columns for vaccine data to be the same as the ones in the covid cases data so that later we can merge vaccine and cases data together
df_covid_vaccine_no_missing = df_covid_vaccine_no_missing.rename(columns= {'date': 'date', 
                                                                           'fips': 'fips_code', 'recip_county': 'county_name', 
                                                                           'recip_state': 'state_name'})

df_cases_selected_by_dates = select_data_by_dates(df_covid_cases_no_missing)
df_vaccine_selected_by_dates = select_data_by_dates(df_covid_vaccine_no_missing)

# Modify the location names of the covid cases dynamic data so that it is consistent with the static data
# Convert the state abbrev to full name for vaccine data
df_vaccine_selected_by_dates['state_name'] = df_vaccine_selected_by_dates['state_name'].map(us_abbrev_to_state)
df_vaccine_selected_by_dates['state_name'] = df_vaccine_selected_by_dates['state_name'].str.lower()
df_vaccine_selected_by_dates['county_name'] = df_vaccine_selected_by_dates['county_name'].str.lower()
# Somehow there are rows with FIPS code be "UNK". Need to drop these rows.
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['fips_code'] != 'UNK']
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates.reset_index(drop = True)
df_vaccine_selected_by_dates['fips_code'] = pd.to_numeric(df_vaccine_selected_by_dates['fips_code'])

df_cases_selected_by_dates['state_name'] = df_cases_selected_by_dates['state_name'].str.lower()
df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].str.lower()
df_cases_selected_by_dates['state_name'] = df_cases_selected_by_dates['state_name'].replace('district of columbia', 'washington dc')
df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('district of columbia', 'washington dc')
df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('lasalle parish', 'la salle parish')
df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('doÃ±a ana county', 'dona ana county')

# Get rid of locations that don't exist in the static data 
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'american samoa'] 
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'northern mariana islands'] 
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'guam'] 
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'puerto rico'] 
df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'u.s. virgin islands'] 


df_cases_selected_by_dates = df_cases_selected_by_dates[df_cases_selected_by_dates['state_name'] != 'puerto rico'] 
df_cases_selected_by_dates = df_cases_selected_by_dates.reset_index(drop = True)

# Store the dates in a list so that later we can loop through each date and create relevant data for each date.
dates_list = df_cases_selected_by_dates['date'].drop_duplicates()
dates_list = dates_list.reset_index(drop = True)

def create_cleaned_df_for_dynamic_features(dates_list, df_input, variable_name):
  print('For variable ', variable_name)
  results_across_days = []
  for date_index in range(len(dates_list)):
    # Process each day separately 
    date = dates_list[date_index]
    groups_by_date = df_input.groupby('date')
    print('Processing ', date)

    # Find the rows/locations corresponding to the same date
    df_data_with_same_date = groups_by_date.get_group(date)
    df_data_with_same_date = df_data_with_same_date.reset_index(drop = True)

    # Calculate state-level COVID numbers by aggregating across the counties in that state
    # Within the rows with the same date, find the ones with the same US state name
    groups_by_state_indices = df_data_with_same_date.groupby('state_name').indices
    for key, value in groups_by_state_indices.items():
      state_name = key
      # For each state, get its COVID data by summing over all its counties 
      FIPS_of_location = location_fips_mapping[state_name]
      state_level_result = sum(df_data_with_same_date[variable_name][value])
      results_across_days.append((date, state_name, FIPS_of_location, state_level_result))

    # After getting the state-level data, we simply save the county level COVID data 
    for county_index in range(len(df_data_with_same_date)): 
      county_name = df_data_with_same_date['county_name'][county_index]
      county_FIPS = df_data_with_same_date['fips_code'][county_index]
      county_level_result = df_data_with_same_date[variable_name][county_index]
      results_across_days.append((date, county_name, county_FIPS, county_level_result))

  # Save as a dataframe
  df_results = pd.DataFrame(results_across_days, columns = ['date', 'location', 'FIPS', variable_name])
  # Convert the date type into datetime so that the dates can be calculated later
  df_results['date'] = pd.to_datetime(df_results['date'])
  print('Number of resulting rows ', len(df_results))
  return df_results

df_cleaned_covid_cases = create_cleaned_df_for_dynamic_features(dates_list, df_cases_selected_by_dates, 'cases_per_100k_7_day_count')
df_cleaned_covid_vaccine = create_cleaned_df_for_dynamic_features(dates_list, df_vaccine_selected_by_dates, 'series_complete_yes')


variable_name = 'cases_per_100k_7_day_count'
grouped_results = df_cleaned_covid_cases.groupby('date')
df_merged_temporal_current = grouped_results.get_group(dates_list[0])
for date_index in range(1, len(dates_list)):
  df_nth_day = grouped_results.get_group(dates_list[date_index])
  # Some preprocessing on column names to make the merging easier
  df_nth_day = df_nth_day.drop('date', 1)
  df_nth_day = df_nth_day.reset_index(drop = True)
  df_nth_day = df_nth_day.rename(columns = {variable_name: str(dates_list[date_index])[0:10]})

  # Merging 
  df_merged_temporal = pd.merge(df_nth_day, df_merged_temporal_current, on = 'FIPS')
  df_merged_temporal = df_merged_temporal.drop('location_y', 1)
  df_merged_temporal = df_merged_temporal.rename(columns = {'location_x': 'location'})
  df_merged_temporal_current = df_merged_temporal
  
df_merged_temporal_current = df_merged_temporal_current.drop('date', 1)
df_merged_temporal_current = df_merged_temporal_current.rename(columns = {'cases_per_100k_7_day_count' : str(dates_list[0])[0:10]})
df_merged_temporal_current = df_merged_temporal_current.sort_values(by = 'FIPS')
df_merged_temporal_current = df_merged_temporal_current.reset_index(drop = True)


# When making the feature matrix, we only use the locations that have complete data
locations_to_keep = df_merged_temporal_current['FIPS'].tolist()
df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases.query('FIPS in @locations_to_keep').copy()
df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine.query('FIPS in @locations_to_keep').copy()
df_state_and_county_static_complete_data = df_state_and_county_static.query('FIPS in @locations_to_keep').copy()

df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases_complete_data.sort_values(by = ['date', 'FIPS'])
df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases_complete_data.reset_index(drop = True)
df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine_complete_data.sort_values(by = ['date', 'FIPS'])
df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine_complete_data.reset_index(drop = True)
df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.sort_values(by = 'FIPS')
df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.reset_index(drop = True)
df_cleaned_covid_cases_vaccine_complete_data = df_cleaned_covid_cases_complete_data
df_cleaned_covid_cases_vaccine_complete_data.insert(3, 'series_complete_yes', df_cleaned_covid_vaccine_complete_data['series_complete_yes'].values)


# Clean the static data
# Delete the static features that have more than 50 locations having missing values for it
df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.dropna(thresh = (len(df_state_and_county_static_complete_data) - 75), axis = 1)

# For other features, use average feature column value to impute the missing values
df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.fillna(df_state_and_county_static_complete_data.mean())

# Check which features are deleted
set(df_state_and_county_static.columns) - set(df_state_and_county_static_complete_data.columns)

# Store the dates in a list so that later we can loop through each date and create feature matrix for each date.
# The starting date is the user-specified starting date, therefore we won't include the historical dates of the starting date.
def make_feature_matrix(df_input):
  dates_list_excluding_historical_dates = df_input['date'].drop_duplicates()[df_input['date'] >= (df_input['date'].min() + timedelta(days = 7))]
  dates_list_excluding_historical_dates = dates_list_excluding_historical_dates.reset_index(drop = True)

  # Create the feature matrix that contains the previous 7 days' covid information plus the static features 
  grouped_results = df_input.groupby('date')
  n_days_before_today = [1, 2, 3, 4, 5, 6, 7]
  features_list = []
  for date_index in range(len(dates_list_excluding_historical_dates)):
    df_merged_static_plus_temporal = df_state_and_county_static_complete_data
    print('Processing ', dates_list_excluding_historical_dates[date_index])

    # Iteratively retrieve the past Nth day's COVID data and merge it into the current dataframe
    for t in range(len(n_days_before_today)):
      # Retrieve the past Nth day's COVID data
      n = n_days_before_today[t]
      historical_date = dates_list_excluding_historical_dates[date_index] - timedelta(days = n)
      df_historical_nth_day = grouped_results.get_group(historical_date)
      # Some preprocessing on column names to make the merging easier
      df_historical_nth_day = df_historical_nth_day.drop('date', 1)
      df_historical_nth_day = df_historical_nth_day.sort_values(by = 'FIPS')
      df_historical_nth_day = df_historical_nth_day.reset_index(drop = True)
      # Calculate the vaccine data by dividing the raw numbers by population in that location.
      df_historical_nth_day['series_complete_yes_normed'] = df_historical_nth_day['series_complete_yes'] / df_state_and_county_static_complete_data['POPESTIMATE']
      df_historical_nth_day = df_historical_nth_day.drop('series_complete_yes', 1)
      df_historical_nth_day = df_historical_nth_day.rename(columns = {'cases_per_100k_7_day_count': 'cases_per_100k_7_day_count_' + str(n) + '_days_before_current_day'})
      df_historical_nth_day = df_historical_nth_day.rename(columns = {'series_complete_yes_normed': 'series_complete_yes_normed_' + str(n) + '_days_before_current_day'})
      

      # Merging 
      df_merged_static_plus_temporal = pd.merge(df_historical_nth_day, df_merged_static_plus_temporal, on = 'FIPS')
      df_merged_static_plus_temporal = df_merged_static_plus_temporal.drop('location_y', 1)
      df_merged_static_plus_temporal = df_merged_static_plus_temporal.rename(columns = {'location_x': 'location'})
    
    print('Shape of feature matrix ', df_merged_static_plus_temporal.shape)
    df_merged_static_plus_temporal_features_only = df_merged_static_plus_temporal.drop(['location', 'FIPS'], 1)
    features_list.append(df_merged_static_plus_temporal_features_only.values)
    #df_merged_static_plus_temporal.to_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Data/Location_feature_matrix_per_day/' + variable_name + str(dates_list_excluding_historical_dates[date_index])[0:10] +  '_feature_matrix.csv')
  

  # This will only return the dataframe of the most recent day
  return df_merged_static_plus_temporal, features_list
    # Save each day's feature matrix

df_merged_static_plus_temporal_last_day, features_list = make_feature_matrix(df_cleaned_covid_cases_vaccine_complete_data)

# Convert FIPS into a sequential list of integers and use them as IDs
df_merged_temporal_current['ID'] = range(len(df_merged_temporal_current))
df_FIPS_ID_mappping = df_merged_temporal_current[['FIPS', 'ID']]
df_FIPS_ID_mappping.to_csv('/content/gdrive/MyDrive/COVID_DGNN_KG/Data/Location_feature_matrix_per_day/FIPS_ID_mappping.csv')
df_merged_temporal_current = df_merged_temporal_current.drop(['location', 'FIPS', 'ID'], 1)

# Generate the list for y
y_list = []
# Reversed to get the earliest date to be appended to the list first
# minus 7 becasue we don't need the 7-day historical dates of the earliest date
for column_index in reversed(range(len(df_merged_temporal_current.columns) - 7)): 
  y_list.append(df_merged_temporal_current.iloc[:, column_index].values)