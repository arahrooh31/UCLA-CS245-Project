{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mobility_and_riskfactors_run_and_config_model.ipynb","provenance":[{"file_id":"1hhgjIRra3ao1Q9p6i3oODWBT3-cbc3tY","timestamp":1638838835314}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXyoT8z7cJ43","executionInfo":{"status":"ok","timestamp":1638851349746,"user_tz":480,"elapsed":1205,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}},"outputId":"a03a4d91-0f74-4fca-9dec-19047dc5c5f6"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"aRB23Rm05Pz-","executionInfo":{"status":"ok","timestamp":1638851349747,"user_tz":480,"elapsed":2,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["# Only run once. Prepares the locations' static features,  \n","#!python3 '/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data_prep.py'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUatuNQd_nkP","executionInfo":{"status":"ok","timestamp":1638851349747,"user_tz":480,"elapsed":2,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["import pandas as pd\n","import numpy as np\n","import json"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3apCnWjjFhB","executionInfo":{"status":"ok","timestamp":1638851351380,"user_tz":480,"elapsed":1635,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["fips_to_id = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/FIPS_ID_mappping.csv', usecols = ['FIPS','ID'])\n","f2i = {}\n","f2iK = list(fips_to_id['FIPS'])\n","f2iV = list(fips_to_id['ID'])\n","for i in range(0,len(f2iK)):\n","  f2i[f2iK[i]] = f2iV[i]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FeOlaELtjG-L","executionInfo":{"status":"ok","timestamp":1638851352512,"user_tz":480,"elapsed":1134,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["############################### CREATING GRAPHS ##############################################################\n","# Modeled after https://github.com/joey1993/pandemic-forecast/blob/main/pandemic-forecast/code/covid_graph_prep.py, https://github.com/joey1993/pandemic-forecast/blob/main/pandemic-forecast/code/utils.py\n","import pandas as pd\n","import json\n","import csv\n","import networkx as nx\n","\n","def pre_graphs():\n","    # (1) Edges: County-County 'borders' (county1, county2, weight=1)\n","    # Make bordering county prep file\n","    c_borders_file = json.load(open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countycodes_border.json', 'r'))\n","    c_borders = {int(k):v for k,v in c_borders_file.items()}\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for c1, c2 in c_borders.items():\n","        for C2 in c2:\n","          graph_writer.writerow([c1, C2, 1])\n","    borders_csv.close()\n","    c_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv', header=None)\n","\n","    # (2) Edges: State-State 'borders' (state1, state2, weight=1) ####\n","    # Make bordering state prep file\n","    state_fips = {}\n","    state_fipsDF = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS', 'State'])\n","    for row in state_fipsDF.iterrows(): state_fips[str(row[1][1])] = int(row[1][0])\n","    # Make dictionary of state:fips\n","    s_s_name = json.load(open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state-state_bordering.json', 'r')) # Undirected relationships\n","    # Make directed relationships with fips codes (translate using state_fips' state:id)\n","    s_s_fips = {} \n","    for pair in s_s_name:\n","        state1 = list(pair.keys())[0]\n","        state2 = list(pair.values())[0]\n","        fips1 = state_fips[state1]\n","        fips2 = state_fips[state2]\n","        if fips1 not in s_s_fips.keys(): s_s_fips[fips1] = []\n","        if fips2 not in s_s_fips.keys(): s_s_fips[fips2] = []\n","        s_s_fips[fips1].append(fips2)\n","        s_s_fips[fips2].append(fips1)\n","    s_borders = s_s_fips\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for s1, s2 in s_borders.items():\n","        for S2 in s2:\n","            graph_writer.writerow([s1, S2, 1])\n","    borders_csv.close()\n","    s_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv', header=None)\n","\n","\n","    # (3) Edges: County-State 'belongs to' (state, county, weight=1) ####\n","    # Creating dictionary of stateFIPS:[countyFIPS,...]\n","    sName_cFIPS = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_and_county_static_features.csv', usecols = ['FIPS', 'State'])\n","    sc_belongs = {}\n","    for row in sName_cFIPS.iterrows():\n","        stateName = row[1][1]\n","        stateFIPS = state_fips[stateName]\n","        countyFIPS = row[1][0]\n","        if stateFIPS not in sc_belongs.keys(): sc_belongs[stateFIPS] = []\n","        sc_belongs[stateFIPS].append(countyFIPS)\n","    # Creating a dataframe with rows of (stateFIPS, countyFIPS, 1)\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for s, c in sc_belongs.items():\n","        for C in c:\n","            graph_writer.writerow([s, C, 1])\n","    borders_csv.close()\n","    scb_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv', header=None)\n","\n","    #### Combined Edge Tables\n","    # 2 Edge Types: County-county 'borders', state-state 'borders' (loc1, loc2, weight=1)\n","    #sc_graph_prep1 = s_graph_prep\n","    #sc_graph_prep1 = sc_graph_prep1.append(c_graph_prep)\n","    # 3 Edge Types: County-county 'borders', state-state 'borders', state-county 'belongs' rows (loc1, loc2, weight=1)\n","    #sc_graph_prep2 = sc_graph_prep1.append(scb_graph_prep)\n","    \n","    return c_borders, s_borders, sc_belongs\n","\n","def pre_graphs_completedata():\n","    # (1) Edges: County-County 'borders' (county1, county2, weight=1)\n","    # Make bordering county prep file\n","    c_borders_file = json.load(open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/countycodes_border.json', 'r'))\n","    c_borders = {int(k):v for k,v in c_borders_file.items()}\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for c1, c2 in c_borders.items():\n","        for C2 in c2:\n","            if C2 in f2iK and c1 in f2iK:\n","                graph_writer.writerow([f2i[c1], f2i[C2], 1])\n","    borders_csv.close()\n","    c_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/counties_borders_csv_weight.csv', header=None)\n","\n","    # (2) Edges: State-State 'borders' (state1, state2, weight=1) ####\n","    # Make bordering state prep file\n","    state_fips = {}\n","    state_fipsDF = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS', 'State'])\n","    for row in state_fipsDF.iterrows(): state_fips[str(row[1][1])] = int(row[1][0])\n","    # Make dictionary of state:fips\n","    s_s_name = json.load(open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state-state_bordering.json', 'r')) # Undirected relationships\n","    # Make directed relationships with fips codes (translate using state_fips' state:id)\n","    s_s_fips = {} \n","    for pair in s_s_name:\n","        state1 = list(pair.keys())[0]\n","        state2 = list(pair.values())[0]\n","        fips1 = state_fips[state1]\n","        fips2 = state_fips[state2]\n","        if fips1 not in s_s_fips.keys(): s_s_fips[fips1] = []\n","        if fips2 not in s_s_fips.keys(): s_s_fips[fips2] = []\n","        s_s_fips[fips1].append(fips2)\n","        s_s_fips[fips2].append(fips1)\n","    s_borders = s_s_fips\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for s1, s2 in s_borders.items():\n","        for S2 in s2:\n","            if S2 in f2iK and s1 in f2iK:\n","                graph_writer.writerow([f2i[s1], f2i[S2], 1])\n","    borders_csv.close()\n","    s_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_borders_csv_weight.csv', header=None)\n","\n","\n","    # (3) Edges: County-State 'belongs to' (state, county, weight=1) ####\n","    # Creating dictionary of stateFIPS:[countyFIPS,...]\n","    sName_cFIPS = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/state_and_county_static_features.csv', usecols = ['FIPS', 'State'])\n","    sc_belongs = {}\n","    for row in sName_cFIPS.iterrows():\n","        stateName = row[1][1]\n","        stateFIPS = state_fips[stateName]\n","        countyFIPS = row[1][0]\n","        if stateFIPS not in sc_belongs.keys(): sc_belongs[stateFIPS] = []\n","        sc_belongs[stateFIPS].append(countyFIPS)\n","    # Creating a dataframe with rows of (stateFIPS, countyFIPS, 1)\n","    borders_csv = open('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv','w')\n","    graph_writer = csv.writer(borders_csv, delimiter=',', quotechar='\"')\n","    for s, c in sc_belongs.items():\n","        for C in c:\n","            if C in f2iK and s in f2iK:\n","              graph_writer.writerow([f2i[s], f2i[C], 1])\n","    borders_csv.close()\n","    scb_graph_prep = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/states_counties_belongs_csv_weight.csv', header=None)\n","\n","\n","    \n","    return c_borders, s_borders, sc_belongs\n","\n","\n","####### Graph Construction Functions\n","#### OPTION 1:  Graph of bordering counties\n","def create_tmp_graphs1(date_range):\n","    c_borders, s_borders, sc_belongs = pre_graphs()\n","    G = nx.Graph() # Undirected graph\n","    Gs = []\n","    #nodes = (len(c_borders))\n","    edges = []\n","    for k, v in c_borders.items():\n","        for V in v:\n","            edges.append((k,V))\n","    G.add_edges_from(edges)\n","    print(\"Total number of nodes: \", int(G.number_of_nodes()))\n","    print(\"Total number of edges: \", int(G.number_of_edges()))\n","    for date in date_range:\n","        Gs.append(G)\n","    return Gs\n","\n","#### OPTION 2: Graph of bordering counties, bordering states\n","def create_tmp_graphs2(date_range):\n","    c_borders, s_borders, sc_belongs = pre_graphs()\n","    G = nx.Graph() # Directed graph\n","    Gs = []\n","    #nodes = len(c_borders) + len(s_borders)\n","    edges = []\n","    for k, v in c_borders.items():\n","        for V in v:\n","            edges.append((k,V))\n","    for k, v in s_borders.items():\n","        for V in v:\n","            edges.append((k,V))\n","    G.add_edges_from(edges)\n","    print(\"Total number of nodes: \", int(G.number_of_nodes()))\n","    print(\"Total number of edges: \", int(G.number_of_edges()))\n","    for date in date_range:\n","        Gs.append(G)\n","    return Gs\n","\n","#### OPTION 3: Graph of bordering counties, bordering states, county-state edges\n","def create_tmp_graphs3(date_range):\n","    c_borders, s_borders, sc_belongs = pre_graphs()\n","    G = nx.DiGraph() # Directed graph\n","    Gs = []\n","    #nodes = len(c_borders) + len(s_borders)\n","    edges = []\n","    for c1, c2 in c_borders.items(): # county<->county 'borders'\n","        for C2 in c2:\n","            edges.append((c1,C2))\n","            edges.append((C2,c1))\n","    for s1, s2 in s_borders.items(): # state<->state 'borders'\n","        for S2 in s2:\n","            edges.append((s1,S2))\n","            edges.append((S2,s1))\n","    for s, c in sc_belongs.items(): # county->state 'belongs\n","        for C in c:\n","            edges.append((C,s))\n","\n","    G.add_edges_from(edges)\n","    print(\"Total number of nodes: \", int(G.number_of_nodes()))\n","    print(\"Total number of edges: \", int(G.number_of_edges()))\n","    for date in date_range:\n","        Gs.append(G)\n","    return Gs\n","\n","\n","#### OPTION 3b: Graph of bordering counties, bordering states, county-state edges, complete data only\n","def create_tmp_graphs3b(date_range, f2iK, f2i):\n","    c_borders, s_borders, sc_belongs = pre_graphs_completedata()\n","    G = nx.DiGraph() # Directed graph\n","    Gs = []\n","    #nodes = len(c_borders) + len(s_borders)\n","    edges = []\n","    for c1, c2 in c_borders.items(): # county<->county 'borders'\n","        for C2 in c2:\n","          if C2 in f2iK and c1 in f2iK:\n","              edges.append((f2i[c1],f2i[C2]))\n","              edges.append((f2i[C2],f2i[c1]))\n","    for s1, s2 in s_borders.items(): # state<->state 'borders'\n","        for S2 in s2:\n","            if S2 in f2iK and s1 in f2iK:\n","              edges.append((f2i[s1],f2i[S2]))\n","              edges.append((f2i[S2],f2i[s1]))\n","    for s, c in sc_belongs.items(): # county->state 'belongs\n","        for C in c:\n","            if s in f2iK and C in f2iK:\n","              edges.append((f2i[C],f2i[s]))\n","\n","    G.add_edges_from(edges)\n","    print(\"Total number of nodes: \", int(G.number_of_nodes()))\n","    print(\"Total number of edges: \", int(G.number_of_edges()))\n","    for date in date_range:\n","        Gs.append(G)\n","    return Gs\n","\n","#### OPTION 4b: Graph of bordering counties, bordering states, county-state edges, risk factors, complete data only\n","def create_tmp_graphs4b(dates, f2iK, f2i):\n","    c_borders, s_borders, sc_belongs = pre_graphs_completedata()\n","    Gs = []\n","    #nodes = len(c_borders) + len(s_borders)\n","    static_edges = []\n","    for c1, c2 in c_borders.items(): # county<->county 'borders'\n","        for C2 in c2:\n","          if C2 in f2iK and c1 in f2iK:\n","              static_edges.append((f2i[c1],f2i[C2]))\n","              static_edges.append((f2i[C2],f2i[c1]))\n","    for s1, s2 in s_borders.items(): # state<->state 'borders'\n","        for S2 in s2:\n","            if S2 in f2iK and s1 in f2iK:\n","              static_edges.append((f2i[s1],f2i[S2]))\n","              static_edges.append((f2i[S2],f2i[s1]))\n","    for s, c in sc_belongs.items(): # county->state 'belongs\n","        for C in c:\n","            if s in f2iK and C in f2iK:\n","              static_edges.append((f2i[C],f2i[s]))\n","    \n","    ### Add dynamic edges and nodes (weather risk factors for each date)\n","    state_fips = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS']) \n","    fips2fips = {}\n","    for i in range(len(state_fips)):   # Translates the 1-2 digit state FIPS to 4-5 digit FIPS \n","      fips2fips[np.int64(str(state_fips.iloc[i,0])[:-3])] = state_fips.iloc[i,0]\n","    # creates a graph for each date\n","    #print('Dates', dates)\n","    for d in dates.values():\n","      G = nx.DiGraph() # Directed graph\n","      dyn_edges = []\n","      try:\n","        df = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/risk_factors/{}.csv'.format(d))\n","        print(len(df))\n","        for i in range(len(df)):\n","          state = df.iloc[i,0]\n","          risk_factor = df.iloc[i,1]\n","          S = fips2fips[state]\n","          R = risk_factor\n","          if S in f2iK:\n","            dyn_edges.append((f2i[S], R-1))\n","            dyn_edges.append((R-1, f2i[S]))\n","      except:\n","        print('No risk factors for', d)\n","      G.add_edges_from(static_edges)\n","      G.add_edges_from(dyn_edges)      \n","      print(\"Total number of nodes: \", int(G.number_of_nodes()))\n","      print(\"Total number of edges: \", int(G.number_of_edges()))\n","      print(\"Total number of dynamic edges:\", len(dyn_edges))\n","      Gs.append(G)\n","\n","    return Gs\n","\n","#### OPTION 5b: Graph of bordering counties, bordering states, county-state edges, risk factors, mobility, complete data only\n","def create_tmp_graphs5b(dates, f2iK, f2i):\n","    c_borders, s_borders, sc_belongs = pre_graphs_completedata()\n","    Gs = []\n","    #nodes = len(c_borders) + len(s_borders)\n","    static_edges = []\n","    for c1, c2 in c_borders.items(): # county<->county 'borders'\n","        for C2 in c2:\n","          if C2 in f2iK and c1 in f2iK:\n","              static_edges.append((f2i[c1],f2i[C2]))\n","              static_edges.append((f2i[C2],f2i[c1]))\n","    for s1, s2 in s_borders.items(): # state<->state 'borders'\n","        for S2 in s2:\n","            if S2 in f2iK and s1 in f2iK:\n","              static_edges.append((f2i[s1],f2i[S2]))\n","              static_edges.append((f2i[S2],f2i[s1]))\n","    for s, c in sc_belongs.items(): # county->state 'belongs\n","        for C in c:\n","            if s in f2iK and C in f2iK:\n","              static_edges.append((f2i[C],f2i[s], 1))\n","    \n","    ### Add dynamic edges and nodes (weather risk factors for each date)\n","    state_fips = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/static_state_features.csv', usecols = ['FIPS']) \n","    fips2fips = {}\n","    for i in range(len(state_fips)):   # Translates the 1-2 digit state FIPS to 4-5 digit FIPS \n","      fips2fips[np.int64(str(state_fips.iloc[i,0])[:-3])] = state_fips.iloc[i,0]\n","    # creates a graph for each date\n","    #print('Dates', dates)\n","    for d in dates.values():\n","      G = nx.DiGraph() # Directed graph\n","      dyn_edges = []\n","      # Add risk factor nodes\n","      try:\n","        df = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/risk_factors/{}.csv'.format(d))\n","        print(len(df))\n","        for i in range(len(df)):\n","          state = df.iloc[i,0]\n","          risk_factor = df.iloc[i,1]\n","          S = fips2fips[state]\n","          R = risk_factor\n","          if S in f2iK:\n","            dyn_edges.append(f2i[S], R-1, weight=1)\n","            dyn_edges.append(R-1, f2i[S],weight=1)\n","      except:\n","        print('No risk factors for', d)\n","      # Add mobility edges\n","        try: mobility_df = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/{}-aggregation.csv'.format(d[:7]))\n","        except: mobility_df = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/{}-aggregation.csv'.format('2021-10'))\n","        for j in range(len(mobility_df)):\n","          fromFIPS = mobility_df.iloc[j,0]\n","          toFIPS = mobility_df.iloc[j,1]\n","          people = mobility_df.iloc[j,2]\n","          G.add_edge(f2i[fromFIPS], f2i[toFIPS], weight=people/100)\n","      G.add_edges_from(static_edges, weight=1)\n","      G.add_edges_from(dyn_edges, weight=1)      \n","      Gs.append(G)\n","\n","    return Gs"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtxU0YYJwS8j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638851422891,"user_tz":480,"elapsed":70381,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}},"outputId":"542caba5-54bb-4a2e-9ecf-872fe3338155"},"source":["\n","################################################# Features #################################################\n","\n","import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","\n","# Choose a range of date for which we want to generate the feature matrix \n","# The earliest start date allowed is 2021-01-01\n","start_date = '2021-08-01T00:00:00.000'\n","end_date = '2021-11-28T00:00:00.000'\n","\n","# Read in the dynamic and static data files\n","df_state_static = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/static_state_features.csv')\n","df_county_static = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/counties_static_features.csv')\n","df_covid_cases = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/CDC_COVID19_cases_and_vaccine_time_series/covid_cases_by_county.csv')\n","df_covid_vaccine = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/CDC_COVID19_cases_and_vaccine_time_series/vaccine_status_by_county.csv')\n","\n","# Delete the unnecessary indexing column\n","df_state_static = df_state_static.drop(df_state_static.columns[0], axis=1)\n","df_county_static = df_county_static.drop(df_county_static.columns[0], axis=1)\n","df_covid_cases = df_covid_cases.drop(df_covid_cases.columns[0], axis = 1)\n","\n","# For county data, we don't need the column that indicates the state\n","df_county_static = df_county_static.drop('State', 1)\n","\n","# Combine the state and county static features together into a single matrix\n","# The first 51 rows are state-wise features, the rest of the 3142 rows are county-wise features\n","# Both state and county have the same kind of features\n","df_state_static = df_state_static.rename({'State': 'location'}, axis='columns') \n","df_county_static = df_county_static.rename({'County': 'location'}, axis='columns')\n","df_state_and_county_static = pd.concat([df_state_static, df_county_static])\n","df_state_and_county_static = df_state_and_county_static.reset_index(drop = True)\n","df_state_and_county_static['location'] = df_state_and_county_static['location'].str.lower()\n","# Create a location name -- FIPS mapper in a dictionary\n","location_fips_mapping = dict(zip(df_state_and_county_static['location'][0:51], df_state_and_county_static['FIPS'][0:51]))\n","\n","\n","# For COVID cases data, drop any rows that have missing values for cases_per_100k_7_day_count or percent_test_results_reported\n","df_covid_cases['cases_per_100k_7_day_count'] = df_covid_cases['cases_per_100k_7_day_count'].replace('suppressed', np.nan)\n","\n","# Count number of missing data for each COVID variable\n","print('Number of missing for cases per 100k ', df_covid_cases['cases_per_100k_7_day_count'].isnull().sum())\n","print('Number of missing for percent_test ', df_covid_cases['percent_test_results_reported'].isnull().sum())\n","\n","df_covid_cases_no_missing = df_covid_cases.dropna(subset = ['cases_per_100k_7_day_count'])\n","df_covid_cases_no_missing = df_covid_cases_no_missing.reset_index(drop = True)\n","\n","# Clean the numbers by getting rid of the comma in the numbers\n","df_covid_cases_no_missing['cases_per_100k_7_day_count'] = df_covid_cases_no_missing.apply(lambda x: x['cases_per_100k_7_day_count'].replace(',', ''),axis=1)\n","df_covid_cases_no_missing['cases_per_100k_7_day_count'] = pd.to_numeric(df_covid_cases_no_missing['cases_per_100k_7_day_count'])\n","#df_covid_cases_no_missing['percent_test_results_reported'] = pd.to_numeric(df_covid_cases_no_missing['percent_test_results_reported'])\n","\n","# For vaccine data, drop any rows that have missing values for series_complete_pop_pct \n","df_covid_vaccine_no_missing = df_covid_vaccine.dropna(subset = ['series_complete_yes'])\n","df_covid_vaccine_no_missing = df_covid_vaccine_no_missing.reset_index(drop = True)\n","\n","print('Number of rows with missing values for COVID cases info from March 1 2020 to Nov 2021', len(df_covid_cases) - len(df_covid_cases_no_missing))\n","print('Number of rows with missing values for COVID vaccine info from March 1 2020 to Nov 2021', len(df_covid_vaccine) - len(df_covid_vaccine_no_missing))\n","\n","# Reference: https://gist.github.com/rogerallen/1583593\n","us_state_to_abbrev = {\n","    \"Alabama\": \"AL\",\n","    \"Alaska\": \"AK\",\n","    \"Arizona\": \"AZ\",\n","    \"Arkansas\": \"AR\",\n","    \"California\": \"CA\",\n","    \"Colorado\": \"CO\",\n","    \"Connecticut\": \"CT\",\n","    \"Delaware\": \"DE\",\n","    \"Florida\": \"FL\",\n","    \"Georgia\": \"GA\",\n","    \"Hawaii\": \"HI\",\n","    \"Idaho\": \"ID\",\n","    \"Illinois\": \"IL\",\n","    \"Indiana\": \"IN\",\n","    \"Iowa\": \"IA\",\n","    \"Kansas\": \"KS\",\n","    \"Kentucky\": \"KY\",\n","    \"Louisiana\": \"LA\",\n","    \"Maine\": \"ME\",\n","    \"Maryland\": \"MD\",\n","    \"Massachusetts\": \"MA\",\n","    \"Michigan\": \"MI\",\n","    \"Minnesota\": \"MN\",\n","    \"Mississippi\": \"MS\",\n","    \"Missouri\": \"MO\",\n","    \"Montana\": \"MT\",\n","    \"Nebraska\": \"NE\",\n","    \"Nevada\": \"NV\",\n","    \"New Hampshire\": \"NH\",\n","    \"New Jersey\": \"NJ\",\n","    \"New Mexico\": \"NM\",\n","    \"New York\": \"NY\",\n","    \"North Carolina\": \"NC\",\n","    \"North Dakota\": \"ND\",\n","    \"Ohio\": \"OH\",\n","    \"Oklahoma\": \"OK\",\n","    \"Oregon\": \"OR\",\n","    \"Pennsylvania\": \"PA\",\n","    \"Rhode Island\": \"RI\",\n","    \"South Carolina\": \"SC\",\n","    \"South Dakota\": \"SD\",\n","    \"Tennessee\": \"TN\",\n","    \"Texas\": \"TX\",\n","    \"Utah\": \"UT\",\n","    \"Vermont\": \"VT\",\n","    \"Virginia\": \"VA\",\n","    \"Washington\": \"WA\",\n","    \"West Virginia\": \"WV\",\n","    \"Wisconsin\": \"WI\",\n","    \"Wyoming\": \"WY\",\n","    \"Washington DC\": \"DC\",\n","    \"American Samoa\": \"AS\",\n","    \"Guam\": \"GU\",\n","    \"Northern Mariana Islands\": \"MP\",\n","    \"Puerto Rico\": \"PR\",\n","    \"United States Minor Outlying Islands\": \"UM\",\n","    \"U.S. Virgin Islands\": \"VI\",\n","}\n","\n","# Swap the key and value because we want to convert abbreviation to full name\n","us_abbrev_to_state = dict((v,k) for k,v in us_state_to_abbrev.items())\n","\n","# From the entire COVID dynamic data, take out the data within a desired date range\n","# Here sorting does not affect results. It was used for debugging purpose.\n","def select_data_by_dates(df_input):\n","  df_sorted = df_input.sort_values(['date', 'state_name'])\n","  df_sorted['date'] = pd.to_datetime(df_sorted['date'])\n","\n","  # Incorporate additional past 7 days from the starting day because we need to generate the historical 7-day COVID information later on.\n","  df_sorted = df_sorted[df_sorted['date'] >= pd.to_datetime(start_date) - timedelta(days = 7)]\n","  df_sorted = df_sorted[df_sorted['date'] <= pd.to_datetime(end_date)]\n","\n","  df_sorted = df_sorted.reset_index(drop = True)\n","  df = df_sorted\n","  return df\n","\n","# Rename some columns for vaccine data to be the same as the ones in the covid cases data so that later we can merge vaccine and cases data together\n","df_covid_vaccine_no_missing = df_covid_vaccine_no_missing.rename(columns= {'date': 'date', \n","                                                                           'fips': 'fips_code', 'recip_county': 'county_name', \n","                                                                           'recip_state': 'state_name'})\n","\n","df_cases_selected_by_dates = select_data_by_dates(df_covid_cases_no_missing)\n","df_vaccine_selected_by_dates = select_data_by_dates(df_covid_vaccine_no_missing)\n","\n","# Modify the location names of the covid cases dynamic data so that it is consistent with the static data\n","# Convert the state abbrev to full name for vaccine data\n","df_vaccine_selected_by_dates['state_name'] = df_vaccine_selected_by_dates['state_name'].map(us_abbrev_to_state)\n","df_vaccine_selected_by_dates['state_name'] = df_vaccine_selected_by_dates['state_name'].str.lower()\n","df_vaccine_selected_by_dates['county_name'] = df_vaccine_selected_by_dates['county_name'].str.lower()\n","# Somehow there are rows with FIPS code be \"UNK\". Need to drop these rows.\n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['fips_code'] != 'UNK']\n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates.reset_index(drop = True)\n","df_vaccine_selected_by_dates['fips_code'] = pd.to_numeric(df_vaccine_selected_by_dates['fips_code'])\n","\n","df_cases_selected_by_dates['state_name'] = df_cases_selected_by_dates['state_name'].str.lower()\n","df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].str.lower()\n","df_cases_selected_by_dates['state_name'] = df_cases_selected_by_dates['state_name'].replace('district of columbia', 'washington dc')\n","df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('district of columbia', 'washington dc')\n","df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('lasalle parish', 'la salle parish')\n","df_cases_selected_by_dates['county_name'] = df_cases_selected_by_dates['county_name'].replace('doña ana county', 'dona ana county')\n","\n","# Get rid of locations that don't exist in the static data \n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'american samoa'] \n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'northern mariana islands'] \n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'guam'] \n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'puerto rico'] \n","df_vaccine_selected_by_dates = df_vaccine_selected_by_dates[df_vaccine_selected_by_dates['state_name'] != 'u.s. virgin islands'] \n","\n","\n","df_cases_selected_by_dates = df_cases_selected_by_dates[df_cases_selected_by_dates['state_name'] != 'puerto rico'] \n","df_cases_selected_by_dates = df_cases_selected_by_dates.reset_index(drop = True)\n","\n","# Store the dates in a list so that later we can loop through each date and create relevant data for each date.\n","dates_list = df_cases_selected_by_dates['date'].drop_duplicates()\n","dates_list = dates_list.reset_index(drop = True)\n","\n","def create_cleaned_df_for_dynamic_features(dates_list, df_input, variable_name):\n","  print('For variable ', variable_name)\n","  results_across_days = []\n","  for date_index in range(len(dates_list)):\n","    # Process each day separately \n","    date = dates_list[date_index]\n","    groups_by_date = df_input.groupby('date')\n","    print('Processing ', date)\n","\n","    # Find the rows/locations corresponding to the same date\n","    df_data_with_same_date = groups_by_date.get_group(date)\n","    df_data_with_same_date = df_data_with_same_date.reset_index(drop = True)\n","\n","    # Calculate state-level COVID numbers by aggregating across the counties in that state\n","    # Within the rows with the same date, find the ones with the same US state name\n","    groups_by_state_indices = df_data_with_same_date.groupby('state_name').indices\n","    for key, value in groups_by_state_indices.items():\n","      state_name = key\n","      # For each state, get its COVID data by summing over all its counties \n","      FIPS_of_location = location_fips_mapping[state_name]\n","      state_level_result = sum(df_data_with_same_date[variable_name][value])\n","      results_across_days.append((date, state_name, FIPS_of_location, state_level_result))\n","\n","    # After getting the state-level data, we simply save the county level COVID data \n","    for county_index in range(len(df_data_with_same_date)): \n","      county_name = df_data_with_same_date['county_name'][county_index]\n","      county_FIPS = df_data_with_same_date['fips_code'][county_index]\n","      county_level_result = df_data_with_same_date[variable_name][county_index]\n","      results_across_days.append((date, county_name, county_FIPS, county_level_result))\n","\n","  # Save as a dataframe\n","  df_results = pd.DataFrame(results_across_days, columns = ['date', 'location', 'FIPS', variable_name])\n","  # Convert the date type into datetime so that the dates can be calculated later\n","  df_results['date'] = pd.to_datetime(df_results['date'])\n","  print('Number of resulting rows ', len(df_results))\n","  return df_results\n","\n","df_cleaned_covid_cases = create_cleaned_df_for_dynamic_features(dates_list, df_cases_selected_by_dates, 'cases_per_100k_7_day_count')\n","df_cleaned_covid_vaccine = create_cleaned_df_for_dynamic_features(dates_list, df_vaccine_selected_by_dates, 'series_complete_yes')\n","\n","\n","variable_name = 'cases_per_100k_7_day_count'\n","grouped_results = df_cleaned_covid_cases.groupby('date')\n","df_merged_temporal_current = grouped_results.get_group(dates_list[0])\n","for date_index in range(1, len(dates_list)):\n","  df_nth_day = grouped_results.get_group(dates_list[date_index])\n","  # Some preprocessing on column names to make the merging easier\n","  df_nth_day = df_nth_day.drop('date', 1)\n","  df_nth_day = df_nth_day.reset_index(drop = True)\n","  df_nth_day = df_nth_day.rename(columns = {variable_name: str(dates_list[date_index])[0:10]})\n","\n","  # Merging \n","  df_merged_temporal = pd.merge(df_nth_day, df_merged_temporal_current, on = 'FIPS')\n","  df_merged_temporal = df_merged_temporal.drop('location_y', 1)\n","  df_merged_temporal = df_merged_temporal.rename(columns = {'location_x': 'location'})\n","  df_merged_temporal_current = df_merged_temporal\n","  \n","df_merged_temporal_current = df_merged_temporal_current.drop('date', 1)\n","df_merged_temporal_current = df_merged_temporal_current.rename(columns = {'cases_per_100k_7_day_count' : str(dates_list[0])[0:10]})\n","df_merged_temporal_current = df_merged_temporal_current.sort_values(by = 'FIPS')\n","df_merged_temporal_current = df_merged_temporal_current.reset_index(drop = True)\n","\n","\n","# When making the feature matrix, we only use the locations that have complete data\n","locations_to_keep = df_merged_temporal_current['FIPS'].tolist()\n","df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases.query('FIPS in @locations_to_keep').copy()\n","df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine.query('FIPS in @locations_to_keep').copy()\n","df_state_and_county_static_complete_data = df_state_and_county_static.query('FIPS in @locations_to_keep').copy()\n","\n","df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases_complete_data.sort_values(by = ['date', 'FIPS'])\n","df_cleaned_covid_cases_complete_data = df_cleaned_covid_cases_complete_data.reset_index(drop = True)\n","df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine_complete_data.sort_values(by = ['date', 'FIPS'])\n","df_cleaned_covid_vaccine_complete_data = df_cleaned_covid_vaccine_complete_data.reset_index(drop = True)\n","df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.sort_values(by = 'FIPS')\n","df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.reset_index(drop = True)\n","df_cleaned_covid_cases_vaccine_complete_data = df_cleaned_covid_cases_complete_data\n","df_cleaned_covid_cases_vaccine_complete_data.insert(3, 'series_complete_yes', df_cleaned_covid_vaccine_complete_data['series_complete_yes'].values)\n","\n","\n","# Clean the static data\n","# Delete the static features that have more than 50 locations having missing values for it\n","df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.dropna(thresh = (len(df_state_and_county_static_complete_data) - 75), axis = 1)\n","\n","# For other features, use average feature column value to impute the missing values\n","df_state_and_county_static_complete_data = df_state_and_county_static_complete_data.fillna(df_state_and_county_static_complete_data.mean())\n","\n","# Check which features are deleted\n","set(df_state_and_county_static.columns) - set(df_state_and_county_static_complete_data.columns)\n","\n","# Store the dates in a list so that later we can loop through each date and create feature matrix for each date.\n","# The starting date is the user-specified starting date, therefore we won't include the historical dates of the starting date.\n","def make_feature_matrix(df_input):\n","  dates_list_excluding_historical_dates = df_input['date'].drop_duplicates()[df_input['date'] >= (df_input['date'].min() + timedelta(days = 7))]\n","  dates_list_excluding_historical_dates = dates_list_excluding_historical_dates.reset_index(drop = True)\n","\n","  # Create the feature matrix that contains the previous 7 days' covid information plus the static features \n","  grouped_results = df_input.groupby('date')\n","  n_days_before_today = [1, 2, 3, 4, 5, 6, 7]\n","  features_list = []\n","  for date_index in range(len(dates_list_excluding_historical_dates)):\n","    df_merged_static_plus_temporal = df_state_and_county_static_complete_data\n","    print('Processing ', dates_list_excluding_historical_dates[date_index])\n","\n","    # Iteratively retrieve the past Nth day's COVID data and merge it into the current dataframe\n","    for t in range(len(n_days_before_today)):\n","      # Retrieve the past Nth day's COVID data\n","      n = n_days_before_today[t]\n","      historical_date = dates_list_excluding_historical_dates[date_index] - timedelta(days = n)\n","      df_historical_nth_day = grouped_results.get_group(historical_date)\n","      # Some preprocessing on column names to make the merging easier\n","      df_historical_nth_day = df_historical_nth_day.drop('date', 1)\n","      df_historical_nth_day = df_historical_nth_day.sort_values(by = 'FIPS')\n","      df_historical_nth_day = df_historical_nth_day.reset_index(drop = True)\n","      # Calculate the vaccine data by dividing the raw numbers by population in that location.\n","      df_historical_nth_day['series_complete_yes_normed'] = df_historical_nth_day['series_complete_yes'] / df_state_and_county_static_complete_data['POPESTIMATE']\n","      df_historical_nth_day = df_historical_nth_day.drop('series_complete_yes', 1)\n","      df_historical_nth_day = df_historical_nth_day.rename(columns = {'cases_per_100k_7_day_count': 'cases_per_100k_7_day_count_' + str(n) + '_days_before_current_day'})\n","      df_historical_nth_day = df_historical_nth_day.rename(columns = {'series_complete_yes_normed': 'series_complete_yes_normed_' + str(n) + '_days_before_current_day'})\n","      \n","\n","      # Merging \n","      df_merged_static_plus_temporal = pd.merge(df_historical_nth_day, df_merged_static_plus_temporal, on = 'FIPS')\n","      df_merged_static_plus_temporal = df_merged_static_plus_temporal.drop('location_y', 1)\n","      df_merged_static_plus_temporal = df_merged_static_plus_temporal.rename(columns = {'location_x': 'location'})\n","    \n","    print('Shape of feature matrix ', df_merged_static_plus_temporal.shape)\n","    df_merged_static_plus_temporal_features_only = df_merged_static_plus_temporal.drop(['location', 'FIPS'], 1)\n","\n","    # Append 10 rows of zeros at the end of feature matrix\n","    for i in range(10):\n","      df_merged_static_plus_temporal_features_only.loc[len(df_merged_static_plus_temporal_features_only)] = 0\n","    features_list.append(df_merged_static_plus_temporal_features_only.values)\n","    #df_merged_static_plus_temporal.to_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Data/Location_feature_matrix_per_day/' + variable_name + str(dates_list_excluding_historical_dates[date_index])[0:10] +  '_feature_matrix.csv')\n","  \n","\n","  # This will only return the dataframe of the most recent day\n","  return df_merged_static_plus_temporal, features_list\n","    # Save each day's feature matrix\n","\n","df_merged_static_plus_temporal_last_day, features_list = make_feature_matrix(df_cleaned_covid_cases_vaccine_complete_data)\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of missing for cases per 100k  471771\n","Number of missing for percent_test  170123\n","Number of rows with missing values for COVID cases info from March 1 2020 to Nov 2021 471771\n","Number of rows with missing values for COVID vaccine info from March 1 2020 to Nov 2021 0\n","For variable  cases_per_100k_7_day_count\n","Processing  2021-07-25 00:00:00\n","Processing  2021-07-26 00:00:00\n","Processing  2021-07-27 00:00:00\n","Processing  2021-07-28 00:00:00\n","Processing  2021-07-29 00:00:00\n","Processing  2021-07-30 00:00:00\n","Processing  2021-07-31 00:00:00\n","Processing  2021-08-01 00:00:00\n","Processing  2021-08-02 00:00:00\n","Processing  2021-08-03 00:00:00\n","Processing  2021-08-04 00:00:00\n","Processing  2021-08-05 00:00:00\n","Processing  2021-08-06 00:00:00\n","Processing  2021-08-07 00:00:00\n","Processing  2021-08-08 00:00:00\n","Processing  2021-08-09 00:00:00\n","Processing  2021-08-10 00:00:00\n","Processing  2021-08-11 00:00:00\n","Processing  2021-08-12 00:00:00\n","Processing  2021-08-13 00:00:00\n","Processing  2021-08-14 00:00:00\n","Processing  2021-08-15 00:00:00\n","Processing  2021-08-16 00:00:00\n","Processing  2021-08-17 00:00:00\n","Processing  2021-08-18 00:00:00\n","Processing  2021-08-19 00:00:00\n","Processing  2021-08-20 00:00:00\n","Processing  2021-08-21 00:00:00\n","Processing  2021-08-22 00:00:00\n","Processing  2021-08-23 00:00:00\n","Processing  2021-08-24 00:00:00\n","Processing  2021-08-25 00:00:00\n","Processing  2021-08-26 00:00:00\n","Processing  2021-08-27 00:00:00\n","Processing  2021-08-28 00:00:00\n","Processing  2021-08-29 00:00:00\n","Processing  2021-08-30 00:00:00\n","Processing  2021-08-31 00:00:00\n","Processing  2021-09-01 00:00:00\n","Processing  2021-09-02 00:00:00\n","Processing  2021-09-03 00:00:00\n","Processing  2021-09-04 00:00:00\n","Processing  2021-09-05 00:00:00\n","Processing  2021-09-06 00:00:00\n","Processing  2021-09-07 00:00:00\n","Processing  2021-09-08 00:00:00\n","Processing  2021-09-09 00:00:00\n","Processing  2021-09-10 00:00:00\n","Processing  2021-09-11 00:00:00\n","Processing  2021-09-12 00:00:00\n","Processing  2021-09-13 00:00:00\n","Processing  2021-09-14 00:00:00\n","Processing  2021-09-15 00:00:00\n","Processing  2021-09-16 00:00:00\n","Processing  2021-09-17 00:00:00\n","Processing  2021-09-18 00:00:00\n","Processing  2021-09-19 00:00:00\n","Processing  2021-09-20 00:00:00\n","Processing  2021-09-21 00:00:00\n","Processing  2021-09-22 00:00:00\n","Processing  2021-09-23 00:00:00\n","Processing  2021-09-24 00:00:00\n","Processing  2021-09-25 00:00:00\n","Processing  2021-09-26 00:00:00\n","Processing  2021-09-27 00:00:00\n","Processing  2021-09-28 00:00:00\n","Processing  2021-09-29 00:00:00\n","Processing  2021-09-30 00:00:00\n","Processing  2021-10-01 00:00:00\n","Processing  2021-10-02 00:00:00\n","Processing  2021-10-03 00:00:00\n","Processing  2021-10-04 00:00:00\n","Processing  2021-10-05 00:00:00\n","Processing  2021-10-06 00:00:00\n","Processing  2021-10-07 00:00:00\n","Processing  2021-10-08 00:00:00\n","Processing  2021-10-09 00:00:00\n","Processing  2021-10-10 00:00:00\n","Processing  2021-10-11 00:00:00\n","Processing  2021-10-12 00:00:00\n","Processing  2021-10-13 00:00:00\n","Processing  2021-10-14 00:00:00\n","Processing  2021-10-15 00:00:00\n","Processing  2021-10-16 00:00:00\n","Processing  2021-10-17 00:00:00\n","Processing  2021-10-18 00:00:00\n","Processing  2021-10-19 00:00:00\n","Processing  2021-10-20 00:00:00\n","Processing  2021-10-21 00:00:00\n","Processing  2021-10-22 00:00:00\n","Processing  2021-10-23 00:00:00\n","Processing  2021-10-24 00:00:00\n","Processing  2021-10-25 00:00:00\n","Processing  2021-10-26 00:00:00\n","Processing  2021-10-27 00:00:00\n","Processing  2021-10-28 00:00:00\n","Processing  2021-10-29 00:00:00\n","Processing  2021-10-30 00:00:00\n","Processing  2021-10-31 00:00:00\n","Processing  2021-11-01 00:00:00\n","Processing  2021-11-02 00:00:00\n","Processing  2021-11-03 00:00:00\n","Processing  2021-11-04 00:00:00\n","Processing  2021-11-05 00:00:00\n","Processing  2021-11-06 00:00:00\n","Processing  2021-11-07 00:00:00\n","Processing  2021-11-08 00:00:00\n","Processing  2021-11-09 00:00:00\n","Processing  2021-11-10 00:00:00\n","Processing  2021-11-11 00:00:00\n","Processing  2021-11-12 00:00:00\n","Processing  2021-11-13 00:00:00\n","Processing  2021-11-14 00:00:00\n","Processing  2021-11-15 00:00:00\n","Processing  2021-11-16 00:00:00\n","Processing  2021-11-17 00:00:00\n","Processing  2021-11-18 00:00:00\n","Processing  2021-11-19 00:00:00\n","Processing  2021-11-20 00:00:00\n","Processing  2021-11-21 00:00:00\n","Processing  2021-11-22 00:00:00\n","Processing  2021-11-23 00:00:00\n","Processing  2021-11-24 00:00:00\n","Processing  2021-11-25 00:00:00\n","Processing  2021-11-26 00:00:00\n","Processing  2021-11-27 00:00:00\n","Processing  2021-11-28 00:00:00\n","Number of resulting rows  352578\n","For variable  series_complete_yes\n","Processing  2021-07-25 00:00:00\n","Processing  2021-07-26 00:00:00\n","Processing  2021-07-27 00:00:00\n","Processing  2021-07-28 00:00:00\n","Processing  2021-07-29 00:00:00\n","Processing  2021-07-30 00:00:00\n","Processing  2021-07-31 00:00:00\n","Processing  2021-08-01 00:00:00\n","Processing  2021-08-02 00:00:00\n","Processing  2021-08-03 00:00:00\n","Processing  2021-08-04 00:00:00\n","Processing  2021-08-05 00:00:00\n","Processing  2021-08-06 00:00:00\n","Processing  2021-08-07 00:00:00\n","Processing  2021-08-08 00:00:00\n","Processing  2021-08-09 00:00:00\n","Processing  2021-08-10 00:00:00\n","Processing  2021-08-11 00:00:00\n","Processing  2021-08-12 00:00:00\n","Processing  2021-08-13 00:00:00\n","Processing  2021-08-14 00:00:00\n","Processing  2021-08-15 00:00:00\n","Processing  2021-08-16 00:00:00\n","Processing  2021-08-17 00:00:00\n","Processing  2021-08-18 00:00:00\n","Processing  2021-08-19 00:00:00\n","Processing  2021-08-20 00:00:00\n","Processing  2021-08-21 00:00:00\n","Processing  2021-08-22 00:00:00\n","Processing  2021-08-23 00:00:00\n","Processing  2021-08-24 00:00:00\n","Processing  2021-08-25 00:00:00\n","Processing  2021-08-26 00:00:00\n","Processing  2021-08-27 00:00:00\n","Processing  2021-08-28 00:00:00\n","Processing  2021-08-29 00:00:00\n","Processing  2021-08-30 00:00:00\n","Processing  2021-08-31 00:00:00\n","Processing  2021-09-01 00:00:00\n","Processing  2021-09-02 00:00:00\n","Processing  2021-09-03 00:00:00\n","Processing  2021-09-04 00:00:00\n","Processing  2021-09-05 00:00:00\n","Processing  2021-09-06 00:00:00\n","Processing  2021-09-07 00:00:00\n","Processing  2021-09-08 00:00:00\n","Processing  2021-09-09 00:00:00\n","Processing  2021-09-10 00:00:00\n","Processing  2021-09-11 00:00:00\n","Processing  2021-09-12 00:00:00\n","Processing  2021-09-13 00:00:00\n","Processing  2021-09-14 00:00:00\n","Processing  2021-09-15 00:00:00\n","Processing  2021-09-16 00:00:00\n","Processing  2021-09-17 00:00:00\n","Processing  2021-09-18 00:00:00\n","Processing  2021-09-19 00:00:00\n","Processing  2021-09-20 00:00:00\n","Processing  2021-09-21 00:00:00\n","Processing  2021-09-22 00:00:00\n","Processing  2021-09-23 00:00:00\n","Processing  2021-09-24 00:00:00\n","Processing  2021-09-25 00:00:00\n","Processing  2021-09-26 00:00:00\n","Processing  2021-09-27 00:00:00\n","Processing  2021-09-28 00:00:00\n","Processing  2021-09-29 00:00:00\n","Processing  2021-09-30 00:00:00\n","Processing  2021-10-01 00:00:00\n","Processing  2021-10-02 00:00:00\n","Processing  2021-10-03 00:00:00\n","Processing  2021-10-04 00:00:00\n","Processing  2021-10-05 00:00:00\n","Processing  2021-10-06 00:00:00\n","Processing  2021-10-07 00:00:00\n","Processing  2021-10-08 00:00:00\n","Processing  2021-10-09 00:00:00\n","Processing  2021-10-10 00:00:00\n","Processing  2021-10-11 00:00:00\n","Processing  2021-10-12 00:00:00\n","Processing  2021-10-13 00:00:00\n","Processing  2021-10-14 00:00:00\n","Processing  2021-10-15 00:00:00\n","Processing  2021-10-16 00:00:00\n","Processing  2021-10-17 00:00:00\n","Processing  2021-10-18 00:00:00\n","Processing  2021-10-19 00:00:00\n","Processing  2021-10-20 00:00:00\n","Processing  2021-10-21 00:00:00\n","Processing  2021-10-22 00:00:00\n","Processing  2021-10-23 00:00:00\n","Processing  2021-10-24 00:00:00\n","Processing  2021-10-25 00:00:00\n","Processing  2021-10-26 00:00:00\n","Processing  2021-10-27 00:00:00\n","Processing  2021-10-28 00:00:00\n","Processing  2021-10-29 00:00:00\n","Processing  2021-10-30 00:00:00\n","Processing  2021-10-31 00:00:00\n","Processing  2021-11-01 00:00:00\n","Processing  2021-11-02 00:00:00\n","Processing  2021-11-03 00:00:00\n","Processing  2021-11-04 00:00:00\n","Processing  2021-11-05 00:00:00\n","Processing  2021-11-06 00:00:00\n","Processing  2021-11-07 00:00:00\n","Processing  2021-11-08 00:00:00\n","Processing  2021-11-09 00:00:00\n","Processing  2021-11-10 00:00:00\n","Processing  2021-11-11 00:00:00\n","Processing  2021-11-12 00:00:00\n","Processing  2021-11-13 00:00:00\n","Processing  2021-11-14 00:00:00\n","Processing  2021-11-15 00:00:00\n","Processing  2021-11-16 00:00:00\n","Processing  2021-11-17 00:00:00\n","Processing  2021-11-18 00:00:00\n","Processing  2021-11-19 00:00:00\n","Processing  2021-11-20 00:00:00\n","Processing  2021-11-21 00:00:00\n","Processing  2021-11-22 00:00:00\n","Processing  2021-11-23 00:00:00\n","Processing  2021-11-24 00:00:00\n","Processing  2021-11-25 00:00:00\n","Processing  2021-11-26 00:00:00\n","Processing  2021-11-27 00:00:00\n","Processing  2021-11-28 00:00:00\n","Number of resulting rows  405511\n","Processing  2021-08-01 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-02 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-03 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-04 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-05 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-06 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-07 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-08 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-09 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-10 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-11 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-12 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-13 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-14 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-15 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-16 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-17 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-18 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-19 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-20 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-21 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-22 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-23 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-24 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-25 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-26 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-27 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-28 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-29 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-30 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-08-31 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-01 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-02 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-03 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-04 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-05 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-06 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-07 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-08 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-09 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-10 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-11 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-12 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-13 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-14 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-15 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-16 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-17 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-18 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-19 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-20 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-21 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-22 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-23 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-24 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-25 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-26 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-27 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-28 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-29 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-09-30 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-01 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-02 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-03 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-04 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-05 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-06 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-07 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-08 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-09 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-10 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-11 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-12 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-13 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-14 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-15 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-16 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-17 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-18 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-19 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-20 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-21 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-22 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-23 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-24 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-25 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-26 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-27 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-28 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-29 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-30 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-10-31 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-01 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-02 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-03 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-04 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-05 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-06 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-07 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-08 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-09 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-10 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-11 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-12 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-13 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-14 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-15 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-16 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-17 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-18 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-19 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-20 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-21 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-22 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-23 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-24 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-25 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-26 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-27 00:00:00\n","Shape of feature matrix  (1519, 99)\n","Processing  2021-11-28 00:00:00\n","Shape of feature matrix  (1519, 99)\n"]}]},{"cell_type":"code","metadata":{"id":"hxQMTueldMGo","executionInfo":{"status":"ok","timestamp":1638851423700,"user_tz":480,"elapsed":822,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["# Convert FIPS into a sequential list of integers and use them as IDs\n","df_merged_temporal_current['ID'] = range(len(df_merged_temporal_current))\n","df_FIPS_ID_mappping = df_merged_temporal_current[['FIPS', 'ID']]\n","df_FIPS_ID_mappping.to_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Data/Location_feature_matrix_per_day/FIPS_ID_mappping.csv')\n","df_merged_temporal_current = df_merged_temporal_current.drop(['location', 'FIPS', 'ID'], 1)\n","\n","# Generate the list for y\n","y_list = []\n","# Reversed to get the earliest date to be appended to the list first\n","# minus 7 becasue we don't need the 7-day historical dates of the earliest date\n","for column_index in reversed(range(len(df_merged_temporal_current.columns) - 7)): \n","  y_list.append(df_merged_temporal_current.iloc[:, column_index].values)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NjdEDl-Pmy_","executionInfo":{"status":"ok","timestamp":1638851423703,"user_tz":480,"elapsed":10,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["import os, sys\n","from google.colab import drive"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"CB7lLU-B_u5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638854413830,"user_tz":480,"elapsed":2990135,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}},"outputId":"95860f08-72df-42b0-8d36-3a4c92917a87"},"source":["#!pip install torch-scatter\n","#!pip install torch-sparse\n","!pip install torch-geometric\n","\n","!pip install transformers\n","!pip install datasets\n","!pip install seqeval\n","\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","!pip install torch-geometric --user"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch-geometric\n","  Downloading torch_geometric-2.0.2.tar.gz (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n","Collecting rdflib\n","  Downloading rdflib-6.0.2-py3-none-any.whl (407 kB)\n","\u001b[K     |████████████████████████████████| 407 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n","Collecting yacs\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Collecting isodate\n","  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n","\u001b[K     |████████████████████████████████| 45 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.0.2-py3-none-any.whl size=535570 sha256=d5110ac4897aebe8bd039b9d0b7e0b8551c917bf8fecac00044de12bbbf03ceb\n","  Stored in directory: /root/.cache/pip/wheels/3f/08/13/2321517088bb2e95bfd0e45033bb9c923189e5b2078e0be4ef\n","Successfully built torch-geometric\n","Installing collected packages: isodate, yacs, rdflib, torch-geometric\n","Successfully installed isodate-0.6.0 rdflib-6.0.2 torch-geometric-2.0.2 yacs-0.1.8\n","Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 53.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 54.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 650 kB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 6.8 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 62.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 68.8 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 69.2 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 60.2 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 64.1 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 74.7 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=d7cdb185201a4aeef1440572f79c44fe20efb63e99efbdb4c03fc2529d7ef83e\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","Collecting torch-scatter\n","  Downloading torch_scatter-2.0.9.tar.gz (21 kB)\n","Building wheels for collected packages: torch-scatter\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl size=3874185 sha256=5b7875d94d7efd7adaa744bf5fafeb55a43aa0ca0569f8866317f17af03aa331\n","  Stored in directory: /root/.cache/pip/wheels/dd/57/a3/42ea193b77378ce634eb9454c9bc1e3163f3b482a35cdee4d1\n","Successfully built torch-scatter\n","Installing collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","Collecting torch-sparse\n","  Downloading torch_sparse-0.6.12.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n","Building wheels for collected packages: torch-sparse\n","  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-sparse: filename=torch_sparse-0.6.12-cp37-cp37m-linux_x86_64.whl size=1664876 sha256=fdaf06cbf6852d07adb597ba6c8b92e0c978a48f50297edc6fb4956e4d4fa50c\n","  Stored in directory: /root/.cache/pip/wheels/fb/e2/2f/44956c61e3299573ffe12da9d1374c7576ca0c5fb1fe1ed38c\n","Successfully built torch-sparse\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.12\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","Collecting torch-cluster\n","  Downloading torch_cluster-1.5.9.tar.gz (38 kB)\n","Building wheels for collected packages: torch-cluster\n","  Building wheel for torch-cluster (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-cluster: filename=torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl size=920354 sha256=b1c4cdc9bb25f7e4584d6b2ab52a51e8265885bcc139d263e2f56f41897c3185\n","  Stored in directory: /root/.cache/pip/wheels/a6/60/d8/8bb27f58d8578ba8046f7ea0aadbae89a731db884a644ba361\n","Successfully built torch-cluster\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.5.9\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu111.html\n","Collecting torch-spline-conv\n","  Downloading torch_spline_conv-1.2.1.tar.gz (13 kB)\n","Building wheels for collected packages: torch-spline-conv\n","  Building wheel for torch-spline-conv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-spline-conv: filename=torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl size=400785 sha256=480dca53d4211cbe0aff5ea7487af250e223d71fbe0da30ed5ff99b612823bf8\n","  Stored in directory: /root/.cache/pip/wheels/9c/33/73/780370b7c7bdf2340c0a7b971e915643f14795b4caa7a9a31f\n","Successfully built torch-spline-conv\n","Installing collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.1\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n","Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n","Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n","Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"id":"415oJrdI3MlV","executionInfo":{"status":"ok","timestamp":1638854413832,"user_tz":480,"elapsed":41,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)\n","\n","def generate_new_batches(Gs, features, y, idx, graph_window, shift, batch_size, device, test_sample):\n","    N = len(idx)\n","  \n","    adj_lst = list()\n","    features_lst = list()\n","    y_lst = list()\n","    node_lst = list()\n","\n","    batch_data = []\n","    for i in range(0, N, batch_size):\n","        # For the last batch in the date range, add a batch (list of days between i and N, there are less than 8 days between i and N here)\n","        if i+batch_size >= N:\n","            batch_data += [[idx[x] for x in range(i, N)]]\n","        # Add a batch (list of days between i and i+8)\n","        else:\n","            batch_data += [[idx[x] for x in range(i, i+batch_size)]]\n","    # For each batch (i.e. a list of 8 days) in a list of batches e.g., [2, 4, 6, 8, 10, 12, 14, 16]\n","    for batch in batch_data:\n","        adj_tmp = list(); features_tmp = list();   y_tmp = list(); num_tmp = list()\n","        line_idx = 0\n","\n","        # For each day in the batch\n","        for curr_day in batch:\n","            # For each day in the past 7 days prior to the batch\n","            for day_in_window in range(curr_day-graph_window+1,curr_day+1):\n","                adj_tmp.append(nx.adjacency_matrix(Gs[day_in_window-1]).toarray())          \n","                for nodes_row_of_feat in features[day_in_window]: # for row in 1519 rows x 97 col\n","                    features_tmp.append(nodes_row_of_feat)  # Append features from past 7 days\n","                line_idx += len(features[day_in_window])\n","            y_tmp.append(y[curr_day+shift])\n","        \n","        adj_tmp = sparse_mx_to_torch_sparse_tensor(sp.block_diag(adj_tmp))    # tensor of past 7 days' graphs (35, 1519, 1519) --> (53165, 53165)         i.e (window*daysinbatch, nodes, nodes) or 35  graphs \n","        adj_lst.append(adj_tmp.to(device))                                    # list of tensors of past 7 days' graphs\n","        features_tmp = torch.FloatTensor(features_tmp)                        # tensor of past 7 days' fatures, (nodes*window*days_in_batch, features)    i.e. (53165, 97) or (7*5)\n","        features_lst.append(features_tmp.to(device))                          # a list of tensors of the past 7 days' features\n","        y_tmp = torch.FloatTensor(y_tmp).reshape(-1)                          # a 1D tensor of true values for 'shift' days ahead\n","        y_lst.append(y_tmp.to(device))                                        # a list of 1D tensors of true values for 'shift' days ahead\n","\n","    return adj_lst, features_lst, y_lst, 0\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AEi9Iz507jZ","executionInfo":{"status":"ok","timestamp":1638854414765,"user_tz":480,"elapsed":959,"user":{"displayName":"Scott Mueller","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiqCCkxS9Iz-cRjM8BJuP8b4x4zGg_zR96nGsJomg=s64","userId":"10851035312058159985"}}},"source":["\n","from numpy.lib.arraysetops import ediff1d\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch_geometric.nn import GCNConv,GATConv\n","import networkx as nx\n","import numpy as np\n","import scipy.sparse as sp\n","import os\n","from tqdm import tqdm\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","class DGNN(nn.Module):\n","    def __init__(self, nfeat, nhid, nout, n_nodes, window, dropout, nhist=7):\n","        super(DGNN, self).__init__()\n","\n","        self.window = window\n","        self.n_nodes = n_nodes\n","        self.n_state = 1519\n","        self.nhid = nhid\n","        self.nfeat = nfeat\n","        self.nhist = nhist\n","        self.conv1 = GATConv(nfeat, nhid, concat=False)\n","        self.conv2 = GATConv(nhid, nhid, concat=False)\n","        self.bn1 = nn.BatchNorm1d(nhid)\n","        self.bn2 = nn.BatchNorm1d(nhid)\n","        self.rnn1 = nn.GRU(2*nhid, nhid, 1)\n","        self.rnn2 = nn.GRU(nhid, nhid, 1)\n","        self.fc1 = nn.Linear(2*nhid+window*nhist, nhid)\n","        self.fc2 = nn.Linear(nhid, nout)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","        self.w1 = Parameter(torch.Tensor(nhid, 1))\n","        self.w2 = Parameter(torch.Tensor(nhid, 1))\n","        self.m = nn.Softmax(dim=0)\n","\n","\n","    def forward(self, adj, x):\n","        lst = list()\n","        weight = adj.coalesce().values()\n","        adj = adj.coalesce().indices()\n","        skip = x.view(-1,self.window,self.n_nodes,self.nfeat)  \n","        skip = skip[:,:,:self.n_state,:]\n","        skip = torch.transpose(skip, 1, 2).reshape(-1,self.window,self.nfeat)\n","        skip = skip[:,:,:self.nhist]\n","\n","        x, (edge,att) = self.conv1(x, adj, return_attention_weights=True)\n","        x = self.dropout(self.bn1(self.relu(x)))\n","        x2 = x.reshape(-1,self.n_nodes,self.nhid)\n","        x2 = x2[:,:self.n_state,:]\n","        x2 = x2.reshape(-1,self.nhid)\n","        lst.append(x2)\n","        x = self.dropout(self.bn2(self.relu(self.conv2(x, adj))))\n","        x2 = x.reshape(-1,self.n_nodes,self.nhid)\n","        x2 = x2[:,:self.n_state,:]\n","        x2 = x2.reshape(-1,self.nhid)\n","        lst.append(x2)\n","\n","        x = torch.cat(lst, dim=1)\n","        x = x.view(-1, self.window, self.n_state, x.size(1))\n","        x = torch.transpose(x, 0, 1)\n","        x = x.contiguous().view(self.window, -1, x.size(3))\n","        out1, hn1 = self.rnn1(x)   \n","        out2, hn2 = self.rnn2(out1)\n","        x = torch.cat([hn1[0,:,:],hn2[0,:,:]], dim=1)\n","\n","        skip = skip.reshape(skip.size(0),-1)\n","        x = torch.cat([x,skip], dim=1)\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.relu(self.fc2(x)).squeeze()\n","        x = x.view(-1)\n","        return x, (edge,att)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9DiyJSg_2QF","outputId":"8097d11d-7f34-4f13-8796-b5d15ba95431"},"source":["# 1,7,14,28 days ahead\n","\n","######################################  Experiment ###########################################################\n","#import graph_construction_funcs\n","import os\n","import time\n","import argparse\n","import networkx as nx\n","import numpy as np\n","import scipy.sparse as sp\n","\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset,DataLoader\n","from datetime import date, timedelta\n","\n","from os.path import isfile, join, exists\n","from math import ceil\n","\n","import itertools\n","import pandas as pd\n","import json\n","\n","#zfrom model import DGNN\n","\n","#from utils import generate_new_features, AverageMeter, generate_new_batches\n","#from utils import \n","#from utils import sparse_mx_to_torch_sparse_tensor as to_sparse_tensor\n","#from models import DGNN\n","        \n","import sys\n","\n","import random\n","random.seed(2021)\n","torch.manual_seed(2021)\n","\n","    \n","def train(epoch, adj, features, y):\n","    optimizer.zero_grad()\n","    output,_ = model(adj, features)\n","    loss_train = F.mse_loss(output, y)\n","    loss_train.backward(retain_graph=True)\n","    optimizer.step()\n","    return output, loss_train\n","\n","\n","def test(adj, features, y):    \n","    output,(edge,att) = model(adj, features)\n","    loss_test = F.mse_loss(output, y)\n","    return output, loss_test, (edge,att) \n","\n","\n","\n","if __name__ == '__main__':\n","    LR = 0.001\n","    batch_size = 8\n","    window = 7\n","    ahead = 14\n","    the_shift = [1,7,14,28]\n","    start_exp = 15\n","    early_stop = 100\n","    epochs = 1000\n","    graph_window = 7\n","    dropout = 0.5\n","    hidden = 64\n","    hiddens = '64'\n","    ratio = 0\n","    #model_name = 'DGNN_'\n","    # model_name = 'ODE_baseline'\n","    # model_name = 'DGNN_mobility'\n","    #model_name = 'DGNN_risk_factors'\n","    model_name = 'DGNN_mobility_and_risk_factors'\n","\n","    start_test = 0\n","    end_test = 119\n","    sep = 10\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else torch.device(\"cpu\"))\n","    task = \"case\"\n","\n","    sdate, edate = date(2021, 8, 1), date(2021, 11, 28)   ##### EDIT: CHANGE DATES\n","    delta = edate - sdate\n","    dates = [sdate + timedelta(days=i) for i in range(delta.days+1)]\n","    dates = [str(date) for date in dates]\n","    dates = {i:d for i,d in enumerate(dates)}\n","\n","    # Change graphs so they create with the nodes in the graphs\n","    # Create dictionary of FIPS:ID for the locations with complete data, the data to be used in the graphs\n","    \n","    fips_to_id = pd.read_csv('/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/input/FIPS_ID_mappping.csv', usecols = ['FIPS','ID'])\n","    f2i = {}\n","    f2iK = list(fips_to_id['FIPS'])\n","    f2iV = list(fips_to_id['ID'])\n","    for i in range(0,len(f2iK)):\n","      f2i[f2iK[i]] = f2iV[i]\n","        \n","    #gs_adj = create_tmp_graphs3b(dates, f2iK, f2i)  # (days, nodes)\n","    gs_adj = create_tmp_graphs4b(dates, f2iK, f2i)  # (days, nodes)\n","    features = np.asarray(features_list)            # (days, nodes, features)\n","    y = np.asarray(y_list)                          # (days, nodes) list length 120 for 120 days, has lists of length 1519 for locations \n","    n_samples = len(gs_adj)                         # (days)\n","    nfeat = features_list[0].shape[1]               # (features)\n","\n","    n_risk_factors = 10\n","    n_state = 1519\n","    n_nodes = n_state + n_risk_factors\n","\n","    fw = open(\"/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/results_{}_{}.csv\".format(task,model_name),\"a\")\n","    fw.write(\"shift,edge,topk,window,hidden,MAE,STD,sMAPE\"+\"\\n\")\n","\n","    begin=time.time()\n","\n","\t#---- predict days ahead , 0-> next day etc.\n","    for shift in the_shift:\n","    #for shift in list([int(x)-1 for x in the_shift.split(\",\")]): #range(0,ahead), [int(shift)-1]\n","        for hidden in list([int(x) for x in hiddens.split(\",\")]):\n","\n","            fw = open(\"/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/results_{}_{}.csv\".format(model_name,\"US\"),\"a\")\n","            fw2 = open(\"/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/results_{}_{}.csv\".format(task,model_name),\"a\")                         \n","            fw3 = open(\"/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/atts_{}_{}_shift{}.csv\".format(task,model_name,shift),\"a\")\n","            result,result_rel = [],[]\n","            exp = 0\n","            \n","            for test_sample in range(start_exp,n_samples-shift): \n","                \n","                if test_sample < int(start_test) or test_sample > int(end_test): continue\n","                exp+=1\n","                print(\"testing on day #{}\".format(test_sample))\n","                test_begin=time.time()\n","\n","                if test_sample>=108:\n","                    idx_train = random.sample(list(range(window, test_sample-sep-10)), 80)\n","                    idx_train = idx_train + list(range(test_sample-sep-10, test_sample-sep))\n","                else: \n","                    idx_train = list(range(window-1, test_sample-sep))                        \n","\n","                idx_val = list(range(test_sample-sep,test_sample,2)) \n","                idx_train = idx_train+list(range(test_sample-sep+1,test_sample,2))\n","\n","                adj_train, features_train, y_train, lidx_train = generate_new_batches(gs_adj, features, y, idx_train, graph_window, shift, batch_size,device,test_sample)\n","                adj_val, features_val, y_val, lidx_val = generate_new_batches(gs_adj, features, y, idx_val, graph_window,  shift,batch_size, device,test_sample)\n","                adj_test, features_test, y_test, lidx_test = generate_new_batches(gs_adj, features, y, [test_sample], graph_window,shift, batch_size, device,test_sample) \n","\n","                n_train_batches = ceil(len(idx_train)/batch_size)\n","                n_val_batches = 1\n","                n_test_batches = 1\n","\n","                #-------------------- Training\n","                # Model and optimizer\n","                stop = False#\n","                while(not stop):#\n","                    sign=exp%2\n","                    model_save_name = \"/content/gdrive/My Drive/COVID_DGNN_KG/Final Folder (Draft)/data/output/models/model_{}_{}_{}_{}_{}.pth.tar\".format(model_name,task,shift,'edge',sign)\n","                    if os.path.exists(model_save_name) and exp not in [1,2]:\n","                        checkpoint = torch.load(model_save_name)\n","                        model.load_state_dict(checkpoint['state_dict'])\n","                    else:\n","                        model = DGNN(nfeat=nfeat, nhid=hidden, nout=1, n_nodes=n_nodes, window=graph_window, dropout=dropout).to(device)\n","                    \n","                    optimizer = optim.Adam(model.parameters(), lr=LR)\n","                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n","\n","                    ############################# Train ##############################\n","                    best_val_acc= 1e8\n","                    val_among_epochs = []\n","                    train_among_epochs = []\n","                    stop = False\n","\n","                    start = time.time()\n","                    for epoch in range(epochs):    \n","                        model.train()\n","                        train_loss = AverageMeter()\n","                        for batch in range(n_train_batches):\n","                            output, loss = train(epoch, adj_train[batch], features_train[batch], y_train[batch])#,lidx_train[batch])\n","                            train_loss.update(loss.data.item(), output.size(0))\n","\n","                        # Evaluate on validation set\n","                        model.eval()\n","                        output, val_loss, _ = test(adj_val[0], features_val[0], y_val[0])#, lidx_val[0])\n","                        val_loss = int(val_loss.detach().cpu().numpy())\n","                        val_loss /= (graph_window*n_state*5)  # only for US\n","                        if(epoch%10==0):\n","                            minute=round((time.time() - start)/60,2)\n","                            print(\"Epoch:\", '%03d' % (epoch + 1), \"train_loss=\", \"{:.2f}\".format(train_loss.avg),\"val_loss=\", \"{:.2f}\".format(val_loss), \"time=\", \"{:.2f}\".format(minute))\n","                        train_among_epochs.append(train_loss.avg)\n","                        val_among_epochs.append(val_loss)\n","                        if(epoch<30 and epoch>10):\n","                            if(len(set([round(val_e) for val_e in val_among_epochs[-20:]])) == 1 ):\n","                                break\n","                        if(epoch>early_stop):\n","                            eps = 0.05 if task != \"death\" else 0.005\n","                            if abs(val_loss - np.mean(val_among_epochs[-50:])) <= eps:\n","                                if val_loss < best_val_acc:\n","                                    best_val_acc = val_loss\n","                                    print(model_save_name, \"#########\")\n","                                    torch.save({\n","                                        'state_dict': model.state_dict(),\n","                                        'optimizer' : optimizer.state_dict(),\n","                                    }, model_save_name)\n","                                break\n","                        if val_loss < best_val_acc:\n","                            best_val_acc = val_loss\n","                            torch.save({\n","                                'state_dict': model.state_dict(),\n","                                'optimizer' : optimizer.state_dict(),\n","                            }, model_save_name)\n","                        stop = True\n","                        scheduler.step(val_loss)\n","\n","                ############################### Testing #####################################\n","                test_loss = AverageMeter()\n","                checkpoint = torch.load(model_save_name)\n","                model.load_state_dict(checkpoint['state_dict'])\n","                optimizer.load_state_dict(checkpoint['optimizer'])\n","                model.eval()\n","\n","                output, loss, (edge,att) = test(adj_test[0], features_test[0], y_test[0])#, lidx_test[0])\n","                o = output.cpu().detach().numpy()   # Predicted\n","                l = y_test[0].cpu().numpy()         # Actual [casesLocation1,...casesLocation1519]\n","\n","                error = np.mean(abs(o-abs(l)))                    # MAE: Average of |Predicted - Actual|\n","                error_rel = np.mean(abs(o-abs(l))/(o+l+0.00001))  # sMAPE: Average of |Predicted - Actual|/(Predicted + Actual)\n","\n","                # Print results\n","                result.append(error)\n","                result_rel.append(error_rel)\n","                print(\"test error={:.2f},{:.2f}, \".format(error, error_rel)+\"mean test error={:.2f},{:.2f}\".format(np.mean(result), np.mean(result_rel)))\n","                fw2.write(\"{},{},{},{:.5f},{:.5f},{:.5f},{:.5f}\".format(shift,hidden,test_sample,error,error_rel,np.mean(result),np.mean(result_rel))+'\\n')\n","                fw3.write(\"{}, {}, {}\".format(test_sample, edge, att))\n","            fw.write(str(shift)+\",\"+str('edge')+\",\"+str(ratio)+\",\"+str(window)+\",\"+str(hidden)+\",{:.5f}\".format(np.mean(result))+\",{:.5f}\".format(np.std(result))+\",{:.5f}\".format(np.mean(result_rel))+\"\\n\")\n","            fw.close()\n","            fw2.close()\n","            fw3.close()\n","            print(\"Total time cost: {} minutes.\".format(round((time.time()-begin)/60, 2)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","196\n","Total number of nodes:  1529\n","Total number of edges:  7950\n","Total number of dynamic edges: 384\n","196\n","Total number of nodes:  1529\n","Total number of edges:  7950\n","Total number of dynamic edges: 384\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1529\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","153\n","Total number of nodes:  1529\n","Total number of edges:  7864\n","Total number of dynamic edges: 298\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","198\n","Total number of nodes:  1528\n","Total number of edges:  7954\n","Total number of dynamic edges: 388\n","197\n","Total number of nodes:  1529\n","Total number of edges:  7952\n","Total number of dynamic edges: 386\n","testing on day #15\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 001 train_loss= 18005252.00 val_loss= 303.83 time= 0.01\n","Epoch: 011 train_loss= 5785092.00 val_loss= 90.15 time= 0.04\n","Epoch: 021 train_loss= 1514032.00 val_loss= 16.66 time= 0.04\n","Epoch: 031 train_loss= 2519706.00 val_loss= 18.01 time= 0.05\n","Epoch: 041 train_loss= 1665785.00 val_loss= 16.22 time= 0.06\n","Epoch: 051 train_loss= 2291201.75 val_loss= 15.80 time= 0.07\n","Epoch: 061 train_loss= 1857263.38 val_loss= 15.75 time= 0.07\n","Epoch: 071 train_loss= 2146117.00 val_loss= 15.74 time= 0.08\n","Epoch: 081 train_loss= 2191081.25 val_loss= 15.74 time= 0.09\n","Epoch: 091 train_loss= 2817222.75 val_loss= 15.74 time= 0.09\n","Epoch: 101 train_loss= 2163181.75 val_loss= 15.74 time= 0.10\n","test error=191.50,0.12, mean test error=191.50,0.12\n","testing on day #16\n","Epoch: 001 train_loss= 21107044.00 val_loss= 356.49 time= 0.01\n","Epoch: 011 train_loss= 7015142.50 val_loss= 113.61 time= 0.03\n","Epoch: 021 train_loss= 1819945.38 val_loss= 14.05 time= 0.04\n","Epoch: 031 train_loss= 1828170.75 val_loss= 17.20 time= 0.04\n","Epoch: 041 train_loss= 2730449.75 val_loss= 14.72 time= 0.05\n","Epoch: 051 train_loss= 1613285.25 val_loss= 14.10 time= 0.06\n","Epoch: 061 train_loss= 1883184.12 val_loss= 14.02 time= 0.06\n","Epoch: 071 train_loss= 1823109.50 val_loss= 14.01 time= 0.07\n","Epoch: 081 train_loss= 2050677.38 val_loss= 14.01 time= 0.08\n","Epoch: 091 train_loss= 2685544.25 val_loss= 14.01 time= 0.09\n","Epoch: 101 train_loss= 1796219.75 val_loss= 14.01 time= 0.09\n","test error=190.68,0.11, mean test error=191.09,0.12\n","testing on day #17\n","Epoch: 001 train_loss= 2203464.25 val_loss= 15.34 time= 0.01\n","Epoch: 011 train_loss= 1799727.38 val_loss= 16.13 time= 0.02\n","Epoch: 021 train_loss= 1628331.12 val_loss= 15.83 time= 0.03\n","test error=191.03,0.11, mean test error=191.07,0.11\n","testing on day #18\n","Epoch: 001 train_loss= 1511353.38 val_loss= 15.05 time= 0.01\n","Epoch: 011 train_loss= 1863967.00 val_loss= 14.90 time= 0.02\n","Epoch: 021 train_loss= 1566353.50 val_loss= 15.47 time= 0.03\n","Epoch: 031 train_loss= 1554908.75 val_loss= 14.46 time= 0.04\n","Epoch: 041 train_loss= 1374255.50 val_loss= 13.92 time= 0.05\n","Epoch: 051 train_loss= 1881680.75 val_loss= 15.80 time= 0.06\n","Epoch: 061 train_loss= 1471654.12 val_loss= 13.99 time= 0.07\n","Epoch: 071 train_loss= 1457960.38 val_loss= 13.87 time= 0.08\n","Epoch: 081 train_loss= 1590170.88 val_loss= 13.87 time= 0.09\n","Epoch: 091 train_loss= 1582082.50 val_loss= 13.87 time= 0.09\n","Epoch: 101 train_loss= 1491534.75 val_loss= 13.87 time= 0.10\n","test error=192.50,0.11, mean test error=191.43,0.11\n","testing on day #19\n","Epoch: 001 train_loss= 2017706.38 val_loss= 16.97 time= 0.01\n","Epoch: 011 train_loss= 1680579.25 val_loss= 15.46 time= 0.02\n","Epoch: 021 train_loss= 1829807.75 val_loss= 15.32 time= 0.03\n","Epoch: 031 train_loss= 1703206.25 val_loss= 14.45 time= 0.04\n","Epoch: 041 train_loss= 1434826.25 val_loss= 15.18 time= 0.06\n","Epoch: 051 train_loss= 1610119.62 val_loss= 14.39 time= 0.06\n","Epoch: 061 train_loss= 1486201.50 val_loss= 14.30 time= 0.07\n","Epoch: 071 train_loss= 1396081.50 val_loss= 14.27 time= 0.09\n","Epoch: 081 train_loss= 1364148.75 val_loss= 14.29 time= 0.10\n","Epoch: 091 train_loss= 1589042.50 val_loss= 14.29 time= 0.11\n","Epoch: 101 train_loss= 1422334.25 val_loss= 14.29 time= 0.11\n","test error=178.87,0.11, mean test error=188.92,0.11\n","testing on day #20\n","Epoch: 001 train_loss= 1457614.29 val_loss= 17.16 time= 0.01\n","Epoch: 011 train_loss= 1649860.35 val_loss= 18.66 time= 0.02\n","Epoch: 021 train_loss= 1724899.64 val_loss= 16.63 time= 0.03\n","Epoch: 031 train_loss= 1306069.56 val_loss= 16.83 time= 0.04\n","Epoch: 041 train_loss= 1402373.92 val_loss= 16.83 time= 0.05\n","Epoch: 051 train_loss= 1659784.78 val_loss= 16.83 time= 0.06\n","Epoch: 061 train_loss= 1573107.18 val_loss= 16.83 time= 0.07\n","Epoch: 071 train_loss= 1840650.39 val_loss= 16.83 time= 0.08\n","Epoch: 081 train_loss= 1599437.06 val_loss= 16.83 time= 0.09\n","Epoch: 091 train_loss= 1837856.19 val_loss= 16.83 time= 0.10\n","Epoch: 101 train_loss= 1739060.21 val_loss= 16.83 time= 0.12\n","test error=189.07,0.11, mean test error=188.94,0.11\n","testing on day #21\n","Epoch: 001 train_loss= 1494030.60 val_loss= 14.38 time= 0.01\n","Epoch: 011 train_loss= 1482809.38 val_loss= 16.88 time= 0.03\n","Epoch: 021 train_loss= 1332016.55 val_loss= 15.75 time= 0.04\n","Epoch: 031 train_loss= 1599982.80 val_loss= 15.50 time= 0.05\n","Epoch: 041 train_loss= 1552571.15 val_loss= 15.48 time= 0.06\n","Epoch: 051 train_loss= 1465741.00 val_loss= 15.48 time= 0.07\n","Epoch: 061 train_loss= 1531447.95 val_loss= 15.48 time= 0.08\n","Epoch: 071 train_loss= 1507167.50 val_loss= 15.48 time= 0.09\n","Epoch: 081 train_loss= 1734152.50 val_loss= 15.48 time= 0.10\n","Epoch: 091 train_loss= 1654292.30 val_loss= 15.48 time= 0.12\n","Epoch: 101 train_loss= 1363954.23 val_loss= 15.48 time= 0.13\n","test error=222.59,0.12, mean test error=193.75,0.11\n","testing on day #22\n","Epoch: 001 train_loss= 1714469.11 val_loss= 16.82 time= 0.01\n","Epoch: 011 train_loss= 1667149.70 val_loss= 15.11 time= 0.03\n","Epoch: 021 train_loss= 1588172.11 val_loss= 16.61 time= 0.04\n","Epoch: 031 train_loss= 1605376.65 val_loss= 15.16 time= 0.05\n","Epoch: 041 train_loss= 1359351.83 val_loss= 15.11 time= 0.06\n","Epoch: 051 train_loss= 1508189.39 val_loss= 15.10 time= 0.08\n","Epoch: 061 train_loss= 1775731.11 val_loss= 15.10 time= 0.09\n","Epoch: 071 train_loss= 1694973.64 val_loss= 15.10 time= 0.10\n","Epoch: 081 train_loss= 1600367.66 val_loss= 15.10 time= 0.11\n","Epoch: 091 train_loss= 1581944.98 val_loss= 15.10 time= 0.12\n","Epoch: 101 train_loss= 1750858.95 val_loss= 15.10 time= 0.14\n","test error=213.01,0.11, mean test error=196.16,0.11\n","testing on day #23\n","Epoch: 001 train_loss= 1743070.33 val_loss= 15.38 time= 0.01\n","Epoch: 011 train_loss= 1700675.25 val_loss= 15.74 time= 0.03\n","Epoch: 021 train_loss= 1590837.00 val_loss= 15.35 time= 0.04\n","Epoch: 031 train_loss= 1501192.58 val_loss= 14.28 time= 0.06\n","Epoch: 041 train_loss= 1267152.96 val_loss= 14.45 time= 0.07\n","Epoch: 051 train_loss= 1442166.42 val_loss= 14.23 time= 0.09\n","Epoch: 061 train_loss= 1815754.08 val_loss= 12.89 time= 0.10\n","Epoch: 071 train_loss= 1413903.17 val_loss= 12.29 time= 0.11\n","Epoch: 081 train_loss= 1216533.12 val_loss= 12.21 time= 0.13\n","Epoch: 091 train_loss= 1307173.17 val_loss= 12.02 time= 0.14\n","Epoch: 101 train_loss= 1407463.79 val_loss= 11.53 time= 0.16\n","Epoch: 111 train_loss= 1323201.58 val_loss= 11.00 time= 0.17\n","Epoch: 121 train_loss= 1390367.00 val_loss= 11.39 time= 0.18\n","test error=190.59,0.11, mean test error=195.54,0.11\n","testing on day #24\n","Epoch: 001 train_loss= 1839049.19 val_loss= 18.96 time= 0.00\n","Epoch: 011 train_loss= 1448657.68 val_loss= 17.72 time= 0.02\n","Epoch: 021 train_loss= 1514639.16 val_loss= 15.92 time= 0.03\n","Epoch: 031 train_loss= 1878729.12 val_loss= 16.71 time= 0.05\n","Epoch: 041 train_loss= 1332341.48 val_loss= 14.88 time= 0.06\n","Epoch: 051 train_loss= 1368827.00 val_loss= 14.66 time= 0.08\n","Epoch: 061 train_loss= 1534823.86 val_loss= 14.27 time= 0.09\n","Epoch: 071 train_loss= 1311275.68 val_loss= 13.86 time= 0.11\n","Epoch: 081 train_loss= 1502599.94 val_loss= 13.53 time= 0.12\n","Epoch: 091 train_loss= 1433649.76 val_loss= 13.07 time= 0.14\n","Epoch: 101 train_loss= 1779198.52 val_loss= 12.95 time= 0.15\n","Epoch: 111 train_loss= 1537349.97 val_loss= 13.09 time= 0.17\n","Epoch: 121 train_loss= 1562652.81 val_loss= 12.28 time= 0.19\n","Epoch: 131 train_loss= 1382464.49 val_loss= 13.92 time= 0.20\n","test error=195.65,0.13, mean test error=195.55,0.11\n","testing on day #25\n","Epoch: 001 train_loss= 1530831.61 val_loss= 14.22 time= 0.01\n","Epoch: 011 train_loss= 1134158.95 val_loss= 12.89 time= 0.03\n","Epoch: 021 train_loss= 1505620.39 val_loss= 12.61 time= 0.05\n","Epoch: 031 train_loss= 1479335.64 val_loss= 12.93 time= 0.06\n","Epoch: 041 train_loss= 1202347.02 val_loss= 12.51 time= 0.08\n","Epoch: 051 train_loss= 1326611.12 val_loss= 11.81 time= 0.09\n","Epoch: 061 train_loss= 1319971.57 val_loss= 11.88 time= 0.11\n","Epoch: 071 train_loss= 1091517.07 val_loss= 11.91 time= 0.12\n","Epoch: 081 train_loss= 1472487.89 val_loss= 11.32 time= 0.14\n","Epoch: 091 train_loss= 1091466.11 val_loss= 11.48 time= 0.15\n","Epoch: 101 train_loss= 1247545.14 val_loss= 11.50 time= 0.17\n","test error=171.12,0.11, mean test error=193.33,0.11\n","testing on day #26\n","Epoch: 001 train_loss= 1672938.90 val_loss= 15.91 time= 0.01\n","Epoch: 011 train_loss= 1590674.52 val_loss= 13.15 time= 0.03\n","Epoch: 021 train_loss= 1612727.42 val_loss= 12.71 time= 0.05\n","Epoch: 031 train_loss= 1462568.39 val_loss= 12.68 time= 0.06\n","Epoch: 041 train_loss= 1256487.31 val_loss= 12.12 time= 0.08\n","Epoch: 051 train_loss= 1578742.64 val_loss= 12.48 time= 0.10\n","Epoch: 061 train_loss= 1530634.94 val_loss= 11.82 time= 0.11\n","Epoch: 071 train_loss= 1505903.72 val_loss= 12.14 time= 0.13\n","Epoch: 081 train_loss= 1296084.01 val_loss= 11.28 time= 0.15\n","Epoch: 091 train_loss= 1499420.25 val_loss= 11.21 time= 0.16\n","Epoch: 101 train_loss= 1392546.81 val_loss= 12.09 time= 0.18\n","Epoch: 111 train_loss= 1486677.97 val_loss= 11.30 time= 0.20\n","test error=176.23,0.13, mean test error=191.90,0.12\n","testing on day #27\n","Epoch: 001 train_loss= 1467838.28 val_loss= 13.21 time= 0.01\n","Epoch: 011 train_loss= 1353701.25 val_loss= 12.36 time= 0.03\n","Epoch: 021 train_loss= 1192376.91 val_loss= 11.72 time= 0.05\n","test error=152.33,0.10, mean test error=188.86,0.11\n","testing on day #28\n","Epoch: 001 train_loss= 1518059.88 val_loss= 11.17 time= 0.00\n","Epoch: 011 train_loss= 1421873.79 val_loss= 10.61 time= 0.02\n","Epoch: 021 train_loss= 1808220.71 val_loss= 10.30 time= 0.04\n","Epoch: 031 train_loss= 1478675.64 val_loss= 10.04 time= 0.06\n","Epoch: 041 train_loss= 1566650.52 val_loss= 11.88 time= 0.08\n","Epoch: 051 train_loss= 1471542.63 val_loss= 10.91 time= 0.09\n","Epoch: 061 train_loss= 1485959.14 val_loss= 10.88 time= 0.11\n","Epoch: 071 train_loss= 1224852.04 val_loss= 10.86 time= 0.13\n","Epoch: 081 train_loss= 1453978.18 val_loss= 10.86 time= 0.15\n","Epoch: 091 train_loss= 1264016.68 val_loss= 10.86 time= 0.17\n","Epoch: 101 train_loss= 1246511.15 val_loss= 10.86 time= 0.18\n","test error=163.20,0.11, mean test error=187.03,0.11\n","testing on day #29\n","Epoch: 001 train_loss= 1493896.39 val_loss= 10.82 time= 0.00\n","Epoch: 011 train_loss= 1301237.08 val_loss= 13.22 time= 0.02\n","Epoch: 021 train_loss= 1189960.81 val_loss= 10.99 time= 0.04\n","Epoch: 031 train_loss= 1285532.44 val_loss= 11.03 time= 0.06\n","Epoch: 041 train_loss= 1321594.14 val_loss= 11.02 time= 0.08\n","Epoch: 051 train_loss= 1235370.60 val_loss= 11.02 time= 0.10\n","Epoch: 061 train_loss= 1260249.75 val_loss= 11.02 time= 0.11\n","Epoch: 071 train_loss= 1316428.28 val_loss= 11.02 time= 0.13\n","Epoch: 081 train_loss= 1324336.18 val_loss= 11.02 time= 0.15\n","Epoch: 091 train_loss= 1327262.43 val_loss= 11.02 time= 0.17\n","Epoch: 101 train_loss= 1432169.29 val_loss= 11.02 time= 0.19\n","test error=176.02,0.10, mean test error=186.29,0.11\n","testing on day #30\n","Epoch: 001 train_loss= 1653758.97 val_loss= 9.45 time= 0.00\n","Epoch: 011 train_loss= 1281763.94 val_loss= 10.89 time= 0.02\n","Epoch: 021 train_loss= 1471427.07 val_loss= 11.43 time= 0.04\n","Epoch: 031 train_loss= 1508078.29 val_loss= 9.25 time= 0.06\n","Epoch: 041 train_loss= 1821209.99 val_loss= 10.37 time= 0.08\n","Epoch: 051 train_loss= 1317767.70 val_loss= 10.50 time= 0.10\n","Epoch: 061 train_loss= 1458596.29 val_loss= 9.03 time= 0.12\n","Epoch: 071 train_loss= 1635362.18 val_loss= 9.08 time= 0.14\n","Epoch: 081 train_loss= 1666359.71 val_loss= 9.43 time= 0.16\n","Epoch: 091 train_loss= 1186795.80 val_loss= 9.21 time= 0.18\n","Epoch: 101 train_loss= 1531545.76 val_loss= 9.20 time= 0.20\n","test error=192.89,0.12, mean test error=186.71,0.11\n","testing on day #31\n","Epoch: 001 train_loss= 1335545.00 val_loss= 12.34 time= 0.00\n","Epoch: 011 train_loss= 1489067.50 val_loss= 12.29 time= 0.02\n","Epoch: 021 train_loss= 1332305.40 val_loss= 12.43 time= 0.04\n","Epoch: 031 train_loss= 1384407.15 val_loss= 12.30 time= 0.06\n","Epoch: 041 train_loss= 1317960.93 val_loss= 12.30 time= 0.08\n","Epoch: 051 train_loss= 1373858.07 val_loss= 12.30 time= 0.10\n","Epoch: 061 train_loss= 1459071.35 val_loss= 12.30 time= 0.12\n","Epoch: 071 train_loss= 1431940.90 val_loss= 12.30 time= 0.15\n","Epoch: 081 train_loss= 1365377.68 val_loss= 12.30 time= 0.17\n","Epoch: 091 train_loss= 1228509.62 val_loss= 12.30 time= 0.19\n","Epoch: 101 train_loss= 1141276.85 val_loss= 12.30 time= 0.21\n","test error=176.98,0.10, mean test error=186.13,0.11\n","testing on day #32\n","Epoch: 001 train_loss= 1618857.81 val_loss= 13.21 time= 0.00\n","Epoch: 011 train_loss= 1535973.74 val_loss= 11.88 time= 0.03\n","Epoch: 021 train_loss= 1480022.68 val_loss= 12.07 time= 0.05\n","Epoch: 031 train_loss= 1344507.02 val_loss= 11.40 time= 0.07\n","Epoch: 041 train_loss= 1359418.55 val_loss= 11.48 time= 0.09\n","Epoch: 051 train_loss= 1372826.50 val_loss= 11.46 time= 0.11\n","Epoch: 061 train_loss= 1637225.96 val_loss= 11.46 time= 0.13\n","Epoch: 071 train_loss= 1518477.12 val_loss= 11.46 time= 0.15\n","Epoch: 081 train_loss= 1525625.30 val_loss= 11.46 time= 0.18\n","Epoch: 091 train_loss= 1414552.54 val_loss= 11.46 time= 0.20\n","Epoch: 101 train_loss= 1187394.55 val_loss= 11.46 time= 0.22\n","test error=203.84,0.13, mean test error=187.12,0.11\n","testing on day #33\n","Epoch: 001 train_loss= 1394873.48 val_loss= 15.57 time= 0.00\n","Epoch: 011 train_loss= 1540578.66 val_loss= 14.33 time= 0.03\n","Epoch: 021 train_loss= 1389069.84 val_loss= 13.74 time= 0.05\n","Epoch: 031 train_loss= 1393381.67 val_loss= 14.21 time= 0.07\n","Epoch: 041 train_loss= 1413292.61 val_loss= 13.65 time= 0.09\n","Epoch: 051 train_loss= 1182906.57 val_loss= 13.59 time= 0.11\n","Epoch: 061 train_loss= 1228388.23 val_loss= 13.58 time= 0.14\n","Epoch: 071 train_loss= 1446266.82 val_loss= 13.58 time= 0.16\n","Epoch: 081 train_loss= 1307363.09 val_loss= 13.58 time= 0.18\n","Epoch: 091 train_loss= 1220352.39 val_loss= 13.58 time= 0.20\n","Epoch: 101 train_loss= 1519243.18 val_loss= 13.58 time= 0.22\n","test error=185.88,0.10, mean test error=187.05,0.11\n","testing on day #34\n","Epoch: 001 train_loss= 1662371.36 val_loss= 14.45 time= 0.00\n","Epoch: 011 train_loss= 1482996.43 val_loss= 15.75 time= 0.03\n","Epoch: 021 train_loss= 1630549.86 val_loss= 14.91 time= 0.05\n","Epoch: 031 train_loss= 1401866.81 val_loss= 16.14 time= 0.07\n","Epoch: 041 train_loss= 1592284.50 val_loss= 14.64 time= 0.10\n","Epoch: 051 train_loss= 1407557.48 val_loss= 14.76 time= 0.12\n","Epoch: 061 train_loss= 1543838.51 val_loss= 14.77 time= 0.14\n","Epoch: 071 train_loss= 1296776.71 val_loss= 14.77 time= 0.16\n","Epoch: 081 train_loss= 1328495.80 val_loss= 14.77 time= 0.19\n","Epoch: 091 train_loss= 1749436.29 val_loss= 14.77 time= 0.21\n","Epoch: 101 train_loss= 1447560.49 val_loss= 14.77 time= 0.23\n","test error=180.45,0.12, mean test error=186.72,0.11\n","testing on day #35\n","Epoch: 001 train_loss= 1551079.50 val_loss= 15.22 time= 0.00\n","Epoch: 011 train_loss= 1389436.88 val_loss= 15.42 time= 0.03\n","Epoch: 021 train_loss= 1477532.42 val_loss= 15.45 time= 0.05\n","Epoch: 031 train_loss= 1487828.96 val_loss= 15.06 time= 0.08\n","Epoch: 041 train_loss= 1564717.67 val_loss= 15.06 time= 0.10\n","Epoch: 051 train_loss= 1551892.00 val_loss= 15.06 time= 0.12\n","Epoch: 061 train_loss= 1280046.96 val_loss= 15.03 time= 0.15\n","Epoch: 071 train_loss= 1522307.04 val_loss= 15.03 time= 0.17\n","Epoch: 081 train_loss= 1411754.54 val_loss= 15.03 time= 0.19\n","Epoch: 091 train_loss= 1443177.04 val_loss= 15.03 time= 0.22\n","Epoch: 101 train_loss= 1430516.46 val_loss= 15.03 time= 0.24\n","test error=355.75,0.16, mean test error=194.77,0.12\n","testing on day #36\n","Epoch: 001 train_loss= 1795623.92 val_loss= 29.21 time= 0.00\n","Epoch: 011 train_loss= 1787512.48 val_loss= 20.53 time= 0.03\n","Epoch: 021 train_loss= 1946868.90 val_loss= 24.84 time= 0.05\n","Epoch: 031 train_loss= 2017464.54 val_loss= 27.26 time= 0.08\n","Epoch: 041 train_loss= 1668950.46 val_loss= 27.15 time= 0.10\n","Epoch: 051 train_loss= 1994256.96 val_loss= 27.11 time= 0.13\n","Epoch: 061 train_loss= 2045269.62 val_loss= 27.11 time= 0.16\n","Epoch: 071 train_loss= 1765809.22 val_loss= 27.11 time= 0.18\n","Epoch: 081 train_loss= 1862965.52 val_loss= 27.11 time= 0.21\n","Epoch: 091 train_loss= 2230383.40 val_loss= 27.11 time= 0.23\n","Epoch: 101 train_loss= 1999145.36 val_loss= 27.11 time= 0.26\n","test error=179.30,0.11, mean test error=194.07,0.12\n","testing on day #37\n","Epoch: 001 train_loss= 1581528.94 val_loss= 37.47 time= 0.00\n","Epoch: 011 train_loss= 1725092.37 val_loss= 31.61 time= 0.03\n","Epoch: 021 train_loss= 1484202.37 val_loss= 31.35 time= 0.06\n","Epoch: 031 train_loss= 1447162.33 val_loss= 31.42 time= 0.08\n","Epoch: 041 train_loss= 1485174.21 val_loss= 31.36 time= 0.11\n","Epoch: 051 train_loss= 1537123.17 val_loss= 31.36 time= 0.13\n","Epoch: 061 train_loss= 1564703.42 val_loss= 31.36 time= 0.16\n","Epoch: 071 train_loss= 1506669.17 val_loss= 31.37 time= 0.19\n","Epoch: 081 train_loss= 1414882.13 val_loss= 31.36 time= 0.21\n","Epoch: 091 train_loss= 1469921.17 val_loss= 31.36 time= 0.24\n","Epoch: 101 train_loss= 1498040.33 val_loss= 31.36 time= 0.26\n","test error=206.11,0.11, mean test error=194.59,0.12\n","testing on day #38\n","Epoch: 001 train_loss= 2098648.94 val_loss= 24.70 time= 0.00\n","Epoch: 011 train_loss= 1630896.62 val_loss= 24.42 time= 0.03\n","Epoch: 021 train_loss= 1791911.31 val_loss= 22.42 time= 0.06\n","Epoch: 031 train_loss= 1719832.12 val_loss= 23.08 time= 0.08\n","Epoch: 041 train_loss= 1741735.24 val_loss= 22.77 time= 0.11\n","Epoch: 051 train_loss= 1778742.13 val_loss= 22.76 time= 0.14\n","Epoch: 061 train_loss= 1999060.19 val_loss= 22.76 time= 0.17\n","Epoch: 071 train_loss= 1812643.60 val_loss= 22.76 time= 0.19\n","Epoch: 081 train_loss= 1968519.41 val_loss= 22.76 time= 0.22\n","Epoch: 091 train_loss= 1691860.19 val_loss= 22.76 time= 0.25\n","Epoch: 101 train_loss= 1733634.44 val_loss= 22.76 time= 0.27\n","test error=234.11,0.13, mean test error=196.24,0.12\n","testing on day #39\n","Epoch: 001 train_loss= 2023780.70 val_loss= 33.74 time= 0.00\n","Epoch: 011 train_loss= 1274317.30 val_loss= 34.88 time= 0.03\n","Epoch: 021 train_loss= 1441228.04 val_loss= 35.55 time= 0.06\n","Epoch: 031 train_loss= 1623013.05 val_loss= 34.84 time= 0.09\n","Epoch: 041 train_loss= 1705872.36 val_loss= 34.84 time= 0.11\n","Epoch: 051 train_loss= 1493825.48 val_loss= 34.83 time= 0.14\n","Epoch: 061 train_loss= 1568137.61 val_loss= 34.83 time= 0.17\n","Epoch: 071 train_loss= 1649794.50 val_loss= 34.83 time= 0.20\n","Epoch: 081 train_loss= 1547878.89 val_loss= 34.83 time= 0.22\n","Epoch: 091 train_loss= 1566292.52 val_loss= 34.83 time= 0.25\n","Epoch: 101 train_loss= 1691166.21 val_loss= 34.82 time= 0.28\n","test error=225.87,0.11, mean test error=197.42,0.12\n","testing on day #40\n","Epoch: 001 train_loss= 2032500.69 val_loss= 25.58 time= 0.00\n","Epoch: 011 train_loss= 2133444.40 val_loss= 21.38 time= 0.03\n","Epoch: 021 train_loss= 1982770.76 val_loss= 22.37 time= 0.06\n","Epoch: 031 train_loss= 1751705.00 val_loss= 22.26 time= 0.09\n","Epoch: 041 train_loss= 1905685.97 val_loss= 22.30 time= 0.12\n","Epoch: 051 train_loss= 1883158.21 val_loss= 22.30 time= 0.15\n","Epoch: 061 train_loss= 1974017.94 val_loss= 22.29 time= 0.18\n","Epoch: 071 train_loss= 1856080.00 val_loss= 22.29 time= 0.21\n","Epoch: 081 train_loss= 1645983.89 val_loss= 22.29 time= 0.23\n","Epoch: 091 train_loss= 1583113.46 val_loss= 22.29 time= 0.26\n","Epoch: 101 train_loss= 1728905.10 val_loss= 22.29 time= 0.29\n","test error=230.39,0.14, mean test error=198.69,0.12\n","testing on day #41\n","Epoch: 001 train_loss= 1993481.45 val_loss= 39.05 time= 0.00\n","Epoch: 011 train_loss= 1778391.43 val_loss= 39.21 time= 0.03\n","Epoch: 021 train_loss= 1542007.00 val_loss= 37.21 time= 0.06\n","Epoch: 031 train_loss= 1566468.69 val_loss= 37.03 time= 0.09\n","Epoch: 041 train_loss= 1403487.82 val_loss= 37.12 time= 0.12\n","Epoch: 051 train_loss= 1634873.25 val_loss= 37.13 time= 0.15\n","Epoch: 061 train_loss= 1448548.81 val_loss= 37.13 time= 0.18\n","Epoch: 071 train_loss= 1643362.82 val_loss= 37.13 time= 0.21\n","Epoch: 081 train_loss= 1638516.30 val_loss= 37.14 time= 0.24\n","Epoch: 091 train_loss= 1540304.78 val_loss= 37.14 time= 0.27\n","Epoch: 101 train_loss= 1546370.88 val_loss= 37.14 time= 0.30\n","test error=210.75,0.10, mean test error=199.14,0.12\n","testing on day #42\n","Epoch: 001 train_loss= 2100501.37 val_loss= 26.41 time= 0.00\n","Epoch: 011 train_loss= 1860276.31 val_loss= 24.56 time= 0.03\n","Epoch: 021 train_loss= 1910930.31 val_loss= 22.43 time= 0.07\n","Epoch: 031 train_loss= 1858729.25 val_loss= 23.06 time= 0.10\n","Epoch: 041 train_loss= 1872050.64 val_loss= 22.98 time= 0.13\n","Epoch: 051 train_loss= 1832349.01 val_loss= 22.99 time= 0.16\n","Epoch: 061 train_loss= 1914230.55 val_loss= 22.99 time= 0.19\n","Epoch: 071 train_loss= 1965694.36 val_loss= 22.98 time= 0.22\n","Epoch: 081 train_loss= 1837210.13 val_loss= 22.98 time= 0.25\n","Epoch: 091 train_loss= 1842509.78 val_loss= 22.98 time= 0.28\n","Epoch: 101 train_loss= 1816691.12 val_loss= 22.98 time= 0.31\n","test error=415.73,0.21, mean test error=206.87,0.12\n","testing on day #43\n","Epoch: 001 train_loss= 1972183.47 val_loss= 48.52 time= 0.00\n","Epoch: 011 train_loss= 1769049.45 val_loss= 35.51 time= 0.03\n","Epoch: 021 train_loss= 1644280.53 val_loss= 38.33 time= 0.07\n","Epoch: 031 train_loss= 1673412.94 val_loss= 40.80 time= 0.10\n","Epoch: 041 train_loss= 1884961.34 val_loss= 40.42 time= 0.13\n","Epoch: 051 train_loss= 1641648.73 val_loss= 40.42 time= 0.16\n","Epoch: 061 train_loss= 2190931.08 val_loss= 40.42 time= 0.19\n","Epoch: 071 train_loss= 1660618.17 val_loss= 40.42 time= 0.22\n","Epoch: 081 train_loss= 1944785.66 val_loss= 40.42 time= 0.25\n","Epoch: 091 train_loss= 1556136.66 val_loss= 40.42 time= 0.28\n","Epoch: 101 train_loss= 1914150.00 val_loss= 40.42 time= 0.31\n","test error=220.60,0.10, mean test error=207.35,0.12\n","testing on day #44\n","Epoch: 001 train_loss= 2129278.74 val_loss= 36.59 time= 0.00\n","Epoch: 011 train_loss= 1830015.01 val_loss= 37.34 time= 0.04\n","Epoch: 021 train_loss= 1928130.80 val_loss= 35.90 time= 0.07\n","Epoch: 031 train_loss= 1966156.71 val_loss= 35.45 time= 0.10\n","Epoch: 041 train_loss= 2221171.19 val_loss= 35.54 time= 0.13\n","Epoch: 051 train_loss= 2036539.14 val_loss= 35.53 time= 0.17\n","Epoch: 061 train_loss= 1899063.61 val_loss= 35.53 time= 0.20\n","Epoch: 071 train_loss= 1857290.98 val_loss= 35.53 time= 0.23\n","Epoch: 081 train_loss= 2037309.02 val_loss= 35.53 time= 0.27\n","Epoch: 091 train_loss= 1981904.41 val_loss= 35.53 time= 0.30\n","Epoch: 101 train_loss= 2343823.75 val_loss= 35.53 time= 0.33\n","test error=260.49,0.14, mean test error=209.12,0.12\n","testing on day #45\n","Epoch: 001 train_loss= 2169074.97 val_loss= 33.63 time= 0.00\n","Epoch: 011 train_loss= 1871850.49 val_loss= 40.02 time= 0.04\n","Epoch: 021 train_loss= 1760602.18 val_loss= 42.07 time= 0.07\n","Epoch: 031 train_loss= 1783859.34 val_loss= 40.65 time= 0.10\n","Epoch: 041 train_loss= 1965271.18 val_loss= 41.13 time= 0.14\n","Epoch: 051 train_loss= 1915719.41 val_loss= 41.19 time= 0.17\n","Epoch: 061 train_loss= 1854828.31 val_loss= 41.19 time= 0.20\n","Epoch: 071 train_loss= 1731580.46 val_loss= 41.18 time= 0.24\n","Epoch: 081 train_loss= 1901928.24 val_loss= 41.19 time= 0.27\n","Epoch: 091 train_loss= 1777961.82 val_loss= 41.18 time= 0.30\n","Epoch: 101 train_loss= 1782406.84 val_loss= 41.19 time= 0.34\n","test error=224.37,0.11, mean test error=209.61,0.12\n","testing on day #46\n","Epoch: 001 train_loss= 2549665.43 val_loss= 36.84 time= 0.00\n","Epoch: 011 train_loss= 1999063.21 val_loss= 37.14 time= 0.04\n","Epoch: 021 train_loss= 1918726.26 val_loss= 37.10 time= 0.07\n","Epoch: 031 train_loss= 1778340.86 val_loss= 35.72 time= 0.11\n","Epoch: 041 train_loss= 1993066.49 val_loss= 35.56 time= 0.14\n","Epoch: 051 train_loss= 1909851.66 val_loss= 35.60 time= 0.18\n","Epoch: 061 train_loss= 1782988.99 val_loss= 35.59 time= 0.21\n","Epoch: 071 train_loss= 2000104.44 val_loss= 35.59 time= 0.24\n","Epoch: 081 train_loss= 1877228.69 val_loss= 35.60 time= 0.28\n","Epoch: 091 train_loss= 2134057.71 val_loss= 35.60 time= 0.31\n","Epoch: 101 train_loss= 2017418.02 val_loss= 35.60 time= 0.35\n","test error=212.56,0.12, mean test error=209.70,0.12\n","testing on day #47\n","Epoch: 001 train_loss= 2278718.64 val_loss= 19.22 time= 0.00\n","Epoch: 011 train_loss= 1803998.18 val_loss= 21.67 time= 0.04\n","Epoch: 021 train_loss= 1849661.81 val_loss= 20.85 time= 0.07\n","Epoch: 031 train_loss= 2076299.03 val_loss= 21.02 time= 0.11\n","Epoch: 041 train_loss= 2012840.08 val_loss= 21.41 time= 0.14\n","Epoch: 051 train_loss= 1951175.56 val_loss= 21.40 time= 0.18\n","Epoch: 061 train_loss= 1994603.25 val_loss= 21.40 time= 0.21\n","Epoch: 071 train_loss= 1873919.61 val_loss= 21.40 time= 0.25\n","Epoch: 081 train_loss= 1810237.01 val_loss= 21.40 time= 0.28\n","Epoch: 091 train_loss= 2052402.15 val_loss= 21.40 time= 0.32\n","Epoch: 101 train_loss= 2132241.40 val_loss= 21.40 time= 0.35\n","test error=194.18,0.10, mean test error=209.23,0.12\n","testing on day #48\n","Epoch: 001 train_loss= 2426985.09 val_loss= 40.52 time= 0.00\n","Epoch: 011 train_loss= 1894622.00 val_loss= 37.60 time= 0.04\n","Epoch: 021 train_loss= 2118236.95 val_loss= 35.50 time= 0.08\n","Epoch: 031 train_loss= 1945818.76 val_loss= 34.89 time= 0.11\n","Epoch: 041 train_loss= 1643668.68 val_loss= 35.12 time= 0.15\n","Epoch: 051 train_loss= 1828499.24 val_loss= 35.24 time= 0.18\n","Epoch: 061 train_loss= 2143488.03 val_loss= 35.26 time= 0.22\n","Epoch: 071 train_loss= 2197252.39 val_loss= 35.27 time= 0.26\n","Epoch: 081 train_loss= 1925893.38 val_loss= 35.27 time= 0.29\n","Epoch: 091 train_loss= 1766896.74 val_loss= 35.27 time= 0.33\n","Epoch: 101 train_loss= 2094047.02 val_loss= 35.27 time= 0.36\n","test error=223.38,0.12, mean test error=209.65,0.12\n","testing on day #49\n","Epoch: 001 train_loss= 2545554.99 val_loss= 19.43 time= 0.00\n","Epoch: 011 train_loss= 1896420.09 val_loss= 26.31 time= 0.04\n","Epoch: 021 train_loss= 2043474.80 val_loss= 22.59 time= 0.08\n","Epoch: 031 train_loss= 2003330.87 val_loss= 20.28 time= 0.11\n","Epoch: 041 train_loss= 1892332.26 val_loss= 20.54 time= 0.15\n","Epoch: 051 train_loss= 1860429.86 val_loss= 20.48 time= 0.19\n","Epoch: 061 train_loss= 1973273.66 val_loss= 20.46 time= 0.22\n","Epoch: 071 train_loss= 1828961.93 val_loss= 20.46 time= 0.26\n","Epoch: 081 train_loss= 1882314.42 val_loss= 20.47 time= 0.30\n","Epoch: 091 train_loss= 1861585.05 val_loss= 20.46 time= 0.33\n","Epoch: 101 train_loss= 1823140.83 val_loss= 20.46 time= 0.37\n","test error=221.68,0.11, mean test error=209.99,0.12\n","testing on day #50\n","Epoch: 001 train_loss= 2236126.13 val_loss= 45.70 time= 0.00\n","Epoch: 011 train_loss= 2055148.31 val_loss= 36.14 time= 0.04\n","Epoch: 021 train_loss= 2278976.96 val_loss= 39.61 time= 0.08\n","Epoch: 031 train_loss= 2239158.87 val_loss= 38.59 time= 0.12\n","Epoch: 041 train_loss= 1941526.17 val_loss= 37.46 time= 0.16\n","Epoch: 051 train_loss= 2071529.00 val_loss= 37.46 time= 0.19\n","Epoch: 061 train_loss= 2274805.14 val_loss= 37.44 time= 0.23\n","Epoch: 071 train_loss= 2059436.08 val_loss= 37.45 time= 0.27\n","Epoch: 081 train_loss= 1939782.68 val_loss= 37.45 time= 0.31\n","Epoch: 091 train_loss= 2134773.20 val_loss= 37.45 time= 0.34\n","Epoch: 101 train_loss= 2050198.10 val_loss= 37.45 time= 0.38\n","test error=230.69,0.13, mean test error=210.57,0.12\n","testing on day #51\n","Epoch: 001 train_loss= 2165327.35 val_loss= 21.95 time= 0.00\n","Epoch: 011 train_loss= 1890574.49 val_loss= 21.60 time= 0.04\n","Epoch: 021 train_loss= 2117065.55 val_loss= 21.58 time= 0.08\n","Epoch: 031 train_loss= 1954020.05 val_loss= 21.45 time= 0.12\n","Epoch: 041 train_loss= 2103677.98 val_loss= 21.57 time= 0.16\n","Epoch: 051 train_loss= 2124792.38 val_loss= 21.36 time= 0.20\n","Epoch: 061 train_loss= 1910779.05 val_loss= 21.16 time= 0.24\n","Epoch: 071 train_loss= 2103604.01 val_loss= 21.12 time= 0.27\n","Epoch: 081 train_loss= 2082834.50 val_loss= 21.13 time= 0.31\n","Epoch: 091 train_loss= 1773978.00 val_loss= 21.12 time= 0.35\n","Epoch: 101 train_loss= 2095942.20 val_loss= 21.12 time= 0.39\n","test error=213.83,0.11, mean test error=210.65,0.12\n","testing on day #52\n","Epoch: 001 train_loss= 2187423.53 val_loss= 69.92 time= 0.00\n","Epoch: 011 train_loss= 2001117.09 val_loss= 42.62 time= 0.04\n","Epoch: 021 train_loss= 2163083.85 val_loss= 43.78 time= 0.08\n","Epoch: 031 train_loss= 1922573.52 val_loss= 43.28 time= 0.12\n","Epoch: 041 train_loss= 1668204.66 val_loss= 42.98 time= 0.16\n","Epoch: 051 train_loss= 1942480.97 val_loss= 42.97 time= 0.20\n","Epoch: 061 train_loss= 1845111.11 val_loss= 42.97 time= 0.24\n","Epoch: 071 train_loss= 2076060.47 val_loss= 42.97 time= 0.28\n","Epoch: 081 train_loss= 1998301.49 val_loss= 42.96 time= 0.32\n","Epoch: 091 train_loss= 2098610.62 val_loss= 42.97 time= 0.36\n","Epoch: 101 train_loss= 2184171.39 val_loss= 42.96 time= 0.40\n","test error=281.90,0.15, mean test error=212.53,0.12\n","testing on day #53\n","Epoch: 001 train_loss= 2162253.29 val_loss= 33.11 time= 0.00\n","Epoch: 011 train_loss= 2201747.87 val_loss= 20.42 time= 0.05\n","Epoch: 021 train_loss= 2151532.68 val_loss= 20.07 time= 0.09\n","Epoch: 031 train_loss= 1966360.35 val_loss= 20.11 time= 0.13\n","Epoch: 041 train_loss= 2175016.54 val_loss= 20.07 time= 0.17\n","Epoch: 051 train_loss= 2166459.30 val_loss= 20.04 time= 0.21\n","Epoch: 061 train_loss= 2061337.69 val_loss= 20.03 time= 0.25\n","Epoch: 071 train_loss= 2001503.17 val_loss= 20.03 time= 0.29\n","Epoch: 081 train_loss= 2011857.77 val_loss= 20.03 time= 0.33\n","Epoch: 091 train_loss= 1936059.19 val_loss= 20.03 time= 0.37\n","Epoch: 101 train_loss= 2026277.46 val_loss= 20.03 time= 0.41\n","test error=219.01,0.13, mean test error=212.70,0.12\n","testing on day #54\n","Epoch: 001 train_loss= 2225330.40 val_loss= 34.15 time= 0.00\n","Epoch: 011 train_loss= 2111289.47 val_loss= 21.63 time= 0.05\n","Epoch: 021 train_loss= 2183350.44 val_loss= 21.78 time= 0.09\n","Epoch: 031 train_loss= 2184007.33 val_loss= 21.29 time= 0.13\n","Epoch: 041 train_loss= 2124614.85 val_loss= 21.28 time= 0.17\n","Epoch: 051 train_loss= 2281297.33 val_loss= 21.65 time= 0.21\n","Epoch: 061 train_loss= 1982158.20 val_loss= 21.51 time= 0.25\n","Epoch: 071 train_loss= 2142689.87 val_loss= 21.49 time= 0.30\n","Epoch: 081 train_loss= 2333678.94 val_loss= 21.49 time= 0.34\n","Epoch: 091 train_loss= 2123205.70 val_loss= 21.49 time= 0.38\n","Epoch: 101 train_loss= 2168909.39 val_loss= 21.49 time= 0.42\n","test error=160.52,0.12, mean test error=211.39,0.12\n","testing on day #55\n","Epoch: 001 train_loss= 2084854.16 val_loss= 22.59 time= 0.00\n","Epoch: 011 train_loss= 2217396.32 val_loss= 21.73 time= 0.05\n","Epoch: 021 train_loss= 1908992.22 val_loss= 21.50 time= 0.09\n","Epoch: 031 train_loss= 1895837.03 val_loss= 20.77 time= 0.13\n","Epoch: 041 train_loss= 1812973.14 val_loss= 20.41 time= 0.17\n","Epoch: 051 train_loss= 2072122.40 val_loss= 20.43 time= 0.22\n","Epoch: 061 train_loss= 1970909.17 val_loss= 20.43 time= 0.26\n","Epoch: 071 train_loss= 1990372.83 val_loss= 20.43 time= 0.30\n","Epoch: 081 train_loss= 2077387.14 val_loss= 20.43 time= 0.34\n","Epoch: 091 train_loss= 2108186.93 val_loss= 20.43 time= 0.39\n","Epoch: 101 train_loss= 1963975.50 val_loss= 20.43 time= 0.43\n","test error=113.93,0.09, mean test error=209.01,0.12\n","testing on day #56\n","Epoch: 001 train_loss= 2263165.20 val_loss= 20.38 time= 0.00\n","Epoch: 011 train_loss= 2087760.73 val_loss= 19.07 time= 0.05\n","Epoch: 021 train_loss= 1988528.20 val_loss= 18.77 time= 0.09\n","Epoch: 031 train_loss= 2075483.36 val_loss= 19.24 time= 0.13\n","Epoch: 041 train_loss= 2003832.82 val_loss= 19.49 time= 0.18\n","Epoch: 051 train_loss= 1874691.72 val_loss= 19.49 time= 0.22\n","Epoch: 061 train_loss= 2113949.32 val_loss= 19.50 time= 0.26\n","Epoch: 071 train_loss= 1995473.36 val_loss= 19.49 time= 0.31\n","Epoch: 081 train_loss= 1924122.98 val_loss= 19.50 time= 0.35\n","Epoch: 091 train_loss= 1944805.67 val_loss= 19.50 time= 0.39\n","Epoch: 101 train_loss= 2135357.37 val_loss= 19.49 time= 0.44\n","test error=122.29,0.10, mean test error=206.95,0.12\n","testing on day #57\n","Epoch: 001 train_loss= 1996535.14 val_loss= 12.24 time= 0.00\n","Epoch: 011 train_loss= 2004287.81 val_loss= 20.89 time= 0.05\n","Epoch: 021 train_loss= 1954687.61 val_loss= 18.66 time= 0.09\n","Epoch: 031 train_loss= 1859205.13 val_loss= 18.83 time= 0.14\n","Epoch: 041 train_loss= 2030374.09 val_loss= 18.76 time= 0.18\n","Epoch: 051 train_loss= 1902202.08 val_loss= 18.78 time= 0.22\n","Epoch: 061 train_loss= 1863536.58 val_loss= 18.78 time= 0.27\n","Epoch: 071 train_loss= 2047030.50 val_loss= 18.78 time= 0.31\n","Epoch: 081 train_loss= 1905400.19 val_loss= 18.78 time= 0.36\n","Epoch: 091 train_loss= 1916991.57 val_loss= 18.78 time= 0.40\n","Epoch: 101 train_loss= 2087446.77 val_loss= 18.78 time= 0.44\n","test error=135.27,0.09, mean test error=205.28,0.12\n","testing on day #58\n","Epoch: 001 train_loss= 1913756.15 val_loss= 12.47 time= 0.00\n","Epoch: 011 train_loss= 2013129.81 val_loss= 15.17 time= 0.05\n","Epoch: 021 train_loss= 2202189.44 val_loss= 17.32 time= 0.09\n","Epoch: 031 train_loss= 1981685.84 val_loss= 17.73 time= 0.14\n","Epoch: 041 train_loss= 2184955.18 val_loss= 18.10 time= 0.18\n","Epoch: 051 train_loss= 2032455.03 val_loss= 18.14 time= 0.23\n","Epoch: 061 train_loss= 2050153.33 val_loss= 18.14 time= 0.27\n","Epoch: 071 train_loss= 2021962.07 val_loss= 18.14 time= 0.32\n","Epoch: 081 train_loss= 2034154.34 val_loss= 18.14 time= 0.36\n","Epoch: 091 train_loss= 2011044.28 val_loss= 18.14 time= 0.41\n","Epoch: 101 train_loss= 2107509.46 val_loss= 18.14 time= 0.45\n","test error=136.96,0.12, mean test error=203.73,0.12\n","testing on day #59\n","Epoch: 001 train_loss= 2163468.40 val_loss= 11.08 time= 0.00\n","Epoch: 011 train_loss= 1998992.48 val_loss= 14.37 time= 0.05\n","Epoch: 021 train_loss= 1876361.60 val_loss= 16.53 time= 0.10\n","Epoch: 031 train_loss= 1909119.29 val_loss= 17.02 time= 0.14\n","Epoch: 041 train_loss= 1969020.00 val_loss= 16.90 time= 0.19\n","Epoch: 051 train_loss= 1969739.16 val_loss= 16.87 time= 0.23\n","Epoch: 061 train_loss= 1826978.04 val_loss= 16.87 time= 0.28\n","Epoch: 071 train_loss= 1826248.06 val_loss= 16.87 time= 0.32\n","Epoch: 081 train_loss= 1919022.85 val_loss= 16.87 time= 0.37\n","Epoch: 091 train_loss= 1855729.86 val_loss= 16.87 time= 0.42\n","Epoch: 101 train_loss= 1849395.31 val_loss= 16.87 time= 0.46\n","test error=150.07,0.11, mean test error=202.54,0.12\n","testing on day #60\n","Epoch: 001 train_loss= 2412993.14 val_loss= 14.98 time= 0.01\n","Epoch: 011 train_loss= 2045129.88 val_loss= 13.27 time= 0.05\n","Epoch: 021 train_loss= 1746829.26 val_loss= 13.48 time= 0.10\n","Epoch: 031 train_loss= 2071649.85 val_loss= 13.21 time= 0.15\n","Epoch: 041 train_loss= 1986457.83 val_loss= 12.69 time= 0.19\n","Epoch: 051 train_loss= 2270978.48 val_loss= 12.70 time= 0.24\n","Epoch: 061 train_loss= 2090314.45 val_loss= 12.70 time= 0.29\n","Epoch: 071 train_loss= 2059427.40 val_loss= 12.70 time= 0.34\n","Epoch: 081 train_loss= 2153517.06 val_loss= 12.70 time= 0.38\n","Epoch: 091 train_loss= 1937573.44 val_loss= 12.70 time= 0.43\n","Epoch: 101 train_loss= 2150185.34 val_loss= 12.70 time= 0.48\n","test error=165.83,0.18, mean test error=201.74,0.12\n","testing on day #61\n","Epoch: 001 train_loss= 2142567.17 val_loss= 12.64 time= 0.00\n","Epoch: 011 train_loss= 1868138.96 val_loss= 11.05 time= 0.05\n","Epoch: 021 train_loss= 1923235.73 val_loss= 10.78 time= 0.10\n","Epoch: 031 train_loss= 1937455.38 val_loss= 11.26 time= 0.15\n","Epoch: 041 train_loss= 1828975.02 val_loss= 10.97 time= 0.20\n","Epoch: 051 train_loss= 1974570.59 val_loss= 10.97 time= 0.25\n","Epoch: 061 train_loss= 2038014.93 val_loss= 10.97 time= 0.29\n","Epoch: 071 train_loss= 1907010.68 val_loss= 10.97 time= 0.34\n","Epoch: 081 train_loss= 2145245.50 val_loss= 10.97 time= 0.39\n","Epoch: 091 train_loss= 1898289.94 val_loss= 10.97 time= 0.44\n","Epoch: 101 train_loss= 1914170.30 val_loss= 10.97 time= 0.48\n","test error=127.74,0.11, mean test error=200.16,0.12\n","testing on day #62\n","Epoch: 001 train_loss= 2167230.85 val_loss= 12.54 time= 0.01\n","Epoch: 011 train_loss= 1943842.64 val_loss= 8.34 time= 0.05\n","Epoch: 021 train_loss= 1783321.14 val_loss= 9.01 time= 0.10\n","Epoch: 031 train_loss= 1976980.14 val_loss= 9.04 time= 0.15\n","Epoch: 041 train_loss= 2009006.45 val_loss= 9.01 time= 0.20\n","Epoch: 051 train_loss= 2136662.36 val_loss= 9.03 time= 0.25\n","Epoch: 061 train_loss= 2006368.33 val_loss= 9.02 time= 0.30\n","Epoch: 071 train_loss= 2325587.64 val_loss= 9.02 time= 0.35\n","Epoch: 081 train_loss= 2092755.63 val_loss= 9.02 time= 0.40\n","Epoch: 091 train_loss= 2117546.78 val_loss= 9.02 time= 0.44\n","Epoch: 101 train_loss= 1883525.82 val_loss= 9.02 time= 0.49\n","test error=136.68,0.13, mean test error=198.84,0.12\n","testing on day #63\n","Epoch: 001 train_loss= 2241312.15 val_loss= 17.27 time= 0.02\n","Epoch: 011 train_loss= 2099692.69 val_loss= 8.58 time= 0.07\n","Epoch: 021 train_loss= 2106598.21 val_loss= 6.86 time= 0.12\n","Epoch: 031 train_loss= 2074977.72 val_loss= 10.61 time= 0.17\n","Epoch: 041 train_loss= 1868888.05 val_loss= 7.88 time= 0.22\n","Epoch: 051 train_loss= 1927945.60 val_loss= 7.76 time= 0.27\n","Epoch: 061 train_loss= 1883660.49 val_loss= 7.67 time= 0.32\n","Epoch: 071 train_loss= 1918090.83 val_loss= 7.66 time= 0.37\n","Epoch: 081 train_loss= 2045522.17 val_loss= 7.66 time= 0.42\n","Epoch: 091 train_loss= 1769646.67 val_loss= 7.66 time= 0.46\n","Epoch: 101 train_loss= 1731343.11 val_loss= 7.66 time= 0.51\n","test error=126.64,0.09, mean test error=197.37,0.12\n","testing on day #64\n","Epoch: 001 train_loss= 2204480.30 val_loss= 16.17 time= 0.02\n","Epoch: 011 train_loss= 1976237.82 val_loss= 4.69 time= 0.07\n","Epoch: 021 train_loss= 2228997.35 val_loss= 4.97 time= 0.12\n","test error=130.98,0.12, mean test error=196.04,0.12\n","testing on day #65\n","Epoch: 001 train_loss= 2061808.11 val_loss= 13.58 time= 0.02\n","Epoch: 011 train_loss= 1882159.83 val_loss= 4.31 time= 0.07\n","Epoch: 021 train_loss= 1875498.23 val_loss= 4.31 time= 0.12\n","Epoch: 031 train_loss= 2053545.78 val_loss= 4.28 time= 0.17\n","Epoch: 041 train_loss= 1812013.23 val_loss= 4.31 time= 0.22\n","Epoch: 051 train_loss= 1702052.54 val_loss= 4.29 time= 0.27\n","Epoch: 061 train_loss= 1805645.08 val_loss= 4.29 time= 0.33\n","Epoch: 071 train_loss= 1907522.96 val_loss= 4.29 time= 0.38\n","Epoch: 081 train_loss= 2021587.19 val_loss= 4.29 time= 0.43\n","Epoch: 091 train_loss= 1858716.51 val_loss= 4.29 time= 0.48\n","Epoch: 101 train_loss= 1916228.84 val_loss= 4.29 time= 0.53\n","test error=111.61,0.09, mean test error=194.39,0.12\n","testing on day #66\n","Epoch: 001 train_loss= 2331694.48 val_loss= 14.01 time= 0.02\n","Epoch: 011 train_loss= 1967485.82 val_loss= 4.22 time= 0.07\n","Epoch: 021 train_loss= 1907866.88 val_loss= 3.97 time= 0.12\n","test error=121.91,0.12, mean test error=192.99,0.12\n","testing on day #67\n","Epoch: 001 train_loss= 2078906.31 val_loss= 17.32 time= 0.01\n","Epoch: 011 train_loss= 1859482.71 val_loss= 4.21 time= 0.06\n","Epoch: 021 train_loss= 1795872.73 val_loss= 4.42 time= 0.12\n","test error=118.20,0.11, mean test error=191.58,0.12\n","testing on day #68\n","Epoch: 001 train_loss= 2103830.61 val_loss= 14.70 time= 0.02\n","Epoch: 011 train_loss= 1853390.33 val_loss= 4.04 time= 0.07\n","Epoch: 021 train_loss= 2203883.37 val_loss= 3.90 time= 0.13\n","Epoch: 031 train_loss= 2277638.21 val_loss= 4.06 time= 0.18\n","Epoch: 041 train_loss= 1789430.44 val_loss= 3.76 time= 0.24\n","Epoch: 051 train_loss= 1769783.16 val_loss= 3.75 time= 0.29\n","Epoch: 061 train_loss= 1832909.15 val_loss= 3.76 time= 0.35\n","Epoch: 071 train_loss= 2035496.36 val_loss= 3.76 time= 0.40\n","Epoch: 081 train_loss= 2018980.19 val_loss= 3.76 time= 0.46\n","Epoch: 091 train_loss= 1881812.60 val_loss= 3.76 time= 0.51\n","Epoch: 101 train_loss= 2065506.98 val_loss= 3.76 time= 0.56\n","test error=117.13,0.13, mean test error=190.20,0.12\n","testing on day #69\n","Epoch: 001 train_loss= 1820018.46 val_loss= 12.19 time= 0.01\n","Epoch: 011 train_loss= 1795362.46 val_loss= 4.49 time= 0.07\n","Epoch: 021 train_loss= 1757987.63 val_loss= 5.14 time= 0.12\n","Epoch: 031 train_loss= 1909638.95 val_loss= 3.88 time= 0.18\n","Epoch: 041 train_loss= 1762938.87 val_loss= 3.85 time= 0.24\n","Epoch: 051 train_loss= 1777953.24 val_loss= 3.83 time= 0.29\n","Epoch: 061 train_loss= 1769786.93 val_loss= 3.90 time= 0.35\n","Epoch: 071 train_loss= 1718601.08 val_loss= 3.86 time= 0.40\n","Epoch: 081 train_loss= 1807991.09 val_loss= 3.85 time= 0.46\n","Epoch: 091 train_loss= 1846876.01 val_loss= 3.85 time= 0.51\n","Epoch: 101 train_loss= 1811092.03 val_loss= 3.85 time= 0.57\n","test error=93.92,0.09, mean test error=188.45,0.12\n","testing on day #70\n","Epoch: 001 train_loss= 1828413.44 val_loss= 9.29 time= 0.01\n","Epoch: 011 train_loss= 1934425.73 val_loss= 3.85 time= 0.06\n","Epoch: 021 train_loss= 1912451.17 val_loss= 4.06 time= 0.12\n","Epoch: 031 train_loss= 2003828.50 val_loss= 3.79 time= 0.18\n","Epoch: 041 train_loss= 2019368.81 val_loss= 3.78 time= 0.23\n","Epoch: 051 train_loss= 1767249.86 val_loss= 3.78 time= 0.29\n","Epoch: 061 train_loss= 1705893.10 val_loss= 3.78 time= 0.34\n","Epoch: 071 train_loss= 1843951.55 val_loss= 3.78 time= 0.40\n","Epoch: 081 train_loss= 1860347.53 val_loss= 3.78 time= 0.46\n","Epoch: 091 train_loss= 1937769.56 val_loss= 3.78 time= 0.51\n","Epoch: 101 train_loss= 1890391.39 val_loss= 3.78 time= 0.57\n","test error=118.82,0.13, mean test error=187.21,0.12\n","testing on day #71\n","Epoch: 001 train_loss= 1861489.46 val_loss= 17.57 time= 0.01\n","Epoch: 011 train_loss= 1826887.36 val_loss= 3.67 time= 0.06\n","Epoch: 021 train_loss= 1764653.18 val_loss= 3.64 time= 0.12\n","Epoch: 031 train_loss= 1908974.52 val_loss= 3.88 time= 0.18\n","Epoch: 041 train_loss= 1789332.20 val_loss= 3.66 time= 0.23\n","Epoch: 051 train_loss= 1735914.60 val_loss= 3.65 time= 0.29\n","Epoch: 061 train_loss= 1761939.34 val_loss= 3.65 time= 0.35\n","Epoch: 071 train_loss= 1587761.92 val_loss= 3.65 time= 0.41\n","Epoch: 081 train_loss= 1591069.91 val_loss= 3.65 time= 0.46\n","Epoch: 091 train_loss= 1883450.47 val_loss= 3.65 time= 0.52\n","Epoch: 101 train_loss= 1809276.23 val_loss= 3.65 time= 0.58\n","test error=92.92,0.09, mean test error=185.55,0.12\n","testing on day #72\n","Epoch: 001 train_loss= 1874281.81 val_loss= 13.83 time= 0.01\n","Epoch: 011 train_loss= 1830536.87 val_loss= 4.16 time= 0.06\n","Epoch: 021 train_loss= 1892941.24 val_loss= 3.94 time= 0.12\n","test error=109.72,0.13, mean test error=184.25,0.12\n","testing on day #73\n","Epoch: 001 train_loss= 1861437.08 val_loss= 16.73 time= 0.01\n","Epoch: 011 train_loss= 1664797.96 val_loss= 4.33 time= 0.07\n","Epoch: 021 train_loss= 1704040.83 val_loss= 3.47 time= 0.12\n","Epoch: 031 train_loss= 1665110.15 val_loss= 3.44 time= 0.18\n","Epoch: 041 train_loss= 1774857.46 val_loss= 3.48 time= 0.24\n","Epoch: 051 train_loss= 1690104.17 val_loss= 3.48 time= 0.30\n","Epoch: 061 train_loss= 1831588.54 val_loss= 3.48 time= 0.36\n","Epoch: 071 train_loss= 1636866.54 val_loss= 3.48 time= 0.42\n","Epoch: 081 train_loss= 1779380.43 val_loss= 3.48 time= 0.47\n","Epoch: 091 train_loss= 1664087.37 val_loss= 3.48 time= 0.53\n","Epoch: 101 train_loss= 1667980.20 val_loss= 3.48 time= 0.59\n","test error=102.71,0.11, mean test error=182.86,0.12\n","testing on day #74\n","Epoch: 001 train_loss= 1732506.96 val_loss= 7.56 time= 0.01\n","Epoch: 011 train_loss= 1746141.69 val_loss= 4.08 time= 0.07\n","Epoch: 021 train_loss= 1925013.44 val_loss= 3.90 time= 0.13\n","Epoch: 031 train_loss= 1628414.71 val_loss= 3.81 time= 0.19\n","Epoch: 041 train_loss= 1758139.09 val_loss= 3.83 time= 0.25\n","Epoch: 051 train_loss= 1731814.01 val_loss= 3.84 time= 0.31\n","Epoch: 061 train_loss= 1823332.26 val_loss= 3.83 time= 0.37\n","Epoch: 071 train_loss= 1774646.67 val_loss= 3.82 time= 0.42\n","Epoch: 081 train_loss= 1695903.90 val_loss= 3.81 time= 0.48\n","Epoch: 091 train_loss= 1636001.66 val_loss= 3.81 time= 0.54\n","Epoch: 101 train_loss= 1738208.73 val_loss= 3.81 time= 0.60\n","test error=125.68,0.14, mean test error=181.91,0.12\n","testing on day #75\n","Epoch: 001 train_loss= 1797867.94 val_loss= 10.10 time= 0.01\n","Epoch: 011 train_loss= 1830674.56 val_loss= 2.99 time= 0.07\n","Epoch: 021 train_loss= 1934440.27 val_loss= 3.07 time= 0.13\n","Epoch: 031 train_loss= 1911386.95 val_loss= 3.04 time= 0.19\n","Epoch: 041 train_loss= 1692075.49 val_loss= 3.21 time= 0.25\n","Epoch: 051 train_loss= 1799009.72 val_loss= 3.04 time= 0.31\n","Epoch: 061 train_loss= 1694570.52 val_loss= 3.10 time= 0.37\n","Epoch: 071 train_loss= 1629023.36 val_loss= 3.10 time= 0.43\n","Epoch: 081 train_loss= 1619684.54 val_loss= 3.10 time= 0.49\n","Epoch: 091 train_loss= 1549195.61 val_loss= 3.10 time= 0.55\n","Epoch: 101 train_loss= 1634528.91 val_loss= 3.10 time= 0.61\n","test error=111.28,0.11, mean test error=180.75,0.12\n","testing on day #76\n","Epoch: 001 train_loss= 1791209.03 val_loss= 8.86 time= 0.01\n","Epoch: 011 train_loss= 2113124.58 val_loss= 4.67 time= 0.07\n","Epoch: 021 train_loss= 1744050.70 val_loss= 4.55 time= 0.13\n","Epoch: 031 train_loss= 1681838.01 val_loss= 4.62 time= 0.19\n","Epoch: 041 train_loss= 1684768.73 val_loss= 4.61 time= 0.26\n","Epoch: 051 train_loss= 1703613.38 val_loss= 4.62 time= 0.32\n","Epoch: 061 train_loss= 1669177.21 val_loss= 4.62 time= 0.38\n","Epoch: 071 train_loss= 1610601.92 val_loss= 4.62 time= 0.44\n","Epoch: 081 train_loss= 1655700.83 val_loss= 4.62 time= 0.50\n","Epoch: 091 train_loss= 1751717.72 val_loss= 4.62 time= 0.56\n","Epoch: 101 train_loss= 1785286.77 val_loss= 4.62 time= 0.63\n","test error=114.13,0.12, mean test error=179.68,0.12\n","testing on day #77\n","Epoch: 001 train_loss= 1856582.76 val_loss= 12.04 time= 0.01\n","Epoch: 011 train_loss= 1858832.82 val_loss= 4.61 time= 0.07\n","Epoch: 021 train_loss= 1671976.33 val_loss= 5.05 time= 0.13\n","Epoch: 031 train_loss= 1621923.62 val_loss= 4.95 time= 0.20\n","Epoch: 041 train_loss= 1573019.35 val_loss= 4.62 time= 0.26\n","Epoch: 051 train_loss= 1665408.48 val_loss= 4.59 time= 0.32\n","Epoch: 061 train_loss= 1496989.21 val_loss= 4.59 time= 0.38\n","Epoch: 071 train_loss= 1571185.89 val_loss= 4.59 time= 0.45\n","Epoch: 081 train_loss= 1572374.12 val_loss= 4.59 time= 0.51\n","Epoch: 091 train_loss= 1651031.60 val_loss= 4.59 time= 0.57\n","Epoch: 101 train_loss= 1632920.84 val_loss= 4.59 time= 0.63\n","test error=127.29,0.12, mean test error=178.85,0.12\n","testing on day #78\n","Epoch: 001 train_loss= 1763472.63 val_loss= 15.42 time= 0.01\n","Epoch: 011 train_loss= 2049591.82 val_loss= 5.52 time= 0.07\n","Epoch: 021 train_loss= 1776828.50 val_loss= 5.61 time= 0.13\n","Epoch: 031 train_loss= 1472178.37 val_loss= 5.56 time= 0.20\n","Epoch: 041 train_loss= 1834745.69 val_loss= 5.55 time= 0.26\n","Epoch: 051 train_loss= 1756053.00 val_loss= 5.54 time= 0.32\n","Epoch: 061 train_loss= 1769419.35 val_loss= 5.54 time= 0.39\n","Epoch: 071 train_loss= 1562100.56 val_loss= 5.54 time= 0.45\n","Epoch: 081 train_loss= 1703597.99 val_loss= 5.54 time= 0.51\n","Epoch: 091 train_loss= 1729826.40 val_loss= 5.54 time= 0.58\n","Epoch: 101 train_loss= 1671986.01 val_loss= 5.54 time= 0.64\n","test error=99.13,0.13, mean test error=177.60,0.12\n","testing on day #79\n","Epoch: 001 train_loss= 1766798.85 val_loss= 14.07 time= 0.01\n","Epoch: 011 train_loss= 1728598.44 val_loss= 5.96 time= 0.07\n","Epoch: 021 train_loss= 1713052.71 val_loss= 6.15 time= 0.14\n","Epoch: 031 train_loss= 1696067.15 val_loss= 5.86 time= 0.20\n","Epoch: 041 train_loss= 1529134.04 val_loss= 5.90 time= 0.26\n","Epoch: 051 train_loss= 1753794.65 val_loss= 5.91 time= 0.33\n","Epoch: 061 train_loss= 1526646.09 val_loss= 5.91 time= 0.39\n","Epoch: 071 train_loss= 1571921.06 val_loss= 5.91 time= 0.46\n","Epoch: 081 train_loss= 1542608.42 val_loss= 5.91 time= 0.52\n","Epoch: 091 train_loss= 1654440.80 val_loss= 5.91 time= 0.58\n","Epoch: 101 train_loss= 1488936.81 val_loss= 5.91 time= 0.65\n","test error=87.12,0.10, mean test error=176.21,0.12\n","testing on day #80\n","Epoch: 001 train_loss= 1803695.84 val_loss= 11.93 time= 0.01\n","Epoch: 011 train_loss= 1746236.39 val_loss= 6.18 time= 0.07\n","Epoch: 021 train_loss= 1745005.54 val_loss= 5.63 time= 0.14\n","Epoch: 031 train_loss= 1684078.80 val_loss= 5.59 time= 0.20\n","Epoch: 041 train_loss= 1668937.69 val_loss= 5.54 time= 0.27\n","Epoch: 051 train_loss= 1736504.66 val_loss= 5.54 time= 0.33\n","Epoch: 061 train_loss= 1733873.48 val_loss= 5.54 time= 0.40\n","Epoch: 071 train_loss= 1662350.34 val_loss= 5.54 time= 0.46\n","Epoch: 081 train_loss= 1621480.79 val_loss= 5.54 time= 0.53\n","Epoch: 091 train_loss= 1781729.81 val_loss= 5.54 time= 0.59\n","Epoch: 101 train_loss= 1635988.61 val_loss= 5.54 time= 0.66\n","test error=106.76,0.15, mean test error=175.16,0.12\n","testing on day #81\n","Epoch: 001 train_loss= 1702092.28 val_loss= 17.40 time= 0.01\n","Epoch: 011 train_loss= 1600707.44 val_loss= 7.33 time= 0.07\n","Epoch: 021 train_loss= 1563145.12 val_loss= 5.85 time= 0.14\n","Epoch: 031 train_loss= 1697883.99 val_loss= 6.20 time= 0.21\n","Epoch: 041 train_loss= 1689039.17 val_loss= 6.24 time= 0.27\n","Epoch: 051 train_loss= 1552987.76 val_loss= 6.21 time= 0.34\n","Epoch: 061 train_loss= 1483023.99 val_loss= 6.20 time= 0.40\n","Epoch: 071 train_loss= 1494808.40 val_loss= 6.21 time= 0.47\n","Epoch: 081 train_loss= 1672642.44 val_loss= 6.21 time= 0.53\n","Epoch: 091 train_loss= 1612274.79 val_loss= 6.21 time= 0.60\n","Epoch: 101 train_loss= 1587883.03 val_loss= 6.21 time= 0.67\n","test error=115.54,0.13, mean test error=174.27,0.12\n","testing on day #82\n","Epoch: 001 train_loss= 1633542.32 val_loss= 8.57 time= 0.01\n","Epoch: 011 train_loss= 1770345.68 val_loss= 5.75 time= 0.07\n","Epoch: 021 train_loss= 1681863.25 val_loss= 5.57 time= 0.14\n","Epoch: 031 train_loss= 1580201.50 val_loss= 5.67 time= 0.21\n","Epoch: 041 train_loss= 1666949.61 val_loss= 5.68 time= 0.27\n","Epoch: 051 train_loss= 1787630.71 val_loss= 5.68 time= 0.34\n","Epoch: 061 train_loss= 1739173.11 val_loss= 5.68 time= 0.41\n","Epoch: 071 train_loss= 1737536.85 val_loss= 5.68 time= 0.47\n","Epoch: 081 train_loss= 1658548.95 val_loss= 5.68 time= 0.54\n","Epoch: 091 train_loss= 1694381.65 val_loss= 5.68 time= 0.61\n","Epoch: 101 train_loss= 1660546.56 val_loss= 5.68 time= 0.67\n","test error=138.74,0.16, mean test error=173.75,0.12\n","testing on day #83\n","Epoch: 001 train_loss= 1709298.68 val_loss= 11.22 time= 0.01\n","Epoch: 011 train_loss= 1763337.42 val_loss= 8.23 time= 0.08\n","Epoch: 021 train_loss= 1560501.81 val_loss= 8.00 time= 0.14\n","Epoch: 031 train_loss= 1474657.11 val_loss= 8.06 time= 0.21\n","Epoch: 041 train_loss= 1654865.47 val_loss= 8.04 time= 0.28\n","Epoch: 051 train_loss= 1483348.46 val_loss= 8.04 time= 0.35\n","Epoch: 061 train_loss= 1553476.62 val_loss= 8.04 time= 0.41\n","Epoch: 071 train_loss= 1505366.09 val_loss= 8.04 time= 0.48\n","Epoch: 081 train_loss= 1696073.24 val_loss= 8.04 time= 0.55\n","Epoch: 091 train_loss= 1437660.38 val_loss= 8.04 time= 0.61\n","Epoch: 101 train_loss= 1453994.17 val_loss= 8.04 time= 0.68\n","test error=101.76,0.12, mean test error=172.70,0.12\n","testing on day #84\n","Epoch: 001 train_loss= 1886103.60 val_loss= 11.73 time= 0.01\n","Epoch: 011 train_loss= 1718925.40 val_loss= 8.32 time= 0.08\n","Epoch: 021 train_loss= 1706969.40 val_loss= 7.98 time= 0.15\n","test error=101.87,0.15, mean test error=171.69,0.12\n","testing on day #85\n","Epoch: 001 train_loss= 1968406.61 val_loss= 21.17 time= 0.01\n","Epoch: 011 train_loss= 1517798.95 val_loss= 8.97 time= 0.08\n","Epoch: 021 train_loss= 1514220.84 val_loss= 9.02 time= 0.15\n","test error=75.72,0.10, mean test error=170.34,0.12\n","testing on day #86\n","Epoch: 001 train_loss= 1862327.06 val_loss= 16.21 time= 0.01\n","Epoch: 011 train_loss= 1667947.66 val_loss= 7.89 time= 0.08\n","Epoch: 021 train_loss= 1856977.88 val_loss= 7.12 time= 0.15\n","Epoch: 031 train_loss= 1612036.80 val_loss= 7.13 time= 0.22\n","Epoch: 041 train_loss= 1547407.40 val_loss= 6.78 time= 0.29\n","Epoch: 051 train_loss= 1548185.87 val_loss= 6.77 time= 0.36\n","Epoch: 061 train_loss= 1752857.53 val_loss= 6.77 time= 0.43\n","Epoch: 071 train_loss= 1583610.63 val_loss= 6.77 time= 0.50\n","Epoch: 081 train_loss= 1659505.33 val_loss= 6.77 time= 0.57\n","Epoch: 091 train_loss= 1581327.39 val_loss= 6.77 time= 0.65\n","Epoch: 101 train_loss= 1576945.24 val_loss= 6.77 time= 0.72\n","test error=102.33,0.13, mean test error=169.39,0.12\n","testing on day #87\n","Epoch: 001 train_loss= 1670789.88 val_loss= 10.87 time= 0.01\n","Epoch: 011 train_loss= 1568340.99 val_loss= 7.12 time= 0.08\n","Epoch: 021 train_loss= 1648797.72 val_loss= 7.10 time= 0.15\n","test error=108.98,0.13, mean test error=168.57,0.12\n","testing on day #88\n","Epoch: 001 train_loss= 1861134.61 val_loss= 11.22 time= 0.01\n","Epoch: 011 train_loss= 1615148.29 val_loss= 6.04 time= 0.08\n","Epoch: 021 train_loss= 1671151.66 val_loss= 5.94 time= 0.15\n","test error=134.59,0.17, mean test error=168.11,0.12\n","testing on day #89\n","Epoch: 001 train_loss= 1666641.44 val_loss= 9.09 time= 0.01\n","Epoch: 011 train_loss= 1428343.61 val_loss= 6.14 time= 0.08\n","Epoch: 021 train_loss= 1476607.47 val_loss= 6.27 time= 0.15\n","Epoch: 031 train_loss= 1599050.65 val_loss= 6.16 time= 0.23\n","Epoch: 041 train_loss= 1517531.69 val_loss= 6.20 time= 0.30\n","Epoch: 051 train_loss= 1451260.06 val_loss= 6.21 time= 0.37\n","Epoch: 061 train_loss= 1494371.50 val_loss= 6.21 time= 0.45\n","Epoch: 071 train_loss= 1497521.12 val_loss= 6.21 time= 0.52\n","Epoch: 081 train_loss= 1404112.78 val_loss= 6.21 time= 0.59\n","Epoch: 091 train_loss= 1534801.16 val_loss= 6.21 time= 0.67\n","Epoch: 101 train_loss= 1406454.54 val_loss= 6.21 time= 0.74\n","test error=122.87,0.13, mean test error=167.50,0.12\n","testing on day #90\n","Epoch: 001 train_loss= 1752928.46 val_loss= 12.29 time= 0.01\n","Epoch: 011 train_loss= 1676352.63 val_loss= 6.91 time= 0.08\n","Epoch: 021 train_loss= 1558951.88 val_loss= 7.02 time= 0.16\n","test error=126.64,0.15, mean test error=166.97,0.12\n","testing on day #91\n","Epoch: 001 train_loss= 1759141.64 val_loss= 13.57 time= 0.01\n","Epoch: 011 train_loss= 1678921.26 val_loss= 8.04 time= 0.08\n","Epoch: 021 train_loss= 1383132.57 val_loss= 6.74 time= 0.16\n","Epoch: 031 train_loss= 1401749.55 val_loss= 6.64 time= 0.24\n","Epoch: 041 train_loss= 1428884.29 val_loss= 6.54 time= 0.31\n","Epoch: 051 train_loss= 1588989.53 val_loss= 6.54 time= 0.39\n","Epoch: 061 train_loss= 1544372.23 val_loss= 6.54 time= 0.46\n","Epoch: 071 train_loss= 1509122.69 val_loss= 6.52 time= 0.54\n","Epoch: 081 train_loss= 1440667.40 val_loss= 6.52 time= 0.61\n","Epoch: 091 train_loss= 1373496.95 val_loss= 6.52 time= 0.69\n","Epoch: 101 train_loss= 1407037.49 val_loss= 6.52 time= 0.76\n","test error=102.58,0.11, mean test error=166.13,0.12\n","testing on day #92\n","Epoch: 001 train_loss= 1750883.92 val_loss= 10.90 time= 0.01\n","Epoch: 011 train_loss= 1599543.33 val_loss= 9.85 time= 0.09\n","Epoch: 021 train_loss= 1587654.74 val_loss= 6.16 time= 0.16\n","Epoch: 031 train_loss= 1581584.73 val_loss= 6.19 time= 0.24\n","Epoch: 041 train_loss= 1474259.11 val_loss= 6.18 time= 0.31\n","Epoch: 051 train_loss= 1504140.45 val_loss= 6.18 time= 0.39\n","Epoch: 061 train_loss= 1441086.58 val_loss= 6.18 time= 0.47\n","Epoch: 071 train_loss= 1459322.87 val_loss= 6.18 time= 0.54\n","Epoch: 081 train_loss= 1392824.46 val_loss= 6.18 time= 0.62\n","Epoch: 091 train_loss= 1592008.57 val_loss= 6.18 time= 0.70\n","Epoch: 101 train_loss= 1480227.67 val_loss= 6.18 time= 0.77\n","test error=104.99,0.14, mean test error=165.35,0.12\n","testing on day #93\n","Epoch: 001 train_loss= 1793797.78 val_loss= 12.76 time= 0.01\n","Epoch: 011 train_loss= 1576675.58 val_loss= 7.80 time= 0.09\n","Epoch: 021 train_loss= 1448037.21 val_loss= 5.68 time= 0.16\n","Epoch: 031 train_loss= 1461363.24 val_loss= 5.64 time= 0.24\n","Epoch: 041 train_loss= 1458044.63 val_loss= 5.62 time= 0.32\n","Epoch: 051 train_loss= 1345779.23 val_loss= 5.62 time= 0.39\n","Epoch: 061 train_loss= 1356008.58 val_loss= 5.62 time= 0.47\n","Epoch: 071 train_loss= 1501858.37 val_loss= 5.62 time= 0.55\n","Epoch: 081 train_loss= 1400044.04 val_loss= 5.62 time= 0.62\n","Epoch: 091 train_loss= 1389149.85 val_loss= 5.62 time= 0.70\n","Epoch: 101 train_loss= 1470930.34 val_loss= 5.62 time= 0.78\n","test error=101.10,0.15, mean test error=164.53,0.12\n","testing on day #94\n","Epoch: 001 train_loss= 2001510.55 val_loss= 19.28 time= 0.01\n","Epoch: 011 train_loss= 1600205.45 val_loss= 5.70 time= 0.09\n","Epoch: 021 train_loss= 1580614.68 val_loss= 5.15 time= 0.16\n","Epoch: 031 train_loss= 1620224.01 val_loss= 5.02 time= 0.24\n","Epoch: 041 train_loss= 1521662.51 val_loss= 5.05 time= 0.32\n","Epoch: 051 train_loss= 1617515.45 val_loss= 5.05 time= 0.40\n","Epoch: 061 train_loss= 1435544.61 val_loss= 5.05 time= 0.48\n","Epoch: 071 train_loss= 1563840.80 val_loss= 5.05 time= 0.55\n","Epoch: 081 train_loss= 1527844.03 val_loss= 5.05 time= 0.63\n","Epoch: 091 train_loss= 1552074.22 val_loss= 5.05 time= 0.71\n","Epoch: 101 train_loss= 1510626.63 val_loss= 5.05 time= 0.79\n","test error=117.98,0.16, mean test error=163.95,0.12\n","testing on day #95\n","Epoch: 001 train_loss= 1660936.15 val_loss= 12.09 time= 0.01\n","Epoch: 011 train_loss= 1427366.12 val_loss= 6.48 time= 0.09\n","Epoch: 021 train_loss= 1414722.97 val_loss= 5.66 time= 0.17\n","Epoch: 031 train_loss= 1422835.29 val_loss= 5.59 time= 0.25\n","Epoch: 041 train_loss= 1444885.19 val_loss= 5.78 time= 0.32\n","Epoch: 051 train_loss= 1415680.66 val_loss= 5.40 time= 0.40\n","Epoch: 061 train_loss= 1414549.70 val_loss= 5.48 time= 0.48\n","Epoch: 071 train_loss= 1449652.09 val_loss= 5.48 time= 0.56\n","Epoch: 081 train_loss= 1481556.62 val_loss= 5.48 time= 0.64\n","Epoch: 091 train_loss= 1484963.36 val_loss= 5.48 time= 0.72\n","Epoch: 101 train_loss= 1350063.42 val_loss= 5.48 time= 0.80\n","test error=119.21,0.14, mean test error=163.40,0.12\n","testing on day #96\n","Epoch: 001 train_loss= 1790975.77 val_loss= 18.33 time= 0.01\n","Epoch: 011 train_loss= 1523953.64 val_loss= 9.05 time= 0.09\n","Epoch: 021 train_loss= 1457965.34 val_loss= 5.93 time= 0.17\n","Epoch: 031 train_loss= 1547932.04 val_loss= 5.91 time= 0.25\n","Epoch: 041 train_loss= 1444481.94 val_loss= 5.90 time= 0.33\n","Epoch: 051 train_loss= 1433790.53 val_loss= 5.91 time= 0.41\n","Epoch: 061 train_loss= 1395468.55 val_loss= 5.91 time= 0.49\n","Epoch: 071 train_loss= 1445565.56 val_loss= 5.91 time= 0.57\n","Epoch: 081 train_loss= 1488574.02 val_loss= 5.91 time= 0.65\n","Epoch: 091 train_loss= 1493029.62 val_loss= 5.91 time= 0.73\n","Epoch: 101 train_loss= 1399950.99 val_loss= 5.91 time= 0.80\n","test error=113.93,0.15, mean test error=162.80,0.12\n","testing on day #97\n","Epoch: 001 train_loss= 1583603.87 val_loss= 14.63 time= 0.01\n","Epoch: 011 train_loss= 1620662.19 val_loss= 8.58 time= 0.09\n","Epoch: 021 train_loss= 1314473.98 val_loss= 6.84 time= 0.17\n","Epoch: 031 train_loss= 1305254.81 val_loss= 7.16 time= 0.25\n","Epoch: 041 train_loss= 1327644.49 val_loss= 7.18 time= 0.33\n","Epoch: 051 train_loss= 1451715.96 val_loss= 7.17 time= 0.41\n","Epoch: 061 train_loss= 1302856.68 val_loss= 7.16 time= 0.49\n","Epoch: 071 train_loss= 1403028.60 val_loss= 7.16 time= 0.57\n","Epoch: 081 train_loss= 1388783.65 val_loss= 7.16 time= 0.65\n","Epoch: 091 train_loss= 1361444.12 val_loss= 7.16 time= 0.73\n","Epoch: 101 train_loss= 1481570.90 val_loss= 7.16 time= 0.81\n","test error=90.58,0.12, mean test error=161.93,0.12\n","testing on day #98\n","Epoch: 001 train_loss= 1548842.24 val_loss= 7.29 time= 0.01\n","Epoch: 011 train_loss= 1633579.39 val_loss= 5.93 time= 0.09\n","Epoch: 021 train_loss= 1512464.23 val_loss= 6.56 time= 0.17\n","Epoch: 031 train_loss= 1320225.33 val_loss= 6.23 time= 0.25\n","Epoch: 041 train_loss= 1459837.27 val_loss= 6.12 time= 0.33\n","Epoch: 051 train_loss= 1607587.07 val_loss= 6.12 time= 0.41\n","Epoch: 061 train_loss= 1583762.44 val_loss= 6.12 time= 0.50\n","Epoch: 071 train_loss= 1315374.88 val_loss= 6.12 time= 0.58\n","Epoch: 081 train_loss= 1455085.82 val_loss= 6.11 time= 0.66\n","Epoch: 091 train_loss= 1470482.11 val_loss= 6.12 time= 0.74\n","Epoch: 101 train_loss= 1497735.62 val_loss= 6.12 time= 0.82\n","test error=111.23,0.16, mean test error=161.32,0.12\n","testing on day #99\n","Epoch: 001 train_loss= 1392257.66 val_loss= 7.67 time= 0.01\n","Epoch: 011 train_loss= 1664500.78 val_loss= 5.95 time= 0.09\n","Epoch: 021 train_loss= 1690342.87 val_loss= 5.82 time= 0.17\n","Epoch: 031 train_loss= 1557699.49 val_loss= 6.15 time= 0.26\n","Epoch: 041 train_loss= 1489638.55 val_loss= 7.00 time= 0.34\n","Epoch: 051 train_loss= 1287937.27 val_loss= 6.83 time= 0.42\n","Epoch: 061 train_loss= 1317870.21 val_loss= 6.86 time= 0.50\n","Epoch: 071 train_loss= 1342264.58 val_loss= 6.86 time= 0.58\n","Epoch: 081 train_loss= 1373725.52 val_loss= 6.87 time= 0.67\n","Epoch: 091 train_loss= 1306394.35 val_loss= 6.86 time= 0.75\n","Epoch: 101 train_loss= 1379664.06 val_loss= 6.86 time= 0.83\n","test error=83.96,0.12, mean test error=160.41,0.12\n","testing on day #100\n","Epoch: 001 train_loss= 1458009.62 val_loss= 7.91 time= 0.01\n","Epoch: 011 train_loss= 1758908.20 val_loss= 5.56 time= 0.09\n","Epoch: 021 train_loss= 1509149.06 val_loss= 5.87 time= 0.18\n","Epoch: 031 train_loss= 1422149.37 val_loss= 5.65 time= 0.26\n","Epoch: 041 train_loss= 1469165.86 val_loss= 5.63 time= 0.34\n","Epoch: 051 train_loss= 1439552.24 val_loss= 5.63 time= 0.43\n","Epoch: 061 train_loss= 1441204.09 val_loss= 5.63 time= 0.51\n","Epoch: 071 train_loss= 1597696.43 val_loss= 5.63 time= 0.59\n","Epoch: 081 train_loss= 1365095.07 val_loss= 5.63 time= 0.68\n","Epoch: 091 train_loss= 1465381.70 val_loss= 5.63 time= 0.76\n","Epoch: 101 train_loss= 1457109.88 val_loss= 5.63 time= 0.84\n","test error=102.22,0.16, mean test error=159.74,0.12\n","testing on day #101\n","Epoch: 001 train_loss= 1359049.62 val_loss= 7.51 time= 0.01\n","Epoch: 011 train_loss= 1745025.76 val_loss= 5.82 time= 0.09\n","Epoch: 021 train_loss= 1408601.34 val_loss= 6.60 time= 0.18\n","Epoch: 031 train_loss= 1373714.41 val_loss= 6.49 time= 0.26\n","Epoch: 041 train_loss= 1247438.38 val_loss= 6.49 time= 0.35\n","Epoch: 051 train_loss= 1448147.75 val_loss= 6.50 time= 0.43\n","Epoch: 061 train_loss= 1340478.71 val_loss= 6.50 time= 0.52\n","Epoch: 071 train_loss= 1325722.48 val_loss= 6.50 time= 0.60\n","Epoch: 081 train_loss= 1289706.79 val_loss= 6.50 time= 0.68\n","Epoch: 091 train_loss= 1377174.28 val_loss= 6.50 time= 0.77\n","Epoch: 101 train_loss= 1356069.27 val_loss= 6.50 time= 0.85\n","test error=117.13,0.15, mean test error=159.25,0.12\n","testing on day #102\n","Epoch: 001 train_loss= 1614204.38 val_loss= 12.83 time= 0.01\n","Epoch: 011 train_loss= 1542370.57 val_loss= 7.23 time= 0.09\n","Epoch: 021 train_loss= 1486699.34 val_loss= 6.45 time= 0.18\n","Epoch: 031 train_loss= 1352835.18 val_loss= 5.97 time= 0.27\n","Epoch: 041 train_loss= 1409543.03 val_loss= 5.96 time= 0.35\n","Epoch: 051 train_loss= 1404609.60 val_loss= 5.68 time= 0.44\n","Epoch: 061 train_loss= 1441632.61 val_loss= 5.81 time= 0.52\n","Epoch: 071 train_loss= 1352219.59 val_loss= 5.78 time= 0.61\n","Epoch: 081 train_loss= 1477968.07 val_loss= 5.79 time= 0.69\n","Epoch: 091 train_loss= 1451664.63 val_loss= 5.79 time= 0.78\n","Epoch: 101 train_loss= 1409709.04 val_loss= 5.79 time= 0.86\n","test error=125.76,0.16, mean test error=158.86,0.12\n","testing on day #103\n","Epoch: 001 train_loss= 1398445.58 val_loss= 6.54 time= 0.01\n","Epoch: 011 train_loss= 1846270.96 val_loss= 6.06 time= 0.10\n","Epoch: 021 train_loss= 1635893.54 val_loss= 5.85 time= 0.18\n","Epoch: 031 train_loss= 1412825.61 val_loss= 6.24 time= 0.27\n","Epoch: 041 train_loss= 1362521.14 val_loss= 6.27 time= 0.35\n","Epoch: 051 train_loss= 1282682.16 val_loss= 6.16 time= 0.44\n","Epoch: 061 train_loss= 1298888.83 val_loss= 6.15 time= 0.53\n","Epoch: 071 train_loss= 1357261.78 val_loss= 6.15 time= 0.61\n","Epoch: 081 train_loss= 1256383.88 val_loss= 6.15 time= 0.70\n","Epoch: 091 train_loss= 1350186.47 val_loss= 6.15 time= 0.78\n","Epoch: 101 train_loss= 1292479.96 val_loss= 6.15 time= 0.87\n","test error=100.64,0.13, mean test error=158.21,0.12\n","testing on day #104\n","Epoch: 001 train_loss= 1741071.66 val_loss= 14.43 time= 0.01\n","Epoch: 011 train_loss= 1425812.25 val_loss= 8.04 time= 0.10\n","Epoch: 021 train_loss= 1402712.32 val_loss= 6.03 time= 0.18\n","Epoch: 031 train_loss= 1450861.04 val_loss= 5.94 time= 0.27\n","Epoch: 041 train_loss= 1338963.05 val_loss= 6.00 time= 0.36\n","Epoch: 051 train_loss= 1311523.58 val_loss= 6.00 time= 0.44\n","Epoch: 061 train_loss= 1306037.95 val_loss= 6.00 time= 0.53\n","Epoch: 071 train_loss= 1346302.22 val_loss= 6.00 time= 0.62\n","Epoch: 081 train_loss= 1480683.63 val_loss= 6.00 time= 0.70\n","Epoch: 091 train_loss= 1409665.25 val_loss= 6.00 time= 0.79\n","Epoch: 101 train_loss= 1463000.90 val_loss= 6.00 time= 0.88\n","test error=94.46,0.14, mean test error=157.50,0.12\n","testing on day #105\n","Epoch: 001 train_loss= 1446309.61 val_loss= 9.57 time= 0.01\n","Epoch: 011 train_loss= 1453220.53 val_loss= 6.09 time= 0.10\n","Epoch: 021 train_loss= 1307445.42 val_loss= 5.72 time= 0.19\n","Epoch: 031 train_loss= 1301072.20 val_loss= 5.56 time= 0.27\n","Epoch: 041 train_loss= 1285256.66 val_loss= 5.53 time= 0.36\n","Epoch: 051 train_loss= 1204191.57 val_loss= 5.52 time= 0.45\n","Epoch: 061 train_loss= 1311971.03 val_loss= 5.53 time= 0.54\n","Epoch: 071 train_loss= 1277566.12 val_loss= 5.52 time= 0.62\n","Epoch: 081 train_loss= 1253254.22 val_loss= 5.52 time= 0.71\n","Epoch: 091 train_loss= 1310927.83 val_loss= 5.53 time= 0.80\n","Epoch: 101 train_loss= 1254120.34 val_loss= 5.52 time= 0.89\n","test error=104.01,0.12, mean test error=156.91,0.12\n","testing on day #106\n","Epoch: 001 train_loss= 1582014.58 val_loss= 10.78 time= 0.01\n","Epoch: 011 train_loss= 1640278.92 val_loss= 5.19 time= 0.10\n","Epoch: 021 train_loss= 1356422.71 val_loss= 5.52 time= 0.19\n","Epoch: 031 train_loss= 1377655.35 val_loss= 5.25 time= 0.28\n","Epoch: 041 train_loss= 1451306.38 val_loss= 5.24 time= 0.36\n","Epoch: 051 train_loss= 1384019.03 val_loss= 5.23 time= 0.45\n","Epoch: 061 train_loss= 1362207.03 val_loss= 5.23 time= 0.54\n","Epoch: 071 train_loss= 1406040.17 val_loss= 5.23 time= 0.63\n","Epoch: 081 train_loss= 1400994.04 val_loss= 5.23 time= 0.72\n","Epoch: 091 train_loss= 1515734.13 val_loss= 5.23 time= 0.81\n","Epoch: 101 train_loss= 1406268.66 val_loss= 5.23 time= 0.90\n","test error=102.54,0.15, mean test error=156.32,0.12\n","testing on day #107\n","Epoch: 001 train_loss= 1489486.97 val_loss= 11.93 time= 0.01\n","Epoch: 011 train_loss= 1355550.48 val_loss= 6.96 time= 0.10\n","Epoch: 021 train_loss= 1296817.64 val_loss= 5.12 time= 0.19\n","Epoch: 031 train_loss= 1356486.74 val_loss= 5.09 time= 0.28\n","Epoch: 041 train_loss= 1337915.51 val_loss= 5.09 time= 0.37\n","Epoch: 051 train_loss= 1286878.75 val_loss= 5.08 time= 0.46\n","Epoch: 061 train_loss= 1369131.51 val_loss= 5.08 time= 0.55\n","Epoch: 071 train_loss= 1297120.90 val_loss= 5.09 time= 0.64\n","Epoch: 081 train_loss= 1289869.51 val_loss= 5.08 time= 0.72\n","Epoch: 091 train_loss= 1231767.54 val_loss= 5.09 time= 0.81\n","Epoch: 101 train_loss= 1310452.64 val_loss= 5.08 time= 0.90\n","test error=112.43,0.13, mean test error=155.85,0.12\n","testing on day #108\n","Epoch: 001 train_loss= 1414737.05 val_loss= 4.80 time= 0.01\n","Epoch: 011 train_loss= 1440163.38 val_loss= 5.97 time= 0.10\n","Epoch: 021 train_loss= 1313220.99 val_loss= 5.68 time= 0.19\n","Epoch: 031 train_loss= 1273011.98 val_loss= 5.63 time= 0.28\n","Epoch: 041 train_loss= 1352444.76 val_loss= 5.58 time= 0.36\n","Epoch: 051 train_loss= 1360394.40 val_loss= 5.59 time= 0.45\n","Epoch: 061 train_loss= 1401877.63 val_loss= 5.59 time= 0.54\n","Epoch: 071 train_loss= 1341361.72 val_loss= 5.59 time= 0.63\n","Epoch: 081 train_loss= 1364469.43 val_loss= 5.59 time= 0.72\n","Epoch: 091 train_loss= 1317243.82 val_loss= 5.59 time= 0.81\n","Epoch: 101 train_loss= 1500277.60 val_loss= 5.59 time= 0.90\n","test error=129.16,0.14, mean test error=155.57,0.12\n","testing on day #109\n","Epoch: 001 train_loss= 1450707.56 val_loss= 11.06 time= 0.01\n","Epoch: 011 train_loss= 1272217.41 val_loss= 6.32 time= 0.10\n","Epoch: 021 train_loss= 1362113.81 val_loss= 6.01 time= 0.19\n","Epoch: 031 train_loss= 1331908.57 val_loss= 6.26 time= 0.28\n","Epoch: 041 train_loss= 1267767.40 val_loss= 5.95 time= 0.37\n","Epoch: 051 train_loss= 1155081.74 val_loss= 5.99 time= 0.45\n","Epoch: 061 train_loss= 1323423.38 val_loss= 6.00 time= 0.54\n","Epoch: 071 train_loss= 1221946.82 val_loss= 6.00 time= 0.63\n","Epoch: 081 train_loss= 1347985.86 val_loss= 6.00 time= 0.72\n","Epoch: 091 train_loss= 1261989.87 val_loss= 6.00 time= 0.81\n","Epoch: 101 train_loss= 1269839.28 val_loss= 6.00 time= 0.90\n","test error=142.88,0.14, mean test error=155.43,0.12\n","testing on day #110\n","Epoch: 001 train_loss= 1539618.37 val_loss= 6.62 time= 0.01\n","Epoch: 011 train_loss= 1407718.40 val_loss= 8.13 time= 0.10\n","Epoch: 021 train_loss= 1311682.48 val_loss= 8.30 time= 0.19\n","Epoch: 031 train_loss= 1384639.50 val_loss= 8.68 time= 0.28\n","Epoch: 041 train_loss= 1382735.76 val_loss= 8.68 time= 0.36\n","Epoch: 051 train_loss= 1400643.90 val_loss= 8.70 time= 0.45\n","Epoch: 061 train_loss= 1396401.48 val_loss= 8.70 time= 0.54\n","Epoch: 071 train_loss= 1467765.41 val_loss= 8.70 time= 0.63\n","Epoch: 081 train_loss= 1376478.16 val_loss= 8.70 time= 0.72\n","Epoch: 091 train_loss= 1384447.31 val_loss= 8.70 time= 0.81\n","Epoch: 101 train_loss= 1346208.52 val_loss= 8.70 time= 0.89\n","test error=118.19,0.14, mean test error=155.05,0.12\n","testing on day #111\n","Epoch: 001 train_loss= 1344231.37 val_loss= 8.17 time= 0.01\n","Epoch: 011 train_loss= 1225527.86 val_loss= 8.79 time= 0.10\n","Epoch: 021 train_loss= 1249606.23 val_loss= 8.73 time= 0.19\n","Epoch: 031 train_loss= 1310258.63 val_loss= 8.95 time= 0.28\n","Epoch: 041 train_loss= 1270830.51 val_loss= 8.88 time= 0.36\n","Epoch: 051 train_loss= 1233822.24 val_loss= 8.87 time= 0.45\n","Epoch: 061 train_loss= 1232407.84 val_loss= 8.87 time= 0.54\n","Epoch: 071 train_loss= 1331609.08 val_loss= 8.87 time= 0.63\n","Epoch: 081 train_loss= 1180107.15 val_loss= 8.87 time= 0.72\n","Epoch: 091 train_loss= 1230141.64 val_loss= 8.87 time= 0.81\n","Epoch: 101 train_loss= 1360276.52 val_loss= 8.87 time= 0.89\n","test error=108.28,0.12, mean test error=154.56,0.12\n","testing on day #112\n","Epoch: 001 train_loss= 1414806.76 val_loss= 10.89 time= 0.01\n","Epoch: 011 train_loss= 1388692.62 val_loss= 10.75 time= 0.10\n","Epoch: 021 train_loss= 1321829.64 val_loss= 10.08 time= 0.19\n","Epoch: 031 train_loss= 1429237.65 val_loss= 10.39 time= 0.27\n","Epoch: 041 train_loss= 1409305.03 val_loss= 10.51 time= 0.36\n","Epoch: 051 train_loss= 1351051.07 val_loss= 10.47 time= 0.45\n","Epoch: 061 train_loss= 1433186.07 val_loss= 10.47 time= 0.54\n","Epoch: 071 train_loss= 1351854.92 val_loss= 10.47 time= 0.63\n","Epoch: 081 train_loss= 1368939.62 val_loss= 10.47 time= 0.72\n","Epoch: 091 train_loss= 1245574.41 val_loss= 10.47 time= 0.81\n","Epoch: 101 train_loss= 1511882.49 val_loss= 10.47 time= 0.89\n","test error=104.37,0.12, mean test error=154.05,0.12\n","testing on day #113\n","Epoch: 001 train_loss= 1237492.64 val_loss= 11.34 time= 0.01\n","Epoch: 011 train_loss= 1302540.62 val_loss= 8.49 time= 0.10\n","Epoch: 021 train_loss= 1202909.96 val_loss= 8.68 time= 0.19\n","Epoch: 031 train_loss= 1251812.71 val_loss= 10.32 time= 0.27\n","Epoch: 041 train_loss= 1216205.59 val_loss= 10.06 time= 0.36\n","Epoch: 051 train_loss= 1328933.21 val_loss= 9.91 time= 0.45\n","Epoch: 061 train_loss= 1249994.24 val_loss= 9.90 time= 0.54\n","Epoch: 071 train_loss= 1205986.17 val_loss= 9.89 time= 0.63\n","Epoch: 081 train_loss= 1347411.44 val_loss= 9.90 time= 0.72\n","Epoch: 091 train_loss= 1238151.05 val_loss= 9.90 time= 0.81\n","Epoch: 101 train_loss= 1192038.30 val_loss= 9.89 time= 0.90\n","test error=87.76,0.10, mean test error=153.38,0.12\n","testing on day #114\n","Epoch: 001 train_loss= 1537307.49 val_loss= 16.70 time= 0.01\n","Epoch: 011 train_loss= 1328814.64 val_loss= 14.03 time= 0.10\n","Epoch: 021 train_loss= 1493018.46 val_loss= 10.88 time= 0.19\n","Epoch: 031 train_loss= 1304219.52 val_loss= 15.08 time= 0.28\n","Epoch: 041 train_loss= 1333489.82 val_loss= 11.03 time= 0.37\n","Epoch: 051 train_loss= 1256061.88 val_loss= 11.82 time= 0.45\n","Epoch: 061 train_loss= 1281735.90 val_loss= 11.55 time= 0.54\n","Epoch: 071 train_loss= 1357450.08 val_loss= 11.51 time= 0.63\n","Epoch: 081 train_loss= 1298136.91 val_loss= 11.51 time= 0.72\n","Epoch: 091 train_loss= 1233047.52 val_loss= 11.51 time= 0.81\n","Epoch: 101 train_loss= 1309238.00 val_loss= 11.51 time= 0.90\n","test error=100.88,0.12, mean test error=152.86,0.12\n","testing on day #115\n","Epoch: 001 train_loss= 1258255.01 val_loss= 10.77 time= 0.01\n","Epoch: 011 train_loss= 1231028.43 val_loss= 11.96 time= 0.10\n","Epoch: 021 train_loss= 1250400.86 val_loss= 9.80 time= 0.19\n","Epoch: 031 train_loss= 1215473.79 val_loss= 9.79 time= 0.28\n","Epoch: 041 train_loss= 1259486.98 val_loss= 9.79 time= 0.36\n","Epoch: 051 train_loss= 1186376.63 val_loss= 9.76 time= 0.45\n","Epoch: 061 train_loss= 1174932.05 val_loss= 9.76 time= 0.54\n","Epoch: 071 train_loss= 1216891.32 val_loss= 9.76 time= 0.63\n","Epoch: 081 train_loss= 1204543.22 val_loss= 9.76 time= 0.72\n","Epoch: 091 train_loss= 1132173.41 val_loss= 9.76 time= 0.81\n","Epoch: 101 train_loss= 1185798.49 val_loss= 9.76 time= 0.90\n","test error=150.26,0.17, mean test error=152.83,0.13\n","testing on day #116\n","Epoch: 001 train_loss= 1384316.86 val_loss= 10.63 time= 0.01\n","Epoch: 011 train_loss= 1245655.93 val_loss= 10.95 time= 0.10\n","Epoch: 021 train_loss= 1282568.75 val_loss= 11.88 time= 0.19\n","Epoch: 031 train_loss= 1366947.19 val_loss= 11.60 time= 0.28\n","Epoch: 041 train_loss= 1255858.35 val_loss= 10.74 time= 0.37\n","Epoch: 051 train_loss= 1262918.91 val_loss= 10.90 time= 0.45\n","Epoch: 061 train_loss= 1192372.68 val_loss= 10.92 time= 0.54\n","Epoch: 071 train_loss= 1232701.28 val_loss= 10.93 time= 0.63\n","Epoch: 081 train_loss= 1151637.83 val_loss= 10.93 time= 0.72\n","Epoch: 091 train_loss= 1242722.33 val_loss= 10.93 time= 0.81\n","Epoch: 101 train_loss= 1152485.80 val_loss= 10.93 time= 0.90\n","test error=266.81,0.25, mean test error=153.95,0.13\n","testing on day #117\n","Epoch: 001 train_loss= 1252339.47 val_loss= 12.78 time= 0.01\n","Epoch: 011 train_loss= 1248466.12 val_loss= 10.89 time= 0.10\n","Epoch: 021 train_loss= 1265862.61 val_loss= 10.82 time= 0.19\n","Epoch: 031 train_loss= 1318300.13 val_loss= 10.81 time= 0.28\n","Epoch: 041 train_loss= 1291136.21 val_loss= 10.78 time= 0.36\n","Epoch: 051 train_loss= 1285718.20 val_loss= 10.76 time= 0.45\n","Epoch: 061 train_loss= 1213599.15 val_loss= 10.77 time= 0.54\n","Epoch: 071 train_loss= 1246968.09 val_loss= 10.77 time= 0.63\n","Epoch: 081 train_loss= 1314359.31 val_loss= 10.77 time= 0.72\n","Epoch: 091 train_loss= 1296228.68 val_loss= 10.77 time= 0.81\n","Epoch: 101 train_loss= 1304993.17 val_loss= 10.77 time= 0.90\n","test error=243.11,0.24, mean test error=154.81,0.13\n","testing on day #118\n","Epoch: 001 train_loss= 1400561.75 val_loss= 15.65 time= 0.01\n","Epoch: 011 train_loss= 1373693.94 val_loss= 15.85 time= 0.10\n","Epoch: 021 train_loss= 1374728.48 val_loss= 15.16 time= 0.19\n","Epoch: 031 train_loss= 1362771.74 val_loss= 14.88 time= 0.28\n","Epoch: 041 train_loss= 1378286.28 val_loss= 14.83 time= 0.36\n","Epoch: 051 train_loss= 1357729.39 val_loss= 14.73 time= 0.45\n","Epoch: 061 train_loss= 1268180.23 val_loss= 14.71 time= 0.54\n","Epoch: 071 train_loss= 1211081.05 val_loss= 14.70 time= 0.63\n","Epoch: 081 train_loss= 1316374.19 val_loss= 14.70 time= 0.72\n","Epoch: 091 train_loss= 1295267.33 val_loss= 14.70 time= 0.81\n","Epoch: 101 train_loss= 1406645.75 val_loss= 14.70 time= 0.90\n","test error=236.30,0.26, mean test error=155.60,0.13\n","Total time cost: 89.62 minutes.\n","testing on day #15\n","Epoch: 001 train_loss= 26040714.00 val_loss= 444.21 time= 0.00\n","Epoch: 011 train_loss= 11836288.00 val_loss= 201.56 time= 0.01\n","Epoch: 021 train_loss= 3329832.50 val_loss= 54.94 time= 0.02\n","Epoch: 031 train_loss= 3184327.00 val_loss= 38.42 time= 0.03\n","Epoch: 041 train_loss= 3897914.00 val_loss= 37.94 time= 0.04\n","Epoch: 051 train_loss= 3339105.00 val_loss= 36.90 time= 0.04\n","Epoch: 061 train_loss= 4199169.50 val_loss= 36.76 time= 0.05\n","Epoch: 071 train_loss= 3617729.25 val_loss= 36.75 time= 0.06\n","Epoch: 081 train_loss= 3791797.50 val_loss= 36.75 time= 0.06\n","Epoch: 091 train_loss= 3013031.25 val_loss= 36.74 time= 0.07\n","Epoch: 101 train_loss= 3173923.75 val_loss= 36.74 time= 0.08\n","test error=310.86,0.15, mean test error=310.86,0.15\n","testing on day #16\n","Epoch: 001 train_loss= 29760868.00 val_loss= 534.45 time= 0.00\n","Epoch: 011 train_loss= 17494996.00 val_loss= 281.89 time= 0.01\n","Epoch: 021 train_loss= 7240100.00 val_loss= 91.61 time= 0.02\n","Epoch: 031 train_loss= 4432762.50 val_loss= 37.38 time= 0.03\n","Epoch: 041 train_loss= 2855878.00 val_loss= 39.66 time= 0.04\n","Epoch: 051 train_loss= 4075937.25 val_loss= 38.45 time= 0.04\n","Epoch: 061 train_loss= 2778341.25 val_loss= 38.20 time= 0.05\n","Epoch: 071 train_loss= 3365890.50 val_loss= 38.16 time= 0.06\n","Epoch: 081 train_loss= 3728073.75 val_loss= 38.16 time= 0.06\n","Epoch: 091 train_loss= 2838693.75 val_loss= 38.16 time= 0.07\n","Epoch: 101 train_loss= 4016749.00 val_loss= 38.16 time= 0.08\n","test error=315.79,0.15, mean test error=313.33,0.15\n","testing on day #17\n","Epoch: 001 train_loss= 3944914.00 val_loss= 38.27 time= 0.00\n","Epoch: 011 train_loss= 3696216.25 val_loss= 38.44 time= 0.01\n","Epoch: 021 train_loss= 3513739.00 val_loss= 36.86 time= 0.02\n","Epoch: 031 train_loss= 2700946.75 val_loss= 37.79 time= 0.03\n","Epoch: 041 train_loss= 3187294.00 val_loss= 36.73 time= 0.03\n","Epoch: 051 train_loss= 2796880.75 val_loss= 36.39 time= 0.04\n","Epoch: 061 train_loss= 2817198.75 val_loss= 36.12 time= 0.05\n","Epoch: 071 train_loss= 2519867.25 val_loss= 36.12 time= 0.06\n","Epoch: 081 train_loss= 2646745.00 val_loss= 35.84 time= 0.07\n","Epoch: 091 train_loss= 3030712.00 val_loss= 35.99 time= 0.08\n","Epoch: 101 train_loss= 3602553.75 val_loss= 35.98 time= 0.09\n","test error=299.65,0.14, mean test error=308.77,0.15\n","testing on day #18\n","Epoch: 001 train_loss= 4203281.00 val_loss= 40.58 time= 0.00\n","Epoch: 011 train_loss= 3113469.00 val_loss= 40.36 time= 0.01\n","Epoch: 021 train_loss= 3338098.75 val_loss= 39.63 time= 0.02\n","Epoch: 031 train_loss= 2574715.25 val_loss= 39.44 time= 0.03\n","Epoch: 041 train_loss= 2759935.75 val_loss= 38.38 time= 0.04\n","Epoch: 051 train_loss= 2946952.25 val_loss= 37.02 time= 0.05\n","Epoch: 061 train_loss= 3248717.75 val_loss= 36.53 time= 0.06\n","Epoch: 071 train_loss= 2783186.75 val_loss= 36.43 time= 0.07\n","Epoch: 081 train_loss= 2213645.25 val_loss= 34.45 time= 0.08\n","Epoch: 091 train_loss= 2961748.75 val_loss= 33.34 time= 0.09\n","Epoch: 101 train_loss= 2534423.25 val_loss= 36.27 time= 0.10\n","Epoch: 111 train_loss= 2653386.00 val_loss= 31.75 time= 0.11\n","Epoch: 121 train_loss= 2300057.00 val_loss= 31.41 time= 0.12\n","Epoch: 131 train_loss= 2946389.75 val_loss= 30.81 time= 0.13\n","test error=299.79,0.15, mean test error=306.53,0.15\n","testing on day #19\n","Epoch: 001 train_loss= 2430300.50 val_loss= 40.21 time= 0.00\n","Epoch: 011 train_loss= 3282522.25 val_loss= 38.47 time= 0.01\n","Epoch: 021 train_loss= 3248828.25 val_loss= 36.92 time= 0.02\n","Epoch: 031 train_loss= 2860987.25 val_loss= 35.82 time= 0.04\n","Epoch: 041 train_loss= 3198672.00 val_loss= 35.57 time= 0.05\n","Epoch: 051 train_loss= 2561420.00 val_loss= 33.60 time= 0.06\n","Epoch: 061 train_loss= 2557238.25 val_loss= 33.28 time= 0.07\n","Epoch: 071 train_loss= 2773527.75 val_loss= 31.78 time= 0.08\n","Epoch: 081 train_loss= 2521687.00 val_loss= 30.73 time= 0.09\n","Epoch: 091 train_loss= 2416996.50 val_loss= 30.76 time= 0.10\n","Epoch: 101 train_loss= 2848380.75 val_loss= 29.82 time= 0.11\n","Epoch: 111 train_loss= 2283858.75 val_loss= 29.31 time= 0.12\n","Epoch: 121 train_loss= 2276571.75 val_loss= 27.31 time= 0.13\n","Epoch: 131 train_loss= 2889009.25 val_loss= 26.99 time= 0.15\n","Epoch: 141 train_loss= 2408308.75 val_loss= 27.36 time= 0.15\n","Epoch: 151 train_loss= 2236868.25 val_loss= 26.48 time= 0.16\n","Epoch: 161 train_loss= 2145538.25 val_loss= 26.16 time= 0.18\n","Epoch: 171 train_loss= 2458884.75 val_loss= 26.13 time= 0.19\n","Epoch: 181 train_loss= 2216926.25 val_loss= 26.33 time= 0.20\n","Epoch: 191 train_loss= 2301352.25 val_loss= 26.23 time= 0.21\n","test error=288.18,0.16, mean test error=302.86,0.15\n","testing on day #20\n","Epoch: 001 train_loss= 2761402.67 val_loss= 32.91 time= 0.00\n","Epoch: 011 train_loss= 2665657.58 val_loss= 35.05 time= 0.01\n","Epoch: 021 train_loss= 2916125.58 val_loss= 35.89 time= 0.02\n","Epoch: 031 train_loss= 2783368.19 val_loss= 35.44 time= 0.03\n","Epoch: 041 train_loss= 2918261.89 val_loss= 35.36 time= 0.04\n","Epoch: 051 train_loss= 2857982.75 val_loss= 35.35 time= 0.05\n","Epoch: 061 train_loss= 3009571.39 val_loss= 35.36 time= 0.07\n","Epoch: 071 train_loss= 2902506.31 val_loss= 35.36 time= 0.08\n","Epoch: 081 train_loss= 3376287.78 val_loss= 35.36 time= 0.09\n","Epoch: 091 train_loss= 2786474.25 val_loss= 35.35 time= 0.10\n","Epoch: 101 train_loss= 2703668.07 val_loss= 35.35 time= 0.11\n","test error=301.85,0.15, mean test error=302.69,0.15\n","testing on day #21\n","Epoch: 001 train_loss= 2329136.35 val_loss= 33.21 time= 0.00\n","Epoch: 011 train_loss= 2234212.80 val_loss= 30.58 time= 0.01\n","Epoch: 021 train_loss= 2414988.10 val_loss= 31.39 time= 0.03\n","Epoch: 031 train_loss= 2423007.25 val_loss= 31.08 time= 0.04\n","Epoch: 041 train_loss= 2772374.85 val_loss= 31.07 time= 0.05\n","Epoch: 051 train_loss= 2321249.45 val_loss= 31.06 time= 0.06\n","Epoch: 061 train_loss= 2563207.70 val_loss= 31.06 time= 0.07\n","Epoch: 071 train_loss= 2389161.85 val_loss= 31.06 time= 0.08\n","Epoch: 081 train_loss= 2407199.10 val_loss= 31.06 time= 0.09\n","Epoch: 091 train_loss= 2234041.95 val_loss= 31.06 time= 0.11\n","Epoch: 101 train_loss= 2382449.80 val_loss= 31.06 time= 0.12\n","test error=275.15,0.16, mean test error=298.75,0.15\n","testing on day #22\n","Epoch: 001 train_loss= 2863952.32 val_loss= 37.32 time= 0.00\n","Epoch: 011 train_loss= 2990062.64 val_loss= 34.63 time= 0.01\n","Epoch: 021 train_loss= 2787882.57 val_loss= 34.18 time= 0.03\n","Epoch: 031 train_loss= 2830746.11 val_loss= 33.62 time= 0.04\n","Epoch: 041 train_loss= 2783456.43 val_loss= 34.69 time= 0.05\n","Epoch: 051 train_loss= 2809397.27 val_loss= 33.52 time= 0.07\n","Epoch: 061 train_loss= 3535259.55 val_loss= 33.05 time= 0.08\n","Epoch: 071 train_loss= 2675109.20 val_loss= 33.03 time= 0.09\n","Epoch: 081 train_loss= 2473456.93 val_loss= 33.17 time= 0.11\n","Epoch: 091 train_loss= 2881708.75 val_loss= 33.33 time= 0.12\n","Epoch: 101 train_loss= 2832904.64 val_loss= 33.30 time= 0.13\n","Epoch: 111 train_loss= 2392855.50 val_loss= 33.30 time= 0.14\n","Epoch: 121 train_loss= 2497347.86 val_loss= 33.30 time= 0.16\n","test error=306.59,0.15, mean test error=299.73,0.15\n","testing on day #23\n","Epoch: 001 train_loss= 3366035.83 val_loss= 32.59 time= 0.00\n","Epoch: 011 train_loss= 2813598.58 val_loss= 31.79 time= 0.01\n","Epoch: 021 train_loss= 2494375.25 val_loss= 30.76 time= 0.03\n","Epoch: 031 train_loss= 2792232.33 val_loss= 31.24 time= 0.04\n","Epoch: 041 train_loss= 2639083.50 val_loss= 30.66 time= 0.06\n","Epoch: 051 train_loss= 2586073.67 val_loss= 31.09 time= 0.07\n","Epoch: 061 train_loss= 2251437.92 val_loss= 30.11 time= 0.09\n","Epoch: 071 train_loss= 2509062.00 val_loss= 30.24 time= 0.10\n","Epoch: 081 train_loss= 2163732.17 val_loss= 30.14 time= 0.11\n","Epoch: 091 train_loss= 2415966.75 val_loss= 30.13 time= 0.12\n","Epoch: 101 train_loss= 2767520.83 val_loss= 30.13 time= 0.14\n","test error=284.97,0.16, mean test error=298.09,0.15\n","testing on day #24\n","Epoch: 001 train_loss= 3126232.42 val_loss= 37.88 time= 0.00\n","Epoch: 011 train_loss= 3218362.56 val_loss= 35.07 time= 0.02\n","Epoch: 021 train_loss= 2626077.15 val_loss= 34.37 time= 0.03\n","Epoch: 031 train_loss= 2797865.17 val_loss= 33.75 time= 0.05\n","Epoch: 041 train_loss= 2651961.58 val_loss= 33.62 time= 0.06\n","Epoch: 051 train_loss= 2314868.15 val_loss= 32.72 time= 0.08\n","Epoch: 061 train_loss= 2461327.50 val_loss= 32.23 time= 0.10\n","Epoch: 071 train_loss= 2487153.44 val_loss= 32.12 time= 0.11\n","Epoch: 081 train_loss= 2593121.35 val_loss= 31.53 time= 0.13\n","Epoch: 091 train_loss= 2463458.38 val_loss= 31.23 time= 0.14\n","Epoch: 101 train_loss= 2288438.88 val_loss= 31.15 time= 0.16\n","test error=305.40,0.20, mean test error=298.82,0.16\n","testing on day #25\n","Epoch: 001 train_loss= 3081508.57 val_loss= 35.32 time= 0.00\n","Epoch: 011 train_loss= 2532220.57 val_loss= 33.32 time= 0.02\n","Epoch: 021 train_loss= 2352171.18 val_loss= 32.97 time= 0.03\n","Epoch: 031 train_loss= 2422407.64 val_loss= 33.07 time= 0.05\n","Epoch: 041 train_loss= 2017829.14 val_loss= 33.13 time= 0.06\n","Epoch: 051 train_loss= 2271802.57 val_loss= 33.14 time= 0.08\n","Epoch: 061 train_loss= 2396433.68 val_loss= 33.13 time= 0.09\n","Epoch: 071 train_loss= 2579967.14 val_loss= 33.13 time= 0.11\n","Epoch: 081 train_loss= 2278847.25 val_loss= 33.14 time= 0.12\n","Epoch: 091 train_loss= 2268040.14 val_loss= 33.14 time= 0.14\n","Epoch: 101 train_loss= 2155042.11 val_loss= 33.14 time= 0.15\n","test error=269.17,0.17, mean test error=296.13,0.16\n","testing on day #26\n","Epoch: 001 train_loss= 2867852.42 val_loss= 37.18 time= 0.00\n","Epoch: 011 train_loss= 2725348.45 val_loss= 34.47 time= 0.02\n","Epoch: 021 train_loss= 2683161.38 val_loss= 35.08 time= 0.04\n","Epoch: 031 train_loss= 2409375.82 val_loss= 34.90 time= 0.05\n","Epoch: 041 train_loss= 2717535.98 val_loss= 34.06 time= 0.07\n","Epoch: 051 train_loss= 2385880.75 val_loss= 33.94 time= 0.08\n","Epoch: 061 train_loss= 2415594.23 val_loss= 33.82 time= 0.10\n","Epoch: 071 train_loss= 2046145.43 val_loss= 33.57 time= 0.12\n","Epoch: 081 train_loss= 2543480.22 val_loss= 34.14 time= 0.13\n","Epoch: 091 train_loss= 2566631.72 val_loss= 34.06 time= 0.15\n","Epoch: 101 train_loss= 2160569.30 val_loss= 33.98 time= 0.16\n","test error=284.07,0.20, mean test error=295.12,0.16\n","testing on day #27\n","Epoch: 001 train_loss= 2814301.75 val_loss= 37.12 time= 0.00\n","Epoch: 011 train_loss= 2591032.38 val_loss= 32.31 time= 0.02\n","Epoch: 021 train_loss= 2857505.25 val_loss= 31.97 time= 0.04\n","test error=276.72,0.15, mean test error=293.71,0.16\n","testing on day #28\n","Epoch: 001 train_loss= 3151885.34 val_loss= 33.95 time= 0.00\n","Epoch: 011 train_loss= 2523584.26 val_loss= 35.03 time= 0.02\n","Epoch: 021 train_loss= 2489414.75 val_loss= 34.17 time= 0.04\n","Epoch: 031 train_loss= 2329393.18 val_loss= 34.06 time= 0.06\n","Epoch: 041 train_loss= 2079104.30 val_loss= 34.04 time= 0.08\n","Epoch: 051 train_loss= 2338347.38 val_loss= 34.04 time= 0.09\n","Epoch: 061 train_loss= 2415793.54 val_loss= 34.04 time= 0.11\n","Epoch: 071 train_loss= 2488797.03 val_loss= 34.04 time= 0.13\n","Epoch: 081 train_loss= 2626626.38 val_loss= 34.04 time= 0.15\n","Epoch: 091 train_loss= 2727098.78 val_loss= 34.04 time= 0.17\n","Epoch: 101 train_loss= 2318805.19 val_loss= 34.04 time= 0.18\n","test error=288.84,0.19, mean test error=293.36,0.16\n","testing on day #29\n","Epoch: 001 train_loss= 2725492.67 val_loss= 34.06 time= 0.00\n","Epoch: 011 train_loss= 2564070.72 val_loss= 32.88 time= 0.02\n","Epoch: 021 train_loss= 2560178.50 val_loss= 35.29 time= 0.04\n","Epoch: 031 train_loss= 2178561.49 val_loss= 32.64 time= 0.06\n","Epoch: 041 train_loss= 2527937.92 val_loss= 32.79 time= 0.08\n","Epoch: 051 train_loss= 2802730.42 val_loss= 32.79 time= 0.10\n","Epoch: 061 train_loss= 2510452.97 val_loss= 32.78 time= 0.12\n","Epoch: 071 train_loss= 2485932.36 val_loss= 32.78 time= 0.13\n","Epoch: 081 train_loss= 2970235.97 val_loss= 32.78 time= 0.15\n","Epoch: 091 train_loss= 2622766.97 val_loss= 32.78 time= 0.17\n","Epoch: 101 train_loss= 2433323.64 val_loss= 32.78 time= 0.19\n","test error=405.53,0.21, mean test error=300.84,0.17\n","testing on day #30\n","Epoch: 001 train_loss= 2871310.16 val_loss= 34.33 time= 0.00\n","Epoch: 011 train_loss= 2671642.08 val_loss= 38.40 time= 0.02\n","Epoch: 021 train_loss= 2624731.45 val_loss= 35.24 time= 0.04\n","Epoch: 031 train_loss= 3030276.37 val_loss= 35.33 time= 0.06\n","Epoch: 041 train_loss= 3121693.29 val_loss= 35.30 time= 0.08\n","Epoch: 051 train_loss= 2774013.74 val_loss= 35.30 time= 0.10\n","Epoch: 061 train_loss= 3027305.76 val_loss= 35.30 time= 0.12\n","Epoch: 071 train_loss= 3364753.03 val_loss= 35.30 time= 0.14\n","Epoch: 081 train_loss= 2719732.03 val_loss= 35.30 time= 0.16\n","Epoch: 091 train_loss= 2764994.89 val_loss= 35.30 time= 0.18\n","Epoch: 101 train_loss= 2555090.47 val_loss= 35.30 time= 0.20\n","test error=286.05,0.18, mean test error=299.91,0.17\n","testing on day #31\n","Epoch: 001 train_loss= 3444323.30 val_loss= 56.68 time= 0.00\n","Epoch: 011 train_loss= 2859645.60 val_loss= 49.43 time= 0.02\n","Epoch: 021 train_loss= 2707556.55 val_loss= 49.95 time= 0.04\n","Epoch: 031 train_loss= 2830970.35 val_loss= 49.15 time= 0.07\n","Epoch: 041 train_loss= 2653047.50 val_loss= 48.55 time= 0.09\n","Epoch: 051 train_loss= 3016492.20 val_loss= 48.61 time= 0.11\n","Epoch: 061 train_loss= 2520491.30 val_loss= 48.63 time= 0.13\n","Epoch: 071 train_loss= 2406480.80 val_loss= 48.63 time= 0.15\n","Epoch: 081 train_loss= 2598102.20 val_loss= 48.63 time= 0.17\n","Epoch: 091 train_loss= 2505775.80 val_loss= 48.63 time= 0.19\n","Epoch: 101 train_loss= 2906638.55 val_loss= 48.63 time= 0.21\n","test error=285.50,0.15, mean test error=299.07,0.17\n","testing on day #32\n","Epoch: 001 train_loss= 3361502.67 val_loss= 35.74 time= 0.00\n","Epoch: 011 train_loss= 3040362.00 val_loss= 35.55 time= 0.02\n","Epoch: 021 train_loss= 2713726.24 val_loss= 35.78 time= 0.05\n","Epoch: 031 train_loss= 2826383.14 val_loss= 35.27 time= 0.07\n","Epoch: 041 train_loss= 2911290.60 val_loss= 35.59 time= 0.09\n","Epoch: 051 train_loss= 2834182.79 val_loss= 35.69 time= 0.11\n","Epoch: 061 train_loss= 2862638.29 val_loss= 35.71 time= 0.13\n","Epoch: 071 train_loss= 2905968.69 val_loss= 35.71 time= 0.15\n","Epoch: 081 train_loss= 3076410.45 val_loss= 35.71 time= 0.17\n","Epoch: 091 train_loss= 2795380.80 val_loss= 35.71 time= 0.20\n","Epoch: 101 train_loss= 3110775.60 val_loss= 35.71 time= 0.22\n","test error=315.37,0.19, mean test error=299.97,0.17\n","testing on day #33\n","Epoch: 001 train_loss= 3288717.86 val_loss= 59.74 time= 0.00\n","Epoch: 011 train_loss= 2789231.18 val_loss= 55.61 time= 0.03\n","Epoch: 021 train_loss= 2540661.14 val_loss= 55.99 time= 0.05\n","Epoch: 031 train_loss= 3035887.73 val_loss= 56.35 time= 0.07\n","Epoch: 041 train_loss= 2658969.86 val_loss= 56.12 time= 0.09\n","Epoch: 051 train_loss= 2837768.86 val_loss= 56.09 time= 0.11\n","Epoch: 061 train_loss= 2594350.80 val_loss= 56.09 time= 0.14\n","Epoch: 071 train_loss= 3188258.14 val_loss= 56.09 time= 0.16\n","Epoch: 081 train_loss= 2784314.41 val_loss= 56.09 time= 0.18\n","Epoch: 091 train_loss= 2868735.05 val_loss= 56.09 time= 0.20\n","Epoch: 101 train_loss= 2724594.64 val_loss= 56.09 time= 0.22\n","test error=296.62,0.15, mean test error=299.80,0.17\n","testing on day #34\n","Epoch: 001 train_loss= 3205838.57 val_loss= 42.56 time= 0.00\n","Epoch: 011 train_loss= 3222361.13 val_loss= 40.62 time= 0.03\n","Epoch: 021 train_loss= 3002474.13 val_loss= 41.56 time= 0.05\n","Epoch: 031 train_loss= 3111009.87 val_loss= 41.69 time= 0.07\n","Epoch: 041 train_loss= 3195074.65 val_loss= 41.51 time= 0.10\n","Epoch: 051 train_loss= 2988616.35 val_loss= 41.49 time= 0.12\n","Epoch: 061 train_loss= 2941365.59 val_loss= 41.49 time= 0.14\n","Epoch: 071 train_loss= 2812137.65 val_loss= 41.49 time= 0.16\n","Epoch: 081 train_loss= 3397682.61 val_loss= 41.49 time= 0.19\n","Epoch: 091 train_loss= 2691968.21 val_loss= 41.49 time= 0.21\n","Epoch: 101 train_loss= 2837778.10 val_loss= 41.49 time= 0.23\n","test error=315.02,0.20, mean test error=300.56,0.17\n","testing on day #35\n","Epoch: 001 train_loss= 3354060.42 val_loss= 71.31 time= 0.00\n","Epoch: 011 train_loss= 3745797.25 val_loss= 60.08 time= 0.03\n","Epoch: 021 train_loss= 3292668.25 val_loss= 59.86 time= 0.05\n","Epoch: 031 train_loss= 2782732.00 val_loss= 60.72 time= 0.07\n","Epoch: 041 train_loss= 2861925.33 val_loss= 60.67 time= 0.10\n","Epoch: 051 train_loss= 3335741.25 val_loss= 60.65 time= 0.12\n","Epoch: 061 train_loss= 3189133.50 val_loss= 60.65 time= 0.15\n","Epoch: 071 train_loss= 3058340.25 val_loss= 60.65 time= 0.17\n","Epoch: 081 train_loss= 2591363.25 val_loss= 60.65 time= 0.19\n","Epoch: 091 train_loss= 2945262.83 val_loss= 60.65 time= 0.22\n","Epoch: 101 train_loss= 2705437.25 val_loss= 60.65 time= 0.24\n","test error=262.03,0.14, mean test error=298.72,0.17\n","testing on day #36\n","Epoch: 001 train_loss= 3530317.08 val_loss= 44.30 time= 0.00\n","Epoch: 011 train_loss= 3132155.46 val_loss= 47.58 time= 0.03\n","Epoch: 021 train_loss= 3473389.57 val_loss= 48.48 time= 0.05\n","Epoch: 031 train_loss= 3022938.09 val_loss= 49.27 time= 0.08\n","Epoch: 041 train_loss= 3440600.64 val_loss= 49.28 time= 0.10\n","Epoch: 051 train_loss= 3577545.63 val_loss= 49.27 time= 0.13\n","Epoch: 061 train_loss= 3381712.67 val_loss= 49.27 time= 0.15\n","Epoch: 071 train_loss= 3136716.86 val_loss= 49.27 time= 0.18\n","Epoch: 081 train_loss= 3276746.06 val_loss= 49.27 time= 0.21\n","Epoch: 091 train_loss= 3236041.10 val_loss= 49.27 time= 0.23\n","Epoch: 101 train_loss= 3052078.13 val_loss= 49.27 time= 0.26\n","test error=359.11,0.20, mean test error=301.47,0.17\n","testing on day #37\n","Epoch: 001 train_loss= 3437403.46 val_loss= 73.17 time= 0.00\n","Epoch: 011 train_loss= 3162315.77 val_loss= 62.08 time= 0.03\n","Epoch: 021 train_loss= 3184367.85 val_loss= 61.58 time= 0.06\n","Epoch: 031 train_loss= 2904093.35 val_loss= 62.14 time= 0.08\n","Epoch: 041 train_loss= 3174168.35 val_loss= 61.86 time= 0.11\n","Epoch: 051 train_loss= 3090168.73 val_loss= 61.84 time= 0.13\n","Epoch: 061 train_loss= 2962585.58 val_loss= 61.84 time= 0.16\n","Epoch: 071 train_loss= 3238763.35 val_loss= 61.84 time= 0.18\n","Epoch: 081 train_loss= 3163757.27 val_loss= 61.84 time= 0.21\n","Epoch: 091 train_loss= 3369654.65 val_loss= 61.84 time= 0.24\n","Epoch: 101 train_loss= 3018708.42 val_loss= 61.84 time= 0.26\n","test error=278.97,0.14, mean test error=300.49,0.17\n","testing on day #38\n","Epoch: 001 train_loss= 3580332.63 val_loss= 50.95 time= 0.00\n","Epoch: 011 train_loss= 3258247.31 val_loss= 49.77 time= 0.03\n","Epoch: 021 train_loss= 3103409.59 val_loss= 50.43 time= 0.06\n","Epoch: 031 train_loss= 2890526.10 val_loss= 50.01 time= 0.08\n","Epoch: 041 train_loss= 2983895.77 val_loss= 50.03 time= 0.11\n","Epoch: 051 train_loss= 3451156.02 val_loss= 50.02 time= 0.14\n","Epoch: 061 train_loss= 2858545.40 val_loss= 50.03 time= 0.17\n","Epoch: 071 train_loss= 3183967.81 val_loss= 50.03 time= 0.19\n","Epoch: 081 train_loss= 2794723.39 val_loss= 50.03 time= 0.22\n","Epoch: 091 train_loss= 3114584.43 val_loss= 50.03 time= 0.25\n","Epoch: 101 train_loss= 3465909.63 val_loss= 50.03 time= 0.27\n","test error=342.60,0.20, mean test error=302.24,0.17\n","testing on day #39\n","Epoch: 001 train_loss= 3719630.79 val_loss= 78.20 time= 0.00\n","Epoch: 011 train_loss= 3169693.07 val_loss= 67.01 time= 0.03\n","Epoch: 021 train_loss= 3260121.71 val_loss= 62.57 time= 0.06\n","Epoch: 031 train_loss= 3077779.00 val_loss= 62.35 time= 0.09\n","Epoch: 041 train_loss= 3141469.57 val_loss= 62.46 time= 0.11\n","Epoch: 051 train_loss= 2943682.71 val_loss= 62.46 time= 0.14\n","Epoch: 061 train_loss= 3402208.43 val_loss= 62.46 time= 0.17\n","Epoch: 071 train_loss= 3301678.00 val_loss= 62.46 time= 0.20\n","Epoch: 081 train_loss= 2945433.68 val_loss= 62.46 time= 0.23\n","Epoch: 091 train_loss= 2932909.29 val_loss= 62.46 time= 0.25\n","Epoch: 101 train_loss= 3202754.11 val_loss= 62.46 time= 0.28\n","test error=297.11,0.14, mean test error=302.04,0.17\n","testing on day #40\n","Epoch: 001 train_loss= 3443289.69 val_loss= 53.58 time= 0.00\n","Epoch: 011 train_loss= 3256238.35 val_loss= 50.96 time= 0.03\n","Epoch: 021 train_loss= 3064377.84 val_loss= 49.23 time= 0.06\n","Epoch: 031 train_loss= 3558877.88 val_loss= 49.33 time= 0.09\n","Epoch: 041 train_loss= 3027249.21 val_loss= 49.08 time= 0.12\n","Epoch: 051 train_loss= 3250544.49 val_loss= 49.09 time= 0.15\n","Epoch: 061 train_loss= 3193536.91 val_loss= 49.09 time= 0.18\n","Epoch: 071 train_loss= 3279888.77 val_loss= 49.10 time= 0.20\n","Epoch: 081 train_loss= 3626067.16 val_loss= 49.09 time= 0.23\n","Epoch: 091 train_loss= 3333088.96 val_loss= 49.09 time= 0.26\n","Epoch: 101 train_loss= 3426947.93 val_loss= 49.09 time= 0.29\n","test error=371.84,0.19, mean test error=304.72,0.17\n","testing on day #41\n","Epoch: 001 train_loss= 3897263.50 val_loss= 81.17 time= 0.00\n","Epoch: 011 train_loss= 3470105.80 val_loss= 59.53 time= 0.03\n","Epoch: 021 train_loss= 3475834.27 val_loss= 51.91 time= 0.06\n","Epoch: 031 train_loss= 3403860.80 val_loss= 53.16 time= 0.09\n","Epoch: 041 train_loss= 3113811.08 val_loss= 52.81 time= 0.12\n","Epoch: 051 train_loss= 3531851.07 val_loss= 52.84 time= 0.15\n","Epoch: 061 train_loss= 3061064.72 val_loss= 52.84 time= 0.18\n","Epoch: 071 train_loss= 3459368.67 val_loss= 52.84 time= 0.21\n","Epoch: 081 train_loss= 3505405.27 val_loss= 52.84 time= 0.24\n","Epoch: 091 train_loss= 3303018.23 val_loss= 52.84 time= 0.27\n","Epoch: 101 train_loss= 3682894.87 val_loss= 52.84 time= 0.30\n","test error=342.54,0.15, mean test error=306.12,0.17\n","testing on day #42\n","Epoch: 001 train_loss= 4192835.37 val_loss= 67.64 time= 0.00\n","Epoch: 011 train_loss= 3585661.58 val_loss= 58.41 time= 0.03\n","Epoch: 021 train_loss= 3072656.59 val_loss= 54.21 time= 0.06\n","Epoch: 031 train_loss= 3480686.69 val_loss= 56.10 time= 0.09\n","Epoch: 041 train_loss= 3421318.11 val_loss= 56.53 time= 0.12\n","Epoch: 051 train_loss= 3283169.74 val_loss= 56.59 time= 0.15\n","Epoch: 061 train_loss= 3179336.50 val_loss= 56.61 time= 0.19\n","Epoch: 071 train_loss= 2925846.43 val_loss= 56.61 time= 0.22\n","Epoch: 081 train_loss= 3387636.37 val_loss= 56.61 time= 0.25\n","Epoch: 091 train_loss= 3307970.95 val_loss= 56.61 time= 0.28\n","Epoch: 101 train_loss= 3637986.10 val_loss= 56.61 time= 0.31\n","test error=365.46,0.18, mean test error=308.24,0.17\n","testing on day #43\n","Epoch: 001 train_loss= 4341601.50 val_loss= 81.70 time= 0.00\n","Epoch: 011 train_loss= 3346184.81 val_loss= 52.50 time= 0.04\n","Epoch: 021 train_loss= 3456342.31 val_loss= 55.91 time= 0.07\n","Epoch: 031 train_loss= 3220081.31 val_loss= 54.81 time= 0.10\n","Epoch: 041 train_loss= 3317223.62 val_loss= 55.40 time= 0.13\n","Epoch: 051 train_loss= 3060178.38 val_loss= 55.47 time= 0.16\n","Epoch: 061 train_loss= 3732452.62 val_loss= 55.47 time= 0.19\n","Epoch: 071 train_loss= 3720840.75 val_loss= 55.47 time= 0.22\n","Epoch: 081 train_loss= 3195913.94 val_loss= 55.47 time= 0.25\n","Epoch: 091 train_loss= 3206509.25 val_loss= 55.47 time= 0.28\n","Epoch: 101 train_loss= 3703008.31 val_loss= 55.47 time= 0.32\n","test error=293.33,0.14, mean test error=307.73,0.17\n","testing on day #44\n","Epoch: 001 train_loss= 3996283.20 val_loss= 57.99 time= 0.00\n","Epoch: 011 train_loss= 3286691.26 val_loss= 55.21 time= 0.04\n","Epoch: 021 train_loss= 3591085.58 val_loss= 52.67 time= 0.07\n","Epoch: 031 train_loss= 3282724.61 val_loss= 56.53 time= 0.10\n","Epoch: 041 train_loss= 3407233.71 val_loss= 57.18 time= 0.14\n","Epoch: 051 train_loss= 3115202.14 val_loss= 56.89 time= 0.17\n","Epoch: 061 train_loss= 3474798.55 val_loss= 57.11 time= 0.20\n","Epoch: 071 train_loss= 3092586.27 val_loss= 57.11 time= 0.23\n","Epoch: 081 train_loss= 3313465.56 val_loss= 57.11 time= 0.27\n","Epoch: 091 train_loss= 3319630.60 val_loss= 57.11 time= 0.30\n","Epoch: 101 train_loss= 3034692.29 val_loss= 57.11 time= 0.33\n","test error=315.30,0.18, mean test error=307.98,0.17\n","testing on day #45\n","Epoch: 001 train_loss= 3993340.65 val_loss= 51.38 time= 0.00\n","Epoch: 011 train_loss= 3321256.78 val_loss= 47.42 time= 0.04\n","Epoch: 021 train_loss= 3345437.37 val_loss= 51.78 time= 0.07\n","Epoch: 031 train_loss= 3697269.15 val_loss= 51.26 time= 0.10\n","Epoch: 041 train_loss= 3230981.37 val_loss= 51.13 time= 0.14\n","Epoch: 051 train_loss= 3666076.81 val_loss= 51.11 time= 0.17\n","Epoch: 061 train_loss= 3473092.49 val_loss= 51.10 time= 0.20\n","Epoch: 071 train_loss= 3570385.71 val_loss= 51.10 time= 0.24\n","Epoch: 081 train_loss= 3326793.35 val_loss= 51.10 time= 0.27\n","Epoch: 091 train_loss= 3516847.72 val_loss= 51.10 time= 0.30\n","Epoch: 101 train_loss= 3719449.32 val_loss= 51.10 time= 0.34\n","test error=287.95,0.14, mean test error=307.33,0.17\n","testing on day #46\n","Epoch: 001 train_loss= 4173780.81 val_loss= 58.37 time= 0.00\n","Epoch: 011 train_loss= 3351579.76 val_loss= 59.70 time= 0.04\n","Epoch: 021 train_loss= 3127142.34 val_loss= 57.59 time= 0.07\n","Epoch: 031 train_loss= 3328845.11 val_loss= 57.54 time= 0.11\n","Epoch: 041 train_loss= 3303334.30 val_loss= 57.69 time= 0.14\n","Epoch: 051 train_loss= 3194994.44 val_loss= 57.70 time= 0.17\n","Epoch: 061 train_loss= 3355016.44 val_loss= 57.70 time= 0.21\n","Epoch: 071 train_loss= 3289381.99 val_loss= 57.70 time= 0.24\n","Epoch: 081 train_loss= 3236276.96 val_loss= 57.70 time= 0.28\n","Epoch: 091 train_loss= 3317153.11 val_loss= 57.70 time= 0.31\n","Epoch: 101 train_loss= 3487955.52 val_loss= 57.70 time= 0.35\n","test error=300.91,0.18, mean test error=307.13,0.17\n","testing on day #47\n","Epoch: 001 train_loss= 4653432.17 val_loss= 55.33 time= 0.00\n","Epoch: 011 train_loss= 3683847.28 val_loss= 47.59 time= 0.04\n","Epoch: 021 train_loss= 3403757.89 val_loss= 49.10 time= 0.07\n","Epoch: 031 train_loss= 3450906.58 val_loss= 47.88 time= 0.11\n","Epoch: 041 train_loss= 3490870.83 val_loss= 47.61 time= 0.14\n","Epoch: 051 train_loss= 3472412.03 val_loss= 47.45 time= 0.18\n","Epoch: 061 train_loss= 3435843.67 val_loss= 47.46 time= 0.21\n","Epoch: 071 train_loss= 3940568.56 val_loss= 47.46 time= 0.25\n","Epoch: 081 train_loss= 3810952.56 val_loss= 47.46 time= 0.28\n","Epoch: 091 train_loss= 3351792.11 val_loss= 47.46 time= 0.32\n","Epoch: 101 train_loss= 3391522.75 val_loss= 47.46 time= 0.35\n","test error=316.89,0.17, mean test error=307.43,0.17\n","testing on day #48\n","Epoch: 001 train_loss= 3778575.15 val_loss= 53.09 time= 0.00\n","Epoch: 011 train_loss= 3580903.89 val_loss= 54.34 time= 0.04\n","Epoch: 021 train_loss= 3474884.26 val_loss= 52.84 time= 0.08\n","Epoch: 031 train_loss= 3806662.41 val_loss= 54.17 time= 0.11\n","Epoch: 041 train_loss= 3014303.12 val_loss= 54.30 time= 0.15\n","Epoch: 051 train_loss= 3487569.96 val_loss= 54.31 time= 0.18\n","Epoch: 061 train_loss= 3473224.68 val_loss= 54.31 time= 0.22\n","Epoch: 071 train_loss= 3488141.49 val_loss= 54.30 time= 0.26\n","Epoch: 081 train_loss= 3505271.20 val_loss= 54.30 time= 0.29\n","Epoch: 091 train_loss= 3681172.64 val_loss= 54.31 time= 0.33\n","Epoch: 101 train_loss= 3675842.50 val_loss= 54.30 time= 0.36\n","test error=399.88,0.22, mean test error=310.15,0.17\n","testing on day #49\n","Epoch: 001 train_loss= 4228547.21 val_loss= 50.32 time= 0.00\n","Epoch: 011 train_loss= 3692000.36 val_loss= 50.63 time= 0.04\n","Epoch: 021 train_loss= 3419116.61 val_loss= 50.27 time= 0.08\n","Epoch: 031 train_loss= 3486185.18 val_loss= 50.53 time= 0.11\n","Epoch: 041 train_loss= 3486031.63 val_loss= 50.61 time= 0.15\n","Epoch: 051 train_loss= 3638744.18 val_loss= 50.64 time= 0.19\n","Epoch: 061 train_loss= 3443931.28 val_loss= 50.63 time= 0.22\n","Epoch: 071 train_loss= 3363689.80 val_loss= 50.63 time= 0.26\n","Epoch: 081 train_loss= 3407385.29 val_loss= 50.63 time= 0.30\n","Epoch: 091 train_loss= 3635350.13 val_loss= 50.63 time= 0.33\n","Epoch: 101 train_loss= 3253318.20 val_loss= 50.63 time= 0.37\n","test error=360.38,0.19, mean test error=311.58,0.17\n","testing on day #50\n","Epoch: 001 train_loss= 4311596.33 val_loss= 59.56 time= 0.00\n","Epoch: 011 train_loss= 3881586.33 val_loss= 62.63 time= 0.04\n","Epoch: 021 train_loss= 3371122.06 val_loss= 61.01 time= 0.08\n","Epoch: 031 train_loss= 3453994.77 val_loss= 61.22 time= 0.12\n","Epoch: 041 train_loss= 3644323.94 val_loss= 61.22 time= 0.15\n","Epoch: 051 train_loss= 3077761.60 val_loss= 61.20 time= 0.19\n","Epoch: 061 train_loss= 3577351.41 val_loss= 61.20 time= 0.23\n","Epoch: 071 train_loss= 3696963.87 val_loss= 61.21 time= 0.27\n","Epoch: 081 train_loss= 3475572.32 val_loss= 61.20 time= 0.30\n","Epoch: 091 train_loss= 3664521.58 val_loss= 61.21 time= 0.34\n","Epoch: 101 train_loss= 3029704.48 val_loss= 61.21 time= 0.38\n","test error=321.96,0.19, mean test error=311.87,0.17\n","testing on day #51\n","Epoch: 001 train_loss= 4361542.00 val_loss= 59.65 time= 0.00\n","Epoch: 011 train_loss= 3836344.75 val_loss= 53.38 time= 0.04\n","Epoch: 021 train_loss= 3755858.15 val_loss= 56.93 time= 0.08\n","Epoch: 031 train_loss= 3735718.55 val_loss= 56.51 time= 0.12\n","Epoch: 041 train_loss= 3683794.25 val_loss= 56.58 time= 0.16\n","Epoch: 051 train_loss= 3719813.35 val_loss= 56.58 time= 0.20\n","Epoch: 061 train_loss= 3674716.35 val_loss= 56.57 time= 0.24\n","Epoch: 071 train_loss= 3318160.15 val_loss= 56.58 time= 0.27\n","Epoch: 081 train_loss= 3644954.52 val_loss= 56.58 time= 0.31\n","Epoch: 091 train_loss= 3481308.00 val_loss= 56.58 time= 0.35\n","Epoch: 101 train_loss= 3605843.30 val_loss= 56.58 time= 0.39\n","test error=245.26,0.16, mean test error=310.07,0.17\n","testing on day #52\n","Epoch: 001 train_loss= 4162998.11 val_loss= 56.15 time= 0.00\n","Epoch: 011 train_loss= 3725767.26 val_loss= 57.06 time= 0.05\n","Epoch: 021 train_loss= 3673004.79 val_loss= 52.14 time= 0.09\n","Epoch: 031 train_loss= 3320668.56 val_loss= 52.54 time= 0.13\n","Epoch: 041 train_loss= 3713187.23 val_loss= 54.28 time= 0.17\n","Epoch: 051 train_loss= 3782580.76 val_loss= 54.37 time= 0.21\n","Epoch: 061 train_loss= 3630634.65 val_loss= 54.41 time= 0.25\n","Epoch: 071 train_loss= 3748188.00 val_loss= 54.40 time= 0.29\n","Epoch: 081 train_loss= 3441708.93 val_loss= 54.40 time= 0.33\n","Epoch: 091 train_loss= 3822433.50 val_loss= 54.40 time= 0.37\n","Epoch: 101 train_loss= 3848355.75 val_loss= 54.40 time= 0.41\n","test error=291.15,0.20, mean test error=309.57,0.17\n","testing on day #53\n","Epoch: 001 train_loss= 4968438.26 val_loss= 45.48 time= 0.00\n","Epoch: 011 train_loss= 3640132.36 val_loss= 47.56 time= 0.05\n","Epoch: 021 train_loss= 3646862.80 val_loss= 42.79 time= 0.09\n","Epoch: 031 train_loss= 4157954.99 val_loss= 44.47 time= 0.13\n","Epoch: 041 train_loss= 3794957.71 val_loss= 44.84 time= 0.17\n","Epoch: 051 train_loss= 3726333.89 val_loss= 44.18 time= 0.21\n","Epoch: 061 train_loss= 3490273.69 val_loss= 44.15 time= 0.25\n","Epoch: 071 train_loss= 3655583.95 val_loss= 44.15 time= 0.29\n","Epoch: 081 train_loss= 3313037.01 val_loss= 44.16 time= 0.33\n","Epoch: 091 train_loss= 3753723.95 val_loss= 44.14 time= 0.37\n","Epoch: 101 train_loss= 3555688.82 val_loss= 44.15 time= 0.41\n","test error=272.99,0.18, mean test error=308.64,0.17\n","testing on day #54\n","Epoch: 001 train_loss= 3971686.30 val_loss= 43.20 time= 0.00\n","Epoch: 011 train_loss= 3699678.47 val_loss= 49.05 time= 0.05\n","Epoch: 021 train_loss= 3267937.30 val_loss= 49.59 time= 0.09\n","Epoch: 031 train_loss= 3734294.05 val_loss= 48.67 time= 0.13\n","Epoch: 041 train_loss= 3463082.16 val_loss= 48.66 time= 0.17\n","Epoch: 051 train_loss= 3332337.50 val_loss= 48.66 time= 0.21\n","Epoch: 061 train_loss= 3396225.00 val_loss= 48.66 time= 0.25\n","Epoch: 071 train_loss= 3371211.23 val_loss= 48.65 time= 0.30\n","Epoch: 081 train_loss= 3654766.09 val_loss= 48.66 time= 0.34\n","Epoch: 091 train_loss= 3331095.41 val_loss= 48.66 time= 0.38\n","Epoch: 101 train_loss= 3301570.85 val_loss= 48.66 time= 0.42\n","test error=222.79,0.18, mean test error=306.49,0.17\n","testing on day #55\n","Epoch: 001 train_loss= 4173129.91 val_loss= 41.23 time= 0.00\n","Epoch: 011 train_loss= 3759060.11 val_loss= 43.43 time= 0.05\n","Epoch: 021 train_loss= 3970202.09 val_loss= 45.00 time= 0.09\n","Epoch: 031 train_loss= 3562539.45 val_loss= 45.17 time= 0.13\n","Epoch: 041 train_loss= 3553777.39 val_loss= 45.10 time= 0.17\n","Epoch: 051 train_loss= 3665454.14 val_loss= 45.09 time= 0.22\n","Epoch: 061 train_loss= 3337076.64 val_loss= 45.07 time= 0.26\n","Epoch: 071 train_loss= 3900868.07 val_loss= 45.04 time= 0.30\n","Epoch: 081 train_loss= 3662620.50 val_loss= 45.05 time= 0.34\n","Epoch: 091 train_loss= 3267144.52 val_loss= 45.03 time= 0.39\n","Epoch: 101 train_loss= 3503761.91 val_loss= 45.03 time= 0.43\n","test error=201.25,0.14, mean test error=303.92,0.17\n","testing on day #56\n","Epoch: 001 train_loss= 4218970.31 val_loss= 44.34 time= 0.00\n","Epoch: 011 train_loss= 3426054.98 val_loss= 48.77 time= 0.05\n","Epoch: 021 train_loss= 3428071.76 val_loss= 45.52 time= 0.09\n","Epoch: 031 train_loss= 3547004.51 val_loss= 46.28 time= 0.13\n","Epoch: 041 train_loss= 3452013.62 val_loss= 46.21 time= 0.18\n","Epoch: 051 train_loss= 3283928.85 val_loss= 46.19 time= 0.22\n","Epoch: 061 train_loss= 3415882.72 val_loss= 46.19 time= 0.26\n","Epoch: 071 train_loss= 3540093.68 val_loss= 46.19 time= 0.31\n","Epoch: 081 train_loss= 3829126.23 val_loss= 46.19 time= 0.35\n","Epoch: 091 train_loss= 3443745.84 val_loss= 46.19 time= 0.39\n","Epoch: 101 train_loss= 3689224.47 val_loss= 46.19 time= 0.44\n","test error=221.64,0.17, mean test error=301.96,0.17\n","testing on day #57\n","Epoch: 001 train_loss= 4019323.59 val_loss= 42.83 time= 0.00\n","Epoch: 011 train_loss= 3503699.11 val_loss= 37.38 time= 0.05\n","Epoch: 021 train_loss= 3394518.78 val_loss= 44.76 time= 0.09\n","Epoch: 031 train_loss= 3803716.41 val_loss= 45.21 time= 0.14\n","Epoch: 041 train_loss= 3237250.16 val_loss= 44.37 time= 0.18\n","Epoch: 051 train_loss= 3498000.59 val_loss= 44.34 time= 0.22\n","Epoch: 061 train_loss= 3648165.15 val_loss= 44.36 time= 0.27\n","Epoch: 071 train_loss= 3555172.11 val_loss= 44.35 time= 0.31\n","Epoch: 081 train_loss= 3510333.62 val_loss= 44.35 time= 0.36\n","Epoch: 091 train_loss= 3657862.09 val_loss= 44.36 time= 0.40\n","Epoch: 101 train_loss= 3431039.73 val_loss= 44.35 time= 0.44\n","test error=211.00,0.15, mean test error=299.85,0.17\n","testing on day #58\n","Epoch: 001 train_loss= 4265167.27 val_loss= 47.50 time= 0.00\n","Epoch: 011 train_loss= 3684079.42 val_loss= 41.90 time= 0.05\n","Epoch: 021 train_loss= 3478407.86 val_loss= 43.74 time= 0.09\n","Epoch: 031 train_loss= 3154104.23 val_loss= 41.80 time= 0.14\n","Epoch: 041 train_loss= 3473806.17 val_loss= 42.05 time= 0.18\n","Epoch: 051 train_loss= 3365325.71 val_loss= 42.10 time= 0.23\n","Epoch: 061 train_loss= 3163565.43 val_loss= 42.10 time= 0.27\n","Epoch: 071 train_loss= 3269457.04 val_loss= 42.10 time= 0.32\n","Epoch: 081 train_loss= 3556846.48 val_loss= 42.10 time= 0.36\n","Epoch: 091 train_loss= 3239142.89 val_loss= 42.10 time= 0.41\n","Epoch: 101 train_loss= 3371752.90 val_loss= 42.10 time= 0.45\n","test error=239.25,0.20, mean test error=298.47,0.17\n","testing on day #59\n","Epoch: 001 train_loss= 3769805.88 val_loss= 34.35 time= 0.00\n","Epoch: 011 train_loss= 3150299.88 val_loss= 33.91 time= 0.05\n","Epoch: 021 train_loss= 3200757.42 val_loss= 35.49 time= 0.10\n","Epoch: 031 train_loss= 3490223.58 val_loss= 34.65 time= 0.14\n","Epoch: 041 train_loss= 3365153.42 val_loss= 34.84 time= 0.19\n","Epoch: 051 train_loss= 3575757.54 val_loss= 34.82 time= 0.23\n","Epoch: 061 train_loss= 3341111.79 val_loss= 34.82 time= 0.28\n","Epoch: 071 train_loss= 3387671.96 val_loss= 34.82 time= 0.33\n","Epoch: 081 train_loss= 3331335.88 val_loss= 34.82 time= 0.37\n","Epoch: 091 train_loss= 3554202.67 val_loss= 34.82 time= 0.42\n","Epoch: 101 train_loss= 3652951.00 val_loss= 34.82 time= 0.46\n","test error=214.95,0.16, mean test error=296.61,0.17\n","testing on day #60\n","Epoch: 001 train_loss= 4408831.26 val_loss= 19.80 time= 0.00\n","Epoch: 011 train_loss= 3264001.27 val_loss= 25.95 time= 0.05\n","Epoch: 021 train_loss= 3244441.15 val_loss= 27.39 time= 0.10\n","Epoch: 031 train_loss= 3339767.59 val_loss= 27.32 time= 0.15\n","Epoch: 041 train_loss= 3326920.59 val_loss= 27.41 time= 0.19\n","Epoch: 051 train_loss= 3529602.93 val_loss= 27.42 time= 0.24\n","Epoch: 061 train_loss= 3747596.35 val_loss= 27.42 time= 0.29\n","Epoch: 071 train_loss= 3276434.97 val_loss= 27.42 time= 0.33\n","Epoch: 081 train_loss= 3522544.82 val_loss= 27.42 time= 0.38\n","Epoch: 091 train_loss= 3415031.03 val_loss= 27.42 time= 0.43\n","Epoch: 101 train_loss= 3567098.09 val_loss= 27.42 time= 0.48\n","test error=214.78,0.18, mean test error=294.84,0.17\n","testing on day #61\n","Epoch: 001 train_loss= 4347064.14 val_loss= 17.23 time= 0.01\n","Epoch: 011 train_loss= 3306231.38 val_loss= 22.82 time= 0.05\n","Epoch: 021 train_loss= 3353122.26 val_loss= 23.27 time= 0.10\n","Epoch: 031 train_loss= 3442017.17 val_loss= 22.51 time= 0.15\n","Epoch: 041 train_loss= 3469971.58 val_loss= 22.53 time= 0.20\n","Epoch: 051 train_loss= 3333823.90 val_loss= 22.53 time= 0.24\n","Epoch: 061 train_loss= 3533857.89 val_loss= 22.53 time= 0.29\n","Epoch: 071 train_loss= 3354749.90 val_loss= 22.53 time= 0.34\n","Epoch: 081 train_loss= 3513632.48 val_loss= 22.53 time= 0.39\n","Epoch: 091 train_loss= 3584854.95 val_loss= 22.53 time= 0.43\n","Epoch: 101 train_loss= 3394671.45 val_loss= 22.53 time= 0.48\n","test error=216.17,0.16, mean test error=293.16,0.17\n","testing on day #62\n","Epoch: 001 train_loss= 4380203.23 val_loss= 17.55 time= 0.01\n","Epoch: 011 train_loss= 3273281.94 val_loss= 19.50 time= 0.05\n","Epoch: 021 train_loss= 3330281.25 val_loss= 20.25 time= 0.10\n","Epoch: 031 train_loss= 3434114.02 val_loss= 20.08 time= 0.15\n","Epoch: 041 train_loss= 3262090.12 val_loss= 20.11 time= 0.20\n","Epoch: 051 train_loss= 3277563.05 val_loss= 20.13 time= 0.25\n","Epoch: 061 train_loss= 3115082.67 val_loss= 20.13 time= 0.30\n","Epoch: 071 train_loss= 3238348.66 val_loss= 20.13 time= 0.35\n","Epoch: 081 train_loss= 3437396.75 val_loss= 20.13 time= 0.40\n","Epoch: 091 train_loss= 3497060.52 val_loss= 20.13 time= 0.44\n","Epoch: 101 train_loss= 3317057.19 val_loss= 20.13 time= 0.49\n","test error=224.35,0.22, mean test error=291.73,0.17\n","testing on day #63\n","Epoch: 001 train_loss= 4173010.05 val_loss= 16.18 time= 0.01\n","Epoch: 011 train_loss= 3506328.52 val_loss= 16.31 time= 0.06\n","Epoch: 021 train_loss= 3701909.41 val_loss= 17.25 time= 0.11\n","Epoch: 031 train_loss= 3306247.55 val_loss= 17.28 time= 0.16\n","Epoch: 041 train_loss= 3197790.02 val_loss= 17.18 time= 0.20\n","Epoch: 051 train_loss= 3304853.08 val_loss= 17.21 time= 0.25\n","Epoch: 061 train_loss= 3428918.68 val_loss= 17.21 time= 0.30\n","Epoch: 071 train_loss= 3616273.88 val_loss= 17.21 time= 0.35\n","Epoch: 081 train_loss= 3581998.96 val_loss= 17.21 time= 0.40\n","Epoch: 091 train_loss= 3682045.92 val_loss= 17.21 time= 0.45\n","Epoch: 101 train_loss= 3816808.87 val_loss= 17.21 time= 0.50\n","test error=199.82,0.15, mean test error=289.85,0.17\n","testing on day #64\n","Epoch: 001 train_loss= 3880481.25 val_loss= 19.56 time= 0.01\n","Epoch: 011 train_loss= 3776535.78 val_loss= 14.99 time= 0.06\n","Epoch: 021 train_loss= 3120703.11 val_loss= 14.83 time= 0.11\n","test error=198.79,0.20, mean test error=288.03,0.17\n","testing on day #65\n","Epoch: 001 train_loss= 4006862.00 val_loss= 17.07 time= 0.01\n","Epoch: 011 train_loss= 3415981.78 val_loss= 14.32 time= 0.06\n","Epoch: 021 train_loss= 3247118.95 val_loss= 13.16 time= 0.11\n","Epoch: 031 train_loss= 3337214.96 val_loss= 12.91 time= 0.16\n","Epoch: 041 train_loss= 3337283.74 val_loss= 12.83 time= 0.21\n","Epoch: 051 train_loss= 3456451.68 val_loss= 12.81 time= 0.27\n","Epoch: 061 train_loss= 3365093.76 val_loss= 12.88 time= 0.32\n","Epoch: 071 train_loss= 3471110.97 val_loss= 12.81 time= 0.37\n","Epoch: 081 train_loss= 3540585.37 val_loss= 12.75 time= 0.42\n","Epoch: 091 train_loss= 3419824.16 val_loss= 12.72 time= 0.47\n","Epoch: 101 train_loss= 3373532.82 val_loss= 12.75 time= 0.53\n","test error=159.38,0.15, mean test error=285.51,0.17\n","testing on day #66\n","Epoch: 001 train_loss= 3943241.55 val_loss= 17.10 time= 0.01\n","Epoch: 011 train_loss= 3298163.18 val_loss= 13.60 time= 0.06\n","Epoch: 021 train_loss= 3021429.90 val_loss= 13.11 time= 0.11\n","Epoch: 031 train_loss= 3237796.30 val_loss= 12.75 time= 0.16\n","Epoch: 041 train_loss= 3430550.90 val_loss= 12.90 time= 0.22\n","Epoch: 051 train_loss= 3195009.96 val_loss= 12.90 time= 0.27\n","Epoch: 061 train_loss= 3217491.05 val_loss= 12.90 time= 0.32\n","Epoch: 071 train_loss= 3248245.01 val_loss= 12.90 time= 0.37\n","Epoch: 081 train_loss= 3420849.29 val_loss= 12.90 time= 0.42\n","Epoch: 091 train_loss= 3139097.43 val_loss= 12.90 time= 0.48\n","Epoch: 101 train_loss= 3029149.35 val_loss= 12.90 time= 0.53\n","test error=155.97,0.16, mean test error=283.02,0.17\n","testing on day #67\n","Epoch: 001 train_loss= 3747915.64 val_loss= 23.23 time= 0.01\n","Epoch: 011 train_loss= 3279947.02 val_loss= 11.98 time= 0.06\n","Epoch: 021 train_loss= 3488452.91 val_loss= 12.16 time= 0.11\n","Epoch: 031 train_loss= 3277853.88 val_loss= 11.77 time= 0.17\n","Epoch: 041 train_loss= 3137343.23 val_loss= 11.78 time= 0.22\n","Epoch: 051 train_loss= 3040843.39 val_loss= 11.74 time= 0.27\n","Epoch: 061 train_loss= 3292568.43 val_loss= 11.74 time= 0.33\n","Epoch: 071 train_loss= 3532419.57 val_loss= 11.74 time= 0.38\n","Epoch: 081 train_loss= 3246937.43 val_loss= 11.74 time= 0.43\n","Epoch: 091 train_loss= 3231678.09 val_loss= 11.74 time= 0.48\n","Epoch: 101 train_loss= 2922057.82 val_loss= 11.74 time= 0.54\n","test error=152.15,0.17, mean test error=280.55,0.17\n","testing on day #68\n","Epoch: 001 train_loss= 3986800.52 val_loss= 27.94 time= 0.01\n","Epoch: 011 train_loss= 3217779.66 val_loss= 12.25 time= 0.06\n","Epoch: 021 train_loss= 3180839.92 val_loss= 12.01 time= 0.12\n","Epoch: 031 train_loss= 3087823.96 val_loss= 11.86 time= 0.17\n","Epoch: 041 train_loss= 2988131.67 val_loss= 11.81 time= 0.23\n","Epoch: 051 train_loss= 3304280.63 val_loss= 11.80 time= 0.28\n","Epoch: 061 train_loss= 3477220.70 val_loss= 11.80 time= 0.33\n","Epoch: 071 train_loss= 3398705.50 val_loss= 11.80 time= 0.39\n","Epoch: 081 train_loss= 3411934.75 val_loss= 11.80 time= 0.44\n","Epoch: 091 train_loss= 3299129.48 val_loss= 11.80 time= 0.50\n","Epoch: 101 train_loss= 3310558.02 val_loss= 11.80 time= 0.55\n","test error=136.65,0.15, mean test error=277.88,0.17\n","testing on day #69\n","Epoch: 001 train_loss= 3862603.78 val_loss= 29.60 time= 0.01\n","Epoch: 011 train_loss= 3136630.20 val_loss= 11.17 time= 0.06\n","Epoch: 021 train_loss= 3353535.26 val_loss= 11.75 time= 0.12\n","Epoch: 031 train_loss= 3326578.15 val_loss= 10.74 time= 0.17\n","Epoch: 041 train_loss= 3154813.14 val_loss= 10.72 time= 0.23\n","Epoch: 051 train_loss= 3207962.86 val_loss= 10.65 time= 0.28\n","Epoch: 061 train_loss= 3338182.37 val_loss= 10.65 time= 0.34\n","Epoch: 071 train_loss= 3150547.67 val_loss= 10.65 time= 0.39\n","Epoch: 081 train_loss= 3164853.09 val_loss= 10.65 time= 0.45\n","Epoch: 091 train_loss= 3082307.09 val_loss= 10.65 time= 0.50\n","Epoch: 101 train_loss= 3167527.37 val_loss= 10.65 time= 0.56\n","test error=141.56,0.15, mean test error=275.41,0.17\n","testing on day #70\n","Epoch: 001 train_loss= 3499943.18 val_loss= 21.74 time= 0.01\n","Epoch: 011 train_loss= 3425406.67 val_loss= 10.42 time= 0.06\n","Epoch: 021 train_loss= 3390068.48 val_loss= 11.54 time= 0.12\n","Epoch: 031 train_loss= 3280490.24 val_loss= 10.28 time= 0.18\n","Epoch: 041 train_loss= 3042064.23 val_loss= 10.45 time= 0.23\n","Epoch: 051 train_loss= 3150251.83 val_loss= 10.29 time= 0.29\n","Epoch: 061 train_loss= 3127087.20 val_loss= 10.26 time= 0.34\n","Epoch: 071 train_loss= 3145731.86 val_loss= 10.26 time= 0.40\n","Epoch: 081 train_loss= 2920724.51 val_loss= 10.26 time= 0.46\n","Epoch: 091 train_loss= 3406570.18 val_loss= 10.26 time= 0.51\n","Epoch: 101 train_loss= 3319063.62 val_loss= 10.26 time= 0.57\n","test error=146.54,0.15, mean test error=273.10,0.17\n","testing on day #71\n","Epoch: 001 train_loss= 3735214.79 val_loss= 28.17 time= 0.01\n","Epoch: 011 train_loss= 3227959.66 val_loss= 10.32 time= 0.06\n","Epoch: 021 train_loss= 2936113.48 val_loss= 9.62 time= 0.12\n","Epoch: 031 train_loss= 3162246.62 val_loss= 9.84 time= 0.18\n","Epoch: 041 train_loss= 3186804.81 val_loss= 9.79 time= 0.23\n","Epoch: 051 train_loss= 2931875.72 val_loss= 9.78 time= 0.29\n","Epoch: 061 train_loss= 3149621.48 val_loss= 9.78 time= 0.35\n","Epoch: 071 train_loss= 3274180.09 val_loss= 9.78 time= 0.40\n","Epoch: 081 train_loss= 3018688.58 val_loss= 9.78 time= 0.46\n","Epoch: 091 train_loss= 3058766.40 val_loss= 9.78 time= 0.52\n","Epoch: 101 train_loss= 3133206.89 val_loss= 9.78 time= 0.57\n","test error=166.88,0.17, mean test error=271.24,0.17\n","testing on day #72\n","Epoch: 001 train_loss= 3642746.40 val_loss= 24.24 time= 0.01\n","Epoch: 011 train_loss= 3354800.32 val_loss= 9.66 time= 0.06\n","Epoch: 021 train_loss= 2963510.79 val_loss= 9.78 time= 0.12\n","Epoch: 031 train_loss= 3037421.46 val_loss= 9.94 time= 0.18\n","Epoch: 041 train_loss= 3020396.54 val_loss= 9.96 time= 0.24\n","Epoch: 051 train_loss= 3089637.15 val_loss= 9.96 time= 0.30\n","Epoch: 061 train_loss= 3168034.82 val_loss= 9.96 time= 0.35\n","Epoch: 071 train_loss= 2794290.56 val_loss= 9.96 time= 0.41\n","Epoch: 081 train_loss= 2808390.39 val_loss= 9.96 time= 0.47\n","Epoch: 091 train_loss= 3014315.93 val_loss= 9.96 time= 0.53\n","Epoch: 101 train_loss= 3133237.28 val_loss= 9.96 time= 0.58\n","test error=125.59,0.14, mean test error=268.73,0.17\n","testing on day #73\n","Epoch: 001 train_loss= 3532236.83 val_loss= 36.94 time= 0.01\n","Epoch: 011 train_loss= 3166147.44 val_loss= 10.27 time= 0.07\n","Epoch: 021 train_loss= 2773689.09 val_loss= 9.74 time= 0.12\n","Epoch: 031 train_loss= 3048798.69 val_loss= 9.89 time= 0.18\n","Epoch: 041 train_loss= 2866028.38 val_loss= 9.89 time= 0.24\n","Epoch: 051 train_loss= 2784639.15 val_loss= 9.88 time= 0.30\n","Epoch: 061 train_loss= 2796559.72 val_loss= 9.88 time= 0.36\n","Epoch: 071 train_loss= 3071934.81 val_loss= 9.88 time= 0.42\n","Epoch: 081 train_loss= 3118425.02 val_loss= 9.88 time= 0.47\n","Epoch: 091 train_loss= 3200959.83 val_loss= 9.88 time= 0.53\n","Epoch: 101 train_loss= 2841528.46 val_loss= 9.88 time= 0.59\n","test error=150.90,0.19, mean test error=266.73,0.17\n","testing on day #74\n","Epoch: 001 train_loss= 3433238.99 val_loss= 23.16 time= 0.01\n","Epoch: 011 train_loss= 2707305.16 val_loss= 8.24 time= 0.07\n","Epoch: 021 train_loss= 2801042.29 val_loss= 8.35 time= 0.13\n","Epoch: 031 train_loss= 2888285.56 val_loss= 8.13 time= 0.19\n","Epoch: 041 train_loss= 3127010.50 val_loss= 8.16 time= 0.24\n","Epoch: 051 train_loss= 3169425.40 val_loss= 8.17 time= 0.30\n","Epoch: 061 train_loss= 3110936.50 val_loss= 8.17 time= 0.36\n","Epoch: 071 train_loss= 3077349.91 val_loss= 8.17 time= 0.42\n","Epoch: 081 train_loss= 3047366.88 val_loss= 8.17 time= 0.48\n","Epoch: 091 train_loss= 2971805.95 val_loss= 8.17 time= 0.54\n","Epoch: 101 train_loss= 2742308.18 val_loss= 8.17 time= 0.60\n","test error=154.59,0.16, mean test error=264.86,0.17\n","testing on day #75\n","Epoch: 001 train_loss= 3212765.42 val_loss= 26.92 time= 0.01\n","Epoch: 011 train_loss= 2960915.55 val_loss= 9.17 time= 0.07\n","Epoch: 021 train_loss= 3032718.07 val_loss= 8.06 time= 0.13\n","Epoch: 031 train_loss= 2941632.30 val_loss= 7.72 time= 0.19\n","Epoch: 041 train_loss= 2969644.43 val_loss= 7.73 time= 0.25\n","Epoch: 051 train_loss= 3021832.62 val_loss= 7.73 time= 0.31\n","Epoch: 061 train_loss= 2887331.69 val_loss= 7.73 time= 0.37\n","Epoch: 071 train_loss= 2871247.12 val_loss= 7.73 time= 0.43\n","Epoch: 081 train_loss= 2699244.12 val_loss= 7.73 time= 0.49\n","Epoch: 091 train_loss= 3009034.98 val_loss= 7.73 time= 0.55\n","Epoch: 101 train_loss= 2940850.81 val_loss= 7.73 time= 0.61\n","test error=152.02,0.18, mean test error=263.01,0.17\n","testing on day #76\n","Epoch: 001 train_loss= 3344615.45 val_loss= 24.29 time= 0.01\n","Epoch: 011 train_loss= 3277939.97 val_loss= 8.55 time= 0.07\n","Epoch: 021 train_loss= 2799743.01 val_loss= 6.81 time= 0.13\n","Epoch: 031 train_loss= 2931735.79 val_loss= 6.69 time= 0.19\n","Epoch: 041 train_loss= 3066454.92 val_loss= 6.75 time= 0.25\n","Epoch: 051 train_loss= 2998883.62 val_loss= 6.75 time= 0.32\n","Epoch: 061 train_loss= 2956542.04 val_loss= 6.75 time= 0.38\n","Epoch: 071 train_loss= 2858479.69 val_loss= 6.75 time= 0.44\n","Epoch: 081 train_loss= 2872488.39 val_loss= 6.75 time= 0.50\n","Epoch: 091 train_loss= 2897613.75 val_loss= 6.75 time= 0.56\n","Epoch: 101 train_loss= 2908943.23 val_loss= 6.75 time= 0.63\n","test error=136.94,0.16, mean test error=260.98,0.17\n","testing on day #77\n","Epoch: 001 train_loss= 3464366.36 val_loss= 30.09 time= 0.01\n","Epoch: 011 train_loss= 3240022.76 val_loss= 8.65 time= 0.07\n","Epoch: 021 train_loss= 2845608.08 val_loss= 7.31 time= 0.13\n","Epoch: 031 train_loss= 3009915.88 val_loss= 7.48 time= 0.19\n","Epoch: 041 train_loss= 2788494.15 val_loss= 7.47 time= 0.26\n","Epoch: 051 train_loss= 2970506.42 val_loss= 7.47 time= 0.32\n","Epoch: 061 train_loss= 3052063.91 val_loss= 7.47 time= 0.38\n","Epoch: 071 train_loss= 2972606.86 val_loss= 7.47 time= 0.45\n","Epoch: 081 train_loss= 2987646.13 val_loss= 7.47 time= 0.51\n","Epoch: 091 train_loss= 3208501.05 val_loss= 7.47 time= 0.57\n","Epoch: 101 train_loss= 2942612.23 val_loss= 7.47 time= 0.63\n","test error=173.04,0.21, mean test error=259.58,0.17\n","testing on day #78\n","Epoch: 001 train_loss= 3341086.98 val_loss= 20.41 time= 0.01\n","Epoch: 011 train_loss= 3058673.75 val_loss= 7.36 time= 0.07\n","Epoch: 021 train_loss= 2950014.78 val_loss= 6.70 time= 0.13\n","Epoch: 031 train_loss= 2862940.12 val_loss= 6.89 time= 0.20\n","Epoch: 041 train_loss= 2902257.74 val_loss= 6.85 time= 0.26\n","Epoch: 051 train_loss= 2892582.46 val_loss= 6.85 time= 0.32\n","Epoch: 061 train_loss= 3111809.36 val_loss= 6.85 time= 0.39\n","Epoch: 071 train_loss= 2982365.28 val_loss= 6.85 time= 0.45\n","Epoch: 081 train_loss= 2928552.31 val_loss= 6.85 time= 0.51\n","Epoch: 091 train_loss= 2740016.14 val_loss= 6.85 time= 0.58\n","Epoch: 101 train_loss= 2828420.23 val_loss= 6.85 time= 0.64\n","test error=139.95,0.16, mean test error=257.71,0.17\n","testing on day #79\n","Epoch: 001 train_loss= 3312057.43 val_loss= 26.46 time= 0.01\n","Epoch: 011 train_loss= 2923421.49 val_loss= 8.68 time= 0.07\n","Epoch: 021 train_loss= 2856640.67 val_loss= 7.65 time= 0.14\n","Epoch: 031 train_loss= 2708409.01 val_loss= 7.45 time= 0.20\n","Epoch: 041 train_loss= 2689486.23 val_loss= 7.48 time= 0.26\n","Epoch: 051 train_loss= 2772036.46 val_loss= 7.48 time= 0.33\n","Epoch: 061 train_loss= 3017509.90 val_loss= 7.48 time= 0.39\n","Epoch: 071 train_loss= 2809432.94 val_loss= 7.48 time= 0.46\n","Epoch: 081 train_loss= 2714525.26 val_loss= 7.48 time= 0.52\n","Epoch: 091 train_loss= 2683593.35 val_loss= 7.48 time= 0.58\n","Epoch: 101 train_loss= 2833766.07 val_loss= 7.48 time= 0.65\n","test error=140.86,0.19, mean test error=255.92,0.17\n","testing on day #80\n","Epoch: 001 train_loss= 3137349.52 val_loss= 20.35 time= 0.01\n","Epoch: 011 train_loss= 2918334.76 val_loss= 8.60 time= 0.07\n","Epoch: 021 train_loss= 2739520.94 val_loss= 7.28 time= 0.14\n","Epoch: 031 train_loss= 2808346.11 val_loss= 7.22 time= 0.20\n","Epoch: 041 train_loss= 2724485.07 val_loss= 7.19 time= 0.27\n","Epoch: 051 train_loss= 2861216.28 val_loss= 7.19 time= 0.33\n","Epoch: 061 train_loss= 2746933.21 val_loss= 7.19 time= 0.40\n","Epoch: 071 train_loss= 2587370.51 val_loss= 7.19 time= 0.46\n","Epoch: 081 train_loss= 2983309.14 val_loss= 7.19 time= 0.53\n","Epoch: 091 train_loss= 2659166.31 val_loss= 7.19 time= 0.59\n","Epoch: 101 train_loss= 2661747.38 val_loss= 7.19 time= 0.66\n","test error=149.20,0.16, mean test error=254.30,0.17\n","testing on day #81\n","Epoch: 001 train_loss= 3171045.73 val_loss= 18.69 time= 0.01\n","Epoch: 011 train_loss= 2977940.47 val_loss= 7.67 time= 0.07\n","Epoch: 021 train_loss= 2722566.43 val_loss= 7.33 time= 0.14\n","Epoch: 031 train_loss= 2700817.87 val_loss= 7.12 time= 0.20\n","Epoch: 041 train_loss= 2799552.48 val_loss= 7.08 time= 0.27\n","Epoch: 051 train_loss= 2547988.98 val_loss= 7.08 time= 0.34\n","Epoch: 061 train_loss= 2901005.89 val_loss= 7.08 time= 0.40\n","Epoch: 071 train_loss= 2686269.03 val_loss= 7.09 time= 0.47\n","Epoch: 081 train_loss= 2843527.65 val_loss= 7.08 time= 0.53\n","Epoch: 091 train_loss= 2689101.83 val_loss= 7.08 time= 0.60\n","Epoch: 101 train_loss= 2888798.78 val_loss= 7.08 time= 0.67\n","test error=180.11,0.21, mean test error=253.19,0.17\n","testing on day #82\n","Epoch: 001 train_loss= 3333976.32 val_loss= 13.73 time= 0.01\n","Epoch: 011 train_loss= 3191909.88 val_loss= 7.91 time= 0.07\n","Epoch: 021 train_loss= 2910865.02 val_loss= 7.67 time= 0.14\n","test error=175.35,0.18, mean test error=252.05,0.17\n","testing on day #83\n","Epoch: 001 train_loss= 3100871.76 val_loss= 18.87 time= 0.01\n","Epoch: 011 train_loss= 2665414.59 val_loss= 8.47 time= 0.08\n","Epoch: 021 train_loss= 2710673.60 val_loss= 9.00 time= 0.14\n","Epoch: 031 train_loss= 2696181.85 val_loss= 8.80 time= 0.21\n","Epoch: 041 train_loss= 2712481.07 val_loss= 8.73 time= 0.28\n","Epoch: 051 train_loss= 2865054.33 val_loss= 8.74 time= 0.34\n","Epoch: 061 train_loss= 2847839.46 val_loss= 8.74 time= 0.41\n","Epoch: 071 train_loss= 2768199.53 val_loss= 8.74 time= 0.48\n","Epoch: 081 train_loss= 2809679.83 val_loss= 8.74 time= 0.55\n","Epoch: 091 train_loss= 2862366.98 val_loss= 8.74 time= 0.61\n","Epoch: 101 train_loss= 2829115.24 val_loss= 8.74 time= 0.68\n","test error=176.71,0.19, mean test error=250.96,0.17\n","testing on day #84\n","Epoch: 001 train_loss= 3042540.83 val_loss= 20.07 time= 0.01\n","Epoch: 011 train_loss= 2881191.97 val_loss= 12.35 time= 0.08\n","Epoch: 021 train_loss= 2906438.47 val_loss= 11.82 time= 0.15\n","Epoch: 031 train_loss= 2684272.68 val_loss= 11.74 time= 0.22\n","Epoch: 041 train_loss= 2387641.71 val_loss= 11.75 time= 0.28\n","Epoch: 051 train_loss= 2675558.24 val_loss= 11.76 time= 0.35\n","Epoch: 061 train_loss= 2642662.28 val_loss= 11.76 time= 0.42\n","Epoch: 071 train_loss= 2605678.82 val_loss= 11.76 time= 0.49\n","Epoch: 081 train_loss= 2683563.79 val_loss= 11.76 time= 0.56\n","Epoch: 091 train_loss= 2655073.39 val_loss= 11.76 time= 0.63\n","Epoch: 101 train_loss= 2905818.59 val_loss= 11.76 time= 0.70\n","test error=165.77,0.17, mean test error=249.74,0.17\n","testing on day #85\n","Epoch: 001 train_loss= 3193921.23 val_loss= 20.05 time= 0.01\n","Epoch: 011 train_loss= 3055567.25 val_loss= 11.13 time= 0.08\n","Epoch: 021 train_loss= 2827583.34 val_loss= 12.14 time= 0.15\n","Epoch: 031 train_loss= 2599522.77 val_loss= 11.58 time= 0.22\n","Epoch: 041 train_loss= 2525366.01 val_loss= 11.61 time= 0.29\n","Epoch: 051 train_loss= 2876361.91 val_loss= 11.60 time= 0.36\n","Epoch: 061 train_loss= 2695649.20 val_loss= 11.59 time= 0.43\n","Epoch: 071 train_loss= 2820729.96 val_loss= 11.59 time= 0.50\n","Epoch: 081 train_loss= 2601610.97 val_loss= 11.59 time= 0.57\n","Epoch: 091 train_loss= 2330675.09 val_loss= 11.59 time= 0.64\n","Epoch: 101 train_loss= 2767545.80 val_loss= 11.59 time= 0.71\n","test error=160.37,0.17, mean test error=248.48,0.17\n","testing on day #86\n","Epoch: 001 train_loss= 3313728.97 val_loss= 19.06 time= 0.01\n","Epoch: 011 train_loss= 2969613.12 val_loss= 13.34 time= 0.08\n","Epoch: 021 train_loss= 2774446.26 val_loss= 13.06 time= 0.15\n","Epoch: 031 train_loss= 2618506.48 val_loss= 12.48 time= 0.22\n","Epoch: 041 train_loss= 2522388.56 val_loss= 12.65 time= 0.29\n","Epoch: 051 train_loss= 2670052.12 val_loss= 12.65 time= 0.36\n","Epoch: 061 train_loss= 2703511.33 val_loss= 12.65 time= 0.43\n","Epoch: 071 train_loss= 2568555.08 val_loss= 12.65 time= 0.50\n","Epoch: 081 train_loss= 2679902.30 val_loss= 12.65 time= 0.57\n","Epoch: 091 train_loss= 2635511.25 val_loss= 12.65 time= 0.64\n","Epoch: 101 train_loss= 2738249.89 val_loss= 12.65 time= 0.71\n","test error=170.73,0.18, mean test error=247.40,0.17\n","testing on day #87\n","Epoch: 001 train_loss= 3346639.64 val_loss= 21.75 time= 0.01\n","Epoch: 011 train_loss= 2907459.03 val_loss= 13.85 time= 0.08\n","Epoch: 021 train_loss= 2493987.95 val_loss= 12.62 time= 0.15\n","Epoch: 031 train_loss= 2693590.97 val_loss= 12.67 time= 0.22\n","Epoch: 041 train_loss= 2475224.46 val_loss= 12.63 time= 0.30\n","Epoch: 051 train_loss= 2392391.64 val_loss= 12.62 time= 0.37\n","Epoch: 061 train_loss= 2550066.76 val_loss= 12.62 time= 0.44\n","Epoch: 071 train_loss= 2608328.17 val_loss= 12.62 time= 0.51\n","Epoch: 081 train_loss= 2588380.59 val_loss= 12.62 time= 0.58\n","Epoch: 091 train_loss= 2697552.01 val_loss= 12.62 time= 0.65\n","Epoch: 101 train_loss= 2609699.12 val_loss= 12.62 time= 0.72\n","test error=154.73,0.18, mean test error=246.13,0.17\n","testing on day #88\n","Epoch: 001 train_loss= 3243536.34 val_loss= 21.48 time= 0.01\n","Epoch: 011 train_loss= 2672036.32 val_loss= 15.21 time= 0.08\n","Epoch: 021 train_loss= 2465931.08 val_loss= 14.72 time= 0.15\n","Epoch: 031 train_loss= 2400701.31 val_loss= 14.62 time= 0.23\n","Epoch: 041 train_loss= 2519820.28 val_loss= 14.57 time= 0.30\n","Epoch: 051 train_loss= 2352474.77 val_loss= 14.57 time= 0.37\n","Epoch: 061 train_loss= 2470763.47 val_loss= 14.57 time= 0.44\n","Epoch: 071 train_loss= 2565430.81 val_loss= 14.57 time= 0.51\n","Epoch: 081 train_loss= 2448922.03 val_loss= 14.57 time= 0.59\n","Epoch: 091 train_loss= 2634913.69 val_loss= 14.57 time= 0.66\n","Epoch: 101 train_loss= 2579166.07 val_loss= 14.57 time= 0.73\n","test error=168.81,0.18, mean test error=245.09,0.17\n","testing on day #89\n","Epoch: 001 train_loss= 2857319.79 val_loss= 28.66 time= 0.01\n","Epoch: 011 train_loss= 2719455.96 val_loss= 15.63 time= 0.08\n","Epoch: 021 train_loss= 2919470.54 val_loss= 15.49 time= 0.16\n","Epoch: 031 train_loss= 2556118.59 val_loss= 15.10 time= 0.23\n","Epoch: 041 train_loss= 2373607.83 val_loss= 15.07 time= 0.30\n","Epoch: 051 train_loss= 2397163.71 val_loss= 15.09 time= 0.37\n","Epoch: 061 train_loss= 2570179.84 val_loss= 15.02 time= 0.45\n","Epoch: 071 train_loss= 2435840.06 val_loss= 15.02 time= 0.52\n","Epoch: 081 train_loss= 2677469.69 val_loss= 15.01 time= 0.59\n","Epoch: 091 train_loss= 2586127.08 val_loss= 15.01 time= 0.67\n","Epoch: 101 train_loss= 2583160.21 val_loss= 15.01 time= 0.74\n","test error=183.02,0.18, mean test error=244.26,0.17\n","testing on day #90\n","Epoch: 001 train_loss= 3360575.49 val_loss= 27.31 time= 0.01\n","Epoch: 011 train_loss= 2689066.15 val_loss= 18.34 time= 0.08\n","Epoch: 021 train_loss= 2557191.05 val_loss= 17.92 time= 0.16\n","Epoch: 031 train_loss= 2525900.81 val_loss= 17.82 time= 0.23\n","Epoch: 041 train_loss= 2410099.97 val_loss= 17.79 time= 0.30\n","Epoch: 051 train_loss= 2340423.75 val_loss= 17.78 time= 0.38\n","Epoch: 061 train_loss= 2428107.07 val_loss= 17.78 time= 0.45\n","Epoch: 071 train_loss= 2612818.79 val_loss= 17.78 time= 0.53\n","Epoch: 081 train_loss= 2520012.01 val_loss= 17.78 time= 0.60\n","Epoch: 091 train_loss= 2418006.70 val_loss= 17.78 time= 0.67\n","Epoch: 101 train_loss= 2576177.01 val_loss= 17.78 time= 0.75\n","test error=193.49,0.19, mean test error=243.59,0.17\n","testing on day #91\n","Epoch: 001 train_loss= 3001518.83 val_loss= 31.18 time= 0.01\n","Epoch: 011 train_loss= 2435521.54 val_loss= 19.97 time= 0.08\n","Epoch: 021 train_loss= 2510241.02 val_loss= 18.64 time= 0.16\n","Epoch: 031 train_loss= 2510985.49 val_loss= 18.43 time= 0.23\n","Epoch: 041 train_loss= 2675623.82 val_loss= 18.42 time= 0.31\n","Epoch: 051 train_loss= 2522612.46 val_loss= 18.43 time= 0.38\n","Epoch: 061 train_loss= 2528684.96 val_loss= 18.43 time= 0.46\n","Epoch: 071 train_loss= 2590210.45 val_loss= 18.43 time= 0.53\n","Epoch: 081 train_loss= 2344743.19 val_loss= 18.43 time= 0.61\n","Epoch: 091 train_loss= 2736559.88 val_loss= 18.43 time= 0.68\n","Epoch: 101 train_loss= 2434597.24 val_loss= 18.43 time= 0.76\n","test error=181.18,0.19, mean test error=242.78,0.17\n","testing on day #92\n","Epoch: 001 train_loss= 2886884.45 val_loss= 32.36 time= 0.01\n","Epoch: 011 train_loss= 2562485.92 val_loss= 21.22 time= 0.08\n","Epoch: 021 train_loss= 2554515.78 val_loss= 18.88 time= 0.16\n","Epoch: 031 train_loss= 2440629.21 val_loss= 18.85 time= 0.24\n","Epoch: 041 train_loss= 2588011.05 val_loss= 18.88 time= 0.31\n","Epoch: 051 train_loss= 2431454.93 val_loss= 18.88 time= 0.39\n","Epoch: 061 train_loss= 2532477.23 val_loss= 18.88 time= 0.47\n","Epoch: 071 train_loss= 2389356.83 val_loss= 18.88 time= 0.54\n","Epoch: 081 train_loss= 2509571.79 val_loss= 18.88 time= 0.62\n","Epoch: 091 train_loss= 2425540.45 val_loss= 18.88 time= 0.70\n","Epoch: 101 train_loss= 2524679.17 val_loss= 18.88 time= 0.77\n","test error=160.28,0.17, mean test error=241.72,0.17\n","testing on day #93\n","Epoch: 001 train_loss= 3307231.49 val_loss= 37.19 time= 0.01\n","Epoch: 011 train_loss= 2576398.42 val_loss= 20.77 time= 0.09\n","Epoch: 021 train_loss= 2476299.06 val_loss= 18.85 time= 0.16\n","Epoch: 031 train_loss= 2562709.82 val_loss= 18.80 time= 0.24\n","Epoch: 041 train_loss= 2732092.81 val_loss= 18.81 time= 0.32\n","Epoch: 051 train_loss= 2375926.72 val_loss= 18.80 time= 0.39\n","Epoch: 061 train_loss= 2409146.51 val_loss= 18.80 time= 0.47\n","Epoch: 071 train_loss= 2343450.39 val_loss= 18.80 time= 0.55\n","Epoch: 081 train_loss= 2463929.03 val_loss= 18.80 time= 0.62\n","Epoch: 091 train_loss= 2628462.88 val_loss= 18.80 time= 0.70\n","Epoch: 101 train_loss= 2423262.75 val_loss= 18.80 time= 0.78\n","test error=159.16,0.19, mean test error=240.68,0.17\n","testing on day #94\n","Epoch: 001 train_loss= 2906039.39 val_loss= 38.37 time= 0.01\n","Epoch: 011 train_loss= 2571065.96 val_loss= 19.55 time= 0.09\n","Epoch: 021 train_loss= 2296948.53 val_loss= 16.69 time= 0.16\n","Epoch: 031 train_loss= 2542811.71 val_loss= 16.59 time= 0.24\n","Epoch: 041 train_loss= 2502604.78 val_loss= 16.45 time= 0.32\n","Epoch: 051 train_loss= 2552338.46 val_loss= 16.44 time= 0.40\n","Epoch: 061 train_loss= 2395169.79 val_loss= 16.44 time= 0.48\n","Epoch: 071 train_loss= 2507637.55 val_loss= 16.44 time= 0.55\n","Epoch: 081 train_loss= 2432548.37 val_loss= 16.44 time= 0.63\n","Epoch: 091 train_loss= 2361521.77 val_loss= 16.44 time= 0.71\n","Epoch: 101 train_loss= 2510256.46 val_loss= 16.44 time= 0.79\n","test error=168.17,0.17, mean test error=239.77,0.17\n","testing on day #95\n","Epoch: 001 train_loss= 3090130.12 val_loss= 38.97 time= 0.01\n","Epoch: 011 train_loss= 2527426.66 val_loss= 19.05 time= 0.09\n","Epoch: 021 train_loss= 2592330.26 val_loss= 18.10 time= 0.17\n","Epoch: 031 train_loss= 2509158.59 val_loss= 17.86 time= 0.25\n","Epoch: 041 train_loss= 2455391.67 val_loss= 17.87 time= 0.32\n","Epoch: 051 train_loss= 2444999.01 val_loss= 17.87 time= 0.40\n","Epoch: 061 train_loss= 2350754.88 val_loss= 17.87 time= 0.48\n","Epoch: 071 train_loss= 2534013.21 val_loss= 17.87 time= 0.56\n","Epoch: 081 train_loss= 2411241.20 val_loss= 17.86 time= 0.64\n","Epoch: 091 train_loss= 2464291.98 val_loss= 17.87 time= 0.72\n","Epoch: 101 train_loss= 2413999.49 val_loss= 17.87 time= 0.79\n","test error=186.62,0.21, mean test error=239.12,0.17\n","testing on day #96\n","Epoch: 001 train_loss= 3149126.37 val_loss= 39.27 time= 0.01\n","Epoch: 011 train_loss= 2627822.55 val_loss= 19.22 time= 0.09\n","Epoch: 021 train_loss= 2334419.68 val_loss= 17.94 time= 0.17\n","Epoch: 031 train_loss= 2366924.06 val_loss= 17.52 time= 0.25\n","Epoch: 041 train_loss= 2292413.55 val_loss= 17.53 time= 0.33\n","Epoch: 051 train_loss= 2683437.17 val_loss= 17.52 time= 0.41\n","Epoch: 061 train_loss= 2464959.20 val_loss= 17.51 time= 0.49\n","Epoch: 071 train_loss= 2315543.16 val_loss= 17.51 time= 0.57\n","Epoch: 081 train_loss= 2586204.54 val_loss= 17.51 time= 0.64\n","Epoch: 091 train_loss= 2278835.09 val_loss= 17.51 time= 0.72\n","Epoch: 101 train_loss= 2486062.22 val_loss= 17.51 time= 0.80\n","test error=179.74,0.18, mean test error=238.39,0.17\n","testing on day #97\n","Epoch: 001 train_loss= 2835102.79 val_loss= 35.11 time= 0.01\n","Epoch: 011 train_loss= 2832412.58 val_loss= 22.51 time= 0.09\n","Epoch: 021 train_loss= 2490243.13 val_loss= 18.01 time= 0.17\n","Epoch: 031 train_loss= 2417849.12 val_loss= 17.86 time= 0.25\n","Epoch: 041 train_loss= 2390803.85 val_loss= 17.73 time= 0.33\n","Epoch: 051 train_loss= 2329561.78 val_loss= 17.71 time= 0.41\n","Epoch: 061 train_loss= 2468925.54 val_loss= 17.71 time= 0.49\n","Epoch: 071 train_loss= 2428545.51 val_loss= 17.71 time= 0.57\n","Epoch: 081 train_loss= 2451670.09 val_loss= 17.71 time= 0.65\n","Epoch: 091 train_loss= 2524306.28 val_loss= 17.71 time= 0.73\n","Epoch: 101 train_loss= 2152133.21 val_loss= 17.71 time= 0.81\n","test error=176.52,0.20, mean test error=237.65,0.17\n","testing on day #98\n","Epoch: 001 train_loss= 3232769.43 val_loss= 36.96 time= 0.01\n","Epoch: 011 train_loss= 2516999.35 val_loss= 19.93 time= 0.09\n","Epoch: 021 train_loss= 2515716.94 val_loss= 18.80 time= 0.17\n","Epoch: 031 train_loss= 2503400.99 val_loss= 18.67 time= 0.25\n","Epoch: 041 train_loss= 2473058.16 val_loss= 18.50 time= 0.33\n","Epoch: 051 train_loss= 2435541.22 val_loss= 18.50 time= 0.41\n","Epoch: 061 train_loss= 2281742.60 val_loss= 18.50 time= 0.50\n","Epoch: 071 train_loss= 2385425.08 val_loss= 18.50 time= 0.58\n","Epoch: 081 train_loss= 2435519.56 val_loss= 18.50 time= 0.66\n","Epoch: 091 train_loss= 2444660.95 val_loss= 18.50 time= 0.74\n","Epoch: 101 train_loss= 2441358.62 val_loss= 18.50 time= 0.82\n","test error=167.52,0.17, mean test error=236.81,0.17\n","testing on day #99\n","Epoch: 001 train_loss= 2995018.85 val_loss= 37.96 time= 0.01\n","Epoch: 011 train_loss= 2590991.45 val_loss= 21.00 time= 0.09\n","Epoch: 021 train_loss= 2481989.83 val_loss= 19.15 time= 0.17\n","Epoch: 031 train_loss= 2361848.09 val_loss= 18.83 time= 0.25\n","Epoch: 041 train_loss= 2348491.20 val_loss= 18.76 time= 0.34\n","Epoch: 051 train_loss= 2543716.19 val_loss= 18.76 time= 0.42\n","Epoch: 061 train_loss= 2392996.23 val_loss= 18.76 time= 0.50\n","Epoch: 071 train_loss= 2516980.02 val_loss= 18.76 time= 0.58\n","Epoch: 081 train_loss= 2535457.44 val_loss= 18.76 time= 0.66\n","Epoch: 091 train_loss= 2457265.81 val_loss= 18.76 time= 0.75\n","Epoch: 101 train_loss= 2468319.03 val_loss= 18.76 time= 0.83\n","test error=182.79,0.18, mean test error=236.18,0.17\n","testing on day #100\n","Epoch: 001 train_loss= 3273009.84 val_loss= 32.05 time= 0.01\n","Epoch: 011 train_loss= 2490682.49 val_loss= 21.07 time= 0.09\n","Epoch: 021 train_loss= 2313083.96 val_loss= 17.02 time= 0.18\n","Epoch: 031 train_loss= 2495783.37 val_loss= 17.48 time= 0.26\n","Epoch: 041 train_loss= 2451688.90 val_loss= 17.46 time= 0.34\n","Epoch: 051 train_loss= 2374715.38 val_loss= 17.46 time= 0.43\n","Epoch: 061 train_loss= 2313455.33 val_loss= 17.46 time= 0.51\n","Epoch: 071 train_loss= 2420273.99 val_loss= 17.46 time= 0.59\n","Epoch: 081 train_loss= 2428959.00 val_loss= 17.45 time= 0.68\n","Epoch: 091 train_loss= 2206576.99 val_loss= 17.45 time= 0.76\n","Epoch: 101 train_loss= 2481975.27 val_loss= 17.45 time= 0.84\n","test error=179.77,0.18, mean test error=235.52,0.17\n","testing on day #101\n","Epoch: 001 train_loss= 2998014.73 val_loss= 34.40 time= 0.01\n","Epoch: 011 train_loss= 2429677.58 val_loss= 20.40 time= 0.09\n","Epoch: 021 train_loss= 2495647.16 val_loss= 17.39 time= 0.18\n","Epoch: 031 train_loss= 2534355.97 val_loss= 17.22 time= 0.26\n","Epoch: 041 train_loss= 2333414.07 val_loss= 17.19 time= 0.35\n","Epoch: 051 train_loss= 2414387.35 val_loss= 17.18 time= 0.43\n","Epoch: 061 train_loss= 2396716.97 val_loss= 17.18 time= 0.51\n","Epoch: 071 train_loss= 2239912.97 val_loss= 17.18 time= 0.60\n","Epoch: 081 train_loss= 2346691.24 val_loss= 17.18 time= 0.68\n","Epoch: 091 train_loss= 2423689.02 val_loss= 17.18 time= 0.77\n","Epoch: 101 train_loss= 2162579.49 val_loss= 17.18 time= 0.85\n","test error=179.94,0.17, mean test error=234.88,0.17\n","testing on day #102\n","Epoch: 001 train_loss= 3011523.21 val_loss= 33.67 time= 0.01\n","Epoch: 011 train_loss= 2509674.68 val_loss= 17.70 time= 0.09\n","Epoch: 021 train_loss= 2348708.02 val_loss= 15.48 time= 0.18\n","Epoch: 031 train_loss= 2429405.38 val_loss= 15.38 time= 0.26\n","Epoch: 041 train_loss= 2321719.11 val_loss= 15.41 time= 0.35\n","Epoch: 051 train_loss= 2238208.68 val_loss= 15.40 time= 0.43\n","Epoch: 061 train_loss= 2364469.99 val_loss= 15.40 time= 0.52\n","Epoch: 071 train_loss= 2211686.04 val_loss= 15.40 time= 0.60\n","Epoch: 081 train_loss= 2382625.65 val_loss= 15.40 time= 0.69\n","Epoch: 091 train_loss= 2496418.97 val_loss= 15.40 time= 0.78\n","Epoch: 101 train_loss= 2373385.72 val_loss= 15.40 time= 0.86\n","test error=216.36,0.19, mean test error=234.67,0.17\n","testing on day #103\n","Epoch: 001 train_loss= 2983521.95 val_loss= 35.10 time= 0.01\n","Epoch: 011 train_loss= 2529598.82 val_loss= 22.17 time= 0.10\n","Epoch: 021 train_loss= 2346544.90 val_loss= 17.10 time= 0.18\n","Epoch: 031 train_loss= 2269823.05 val_loss= 17.07 time= 0.27\n","Epoch: 041 train_loss= 2390914.40 val_loss= 17.05 time= 0.35\n","Epoch: 051 train_loss= 2354975.84 val_loss= 17.04 time= 0.44\n","Epoch: 061 train_loss= 2255251.48 val_loss= 17.03 time= 0.52\n","Epoch: 071 train_loss= 2356386.01 val_loss= 17.03 time= 0.61\n","Epoch: 081 train_loss= 2353032.76 val_loss= 17.03 time= 0.70\n","Epoch: 091 train_loss= 2218100.46 val_loss= 17.03 time= 0.78\n","Epoch: 101 train_loss= 2276838.43 val_loss= 17.03 time= 0.87\n","test error=207.97,0.19, mean test error=234.37,0.17\n","testing on day #104\n","Epoch: 001 train_loss= 3000842.03 val_loss= 42.01 time= 0.01\n","Epoch: 011 train_loss= 2508558.47 val_loss= 21.29 time= 0.10\n","Epoch: 021 train_loss= 2361204.84 val_loss= 20.38 time= 0.18\n","Epoch: 031 train_loss= 2314334.25 val_loss= 20.08 time= 0.27\n","Epoch: 041 train_loss= 2238450.92 val_loss= 20.01 time= 0.36\n","Epoch: 051 train_loss= 2292137.22 val_loss= 20.02 time= 0.44\n","Epoch: 061 train_loss= 2322050.03 val_loss= 20.01 time= 0.53\n","Epoch: 071 train_loss= 2403673.83 val_loss= 20.02 time= 0.62\n","Epoch: 081 train_loss= 2333118.13 val_loss= 20.02 time= 0.70\n","Epoch: 091 train_loss= 2320785.56 val_loss= 20.02 time= 0.79\n","Epoch: 101 train_loss= 2371414.01 val_loss= 20.01 time= 0.88\n","test error=216.07,0.20, mean test error=234.17,0.17\n","testing on day #105\n","Epoch: 001 train_loss= 2963607.77 val_loss= 38.95 time= 0.01\n","Epoch: 011 train_loss= 2741502.40 val_loss= 27.22 time= 0.10\n","Epoch: 021 train_loss= 2311828.04 val_loss= 21.10 time= 0.18\n","Epoch: 031 train_loss= 2318745.50 val_loss= 20.73 time= 0.27\n","Epoch: 041 train_loss= 2291372.73 val_loss= 20.66 time= 0.36\n","Epoch: 051 train_loss= 2412464.38 val_loss= 20.65 time= 0.45\n","Epoch: 061 train_loss= 2360489.28 val_loss= 20.65 time= 0.54\n","Epoch: 071 train_loss= 2247126.28 val_loss= 20.65 time= 0.62\n","Epoch: 081 train_loss= 2413364.82 val_loss= 20.65 time= 0.71\n","Epoch: 091 train_loss= 2251776.39 val_loss= 20.65 time= 0.80\n","Epoch: 101 train_loss= 2249790.83 val_loss= 20.65 time= 0.89\n","test error=201.45,0.19, mean test error=233.81,0.17\n","testing on day #106\n","Epoch: 001 train_loss= 2941170.95 val_loss= 51.19 time= 0.01\n","Epoch: 011 train_loss= 2556684.40 val_loss= 26.27 time= 0.10\n","Epoch: 021 train_loss= 2257549.85 val_loss= 24.01 time= 0.19\n","Epoch: 031 train_loss= 2288577.47 val_loss= 23.60 time= 0.28\n","Epoch: 041 train_loss= 2376116.51 val_loss= 23.59 time= 0.36\n","Epoch: 051 train_loss= 2333872.22 val_loss= 23.58 time= 0.45\n","Epoch: 061 train_loss= 2441814.09 val_loss= 23.58 time= 0.54\n","Epoch: 071 train_loss= 2190563.51 val_loss= 23.58 time= 0.63\n","Epoch: 081 train_loss= 2293776.91 val_loss= 23.58 time= 0.72\n","Epoch: 091 train_loss= 2450471.40 val_loss= 23.58 time= 0.81\n","Epoch: 101 train_loss= 2257683.85 val_loss= 23.58 time= 0.90\n","test error=197.57,0.17, mean test error=233.41,0.17\n","testing on day #107\n","Epoch: 001 train_loss= 2868551.31 val_loss= 48.23 time= 0.01\n","Epoch: 011 train_loss= 2541723.11 val_loss= 35.36 time= 0.10\n","Epoch: 021 train_loss= 2320582.80 val_loss= 24.92 time= 0.19\n","Epoch: 031 train_loss= 2392067.84 val_loss= 24.44 time= 0.28\n","Epoch: 041 train_loss= 2428531.59 val_loss= 24.32 time= 0.37\n","Epoch: 051 train_loss= 2109274.05 val_loss= 24.31 time= 0.46\n","Epoch: 061 train_loss= 2302413.88 val_loss= 24.31 time= 0.55\n","Epoch: 071 train_loss= 2405150.92 val_loss= 24.31 time= 0.64\n","Epoch: 081 train_loss= 2261327.12 val_loss= 24.31 time= 0.73\n","Epoch: 091 train_loss= 2392088.92 val_loss= 24.31 time= 0.82\n","Epoch: 101 train_loss= 2414226.77 val_loss= 24.31 time= 0.90\n","test error=192.31,0.17, mean test error=232.97,0.17\n","testing on day #108\n","Epoch: 001 train_loss= 2413912.92 val_loss= 21.27 time= 0.01\n","Epoch: 011 train_loss= 2312039.73 val_loss= 27.04 time= 0.10\n","Epoch: 021 train_loss= 2154895.56 val_loss= 25.76 time= 0.19\n","Epoch: 031 train_loss= 2367402.29 val_loss= 25.63 time= 0.28\n","Epoch: 041 train_loss= 2240890.61 val_loss= 25.48 time= 0.36\n","Epoch: 051 train_loss= 2223250.78 val_loss= 25.50 time= 0.45\n","Epoch: 061 train_loss= 2216516.34 val_loss= 25.50 time= 0.54\n","Epoch: 071 train_loss= 2327642.39 val_loss= 25.50 time= 0.63\n","Epoch: 081 train_loss= 2406294.46 val_loss= 25.50 time= 0.72\n","Epoch: 091 train_loss= 2251349.93 val_loss= 25.50 time= 0.81\n","Epoch: 101 train_loss= 2291723.85 val_loss= 25.50 time= 0.90\n","test error=165.78,0.17, mean test error=232.26,0.17\n","testing on day #109\n","Epoch: 001 train_loss= 2372305.44 val_loss= 26.24 time= 0.01\n","Epoch: 011 train_loss= 2355902.52 val_loss= 29.77 time= 0.10\n","Epoch: 021 train_loss= 2366123.75 val_loss= 26.03 time= 0.19\n","Epoch: 031 train_loss= 2208975.85 val_loss= 25.97 time= 0.28\n","Epoch: 041 train_loss= 2242065.95 val_loss= 26.04 time= 0.36\n","Epoch: 051 train_loss= 2207424.64 val_loss= 26.04 time= 0.45\n","Epoch: 061 train_loss= 2346060.92 val_loss= 26.04 time= 0.54\n","Epoch: 071 train_loss= 2298446.54 val_loss= 26.04 time= 0.63\n","Epoch: 081 train_loss= 2220921.31 val_loss= 26.04 time= 0.72\n","Epoch: 091 train_loss= 2231780.69 val_loss= 26.04 time= 0.81\n","Epoch: 101 train_loss= 2299718.16 val_loss= 26.04 time= 0.90\n","test error=135.15,0.20, mean test error=231.23,0.17\n","testing on day #110\n","Epoch: 001 train_loss= 2429761.35 val_loss= 27.08 time= 0.01\n","Epoch: 011 train_loss= 2442655.12 val_loss= 29.22 time= 0.10\n"]}]}]}